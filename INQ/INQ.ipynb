{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand digit classifier \n",
    "---\n",
    "## Incremental network quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from skimage import io"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root='../', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root=\"../\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_set = [train_data[i] for i in range(50000)]\n",
    "validation_set = [train_data[i] for i in range(50000, 60000)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=64, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definig a VGG-7 inspired architecture model\n",
    "---\n",
    "Featuring 4 convolutional and 3 fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG7, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=\"same\", stride=1, bias=False)\n",
    "        \n",
    "        self.fc1 = nn.Linear(7*7*128, 512, bias=False)\n",
    "        self.fc2 = nn.Linear(512, 256, bias=False)\n",
    "        self.fc3 = nn.Linear(256, 10, bias=False)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, stride=2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, stride=2)  \n",
    "\n",
    "        x = x.view(-1, 7*7*128)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)  \n",
    "\n",
    "        x = F.log_softmax(x, dim=1)  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(net, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    confusion_matrix = np.zeros((10, 10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = net(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
    "            confusion_matrix[pred.view(-1), target] += 1\n",
    "\n",
    "    print(\"[OK] Model evaluation complete [OK]\")\n",
    "    print(\"Average loss: {:.5f}\".format(loss/len(test_loader.dataset)))\n",
    "    print(\"Test data accuracy: {:.2f}%\".format(100.*(correct/len(test_loader.dataset))))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(confusion_matrix/len(train_loader.dataset))\n",
    "    ax.set_xticks(np.arange(10))\n",
    "    ax.set_yticks(np.arange(10))\n",
    "    ax.set_xlabel(\"Truth\")\n",
    "    ax.set_ylabel(\"Predictions\")\n",
    "    ax.set_title(\"Confusion matrix\")\n",
    "    fig.set_size_inches(4, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Quantization operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBounderyExponents(W, b):\n",
    "    s = torch.max(torch.abs(W)).item()\n",
    "    n1 = np.floor(np.log2(4*(s/3)))\n",
    "    n2 = n1 + 1 - (2**(b - 1))/2\n",
    "    return n1, n2\n",
    "\n",
    "def getQuantizationMask(W, percentage, T):\n",
    "    w = W.view(-1)\n",
    "    t = T.view(-1)\n",
    "    idx = t == 1\n",
    "\n",
    "    numberOfWeights = w.size(dim=0)\n",
    "    numberOfQWeights = int(percentage*numberOfWeights - t[idx].size(dim=0))\n",
    "\n",
    "    t_aux = torch.Tensor(np.ones_like(T)).view(-1)\n",
    "    w = w*(t_aux - t)\n",
    "    w = torch.abs(w)\n",
    "    sorted_w, indices_w = w.sort()\n",
    "    t[indices_w[-numberOfQWeights:]] = 1\n",
    "    \n",
    "    return t.view(T.size())\n",
    "\n",
    "def quantizeWeights(W, T, n1, n2):\n",
    "    T_aux = torch.Tensor(np.ones_like(T))\n",
    "    eps = 1e-6\n",
    "    W1 = W*(T_aux - T)\n",
    "    idx = W == 0\n",
    "    W.data[idx] = eps\n",
    "\n",
    "    closestExp = torch.floor(torch.log2(torch.abs(W*4/3)))\n",
    "    Q = W1 + torch.sign(W)*(2**closestExp)*T\n",
    "\n",
    "    idx = closestExp*T < n2\n",
    "    Q[idx] = 0\n",
    "    idx = ((closestExp > n1)*T).bool()\n",
    "    Q[idx] = 2**n1\n",
    "\n",
    "    return closestExp, Q\n",
    "\n",
    "def quantize_conv_layer(W, T, percentage, number_of_bits):\n",
    "    n = T.size(dim=0)\n",
    "    m = T.size(dim=1)\n",
    "\n",
    "    n1, n2 = getBounderyExponents(W, number_of_bits)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            T[i, j, :, :] = getQuantizationMask(W[i, j, :, :], percentage, T[i, j, :, :])\n",
    "            _, W.data[i, j, :, :] = quantizeWeights(W[i, j, :, :], T[i, j, :, :], n1, n2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device initialization for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(\"../Baseline/baseline.pth\")\n",
    "net.to(device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-2, weight_decay=0)\n",
    "\n",
    "quantization_percentages = [0.5, 0.75, 0.875]\n",
    "quantization_precision = 8\n",
    "\n",
    "epochs = 5\n",
    "logs_interval = 100\n",
    "iteration = 0\n",
    "train_loss = []\n",
    "\n",
    "Tconv1 = torch.zeros_like(net.conv1.weight)\n",
    "Tconv2 = torch.zeros_like(net.conv2.weight)\n",
    "Tconv3 = torch.zeros_like(net.conv3.weight)\n",
    "Tconv4 = torch.zeros_like(net.conv4.weight)\n",
    "\n",
    "net.train()\n",
    "\n",
    "for q_stage, percentage in enumerate(quantization_percentages):\n",
    "    # quantize layers\n",
    "    quantize_conv_layer(net.conv1.weight, Tconv1, percentage, quantization_precision)\n",
    "    quantize_conv_layer(net.conv2.weight, Tconv2, percentage, quantization_precision)\n",
    "    quantize_conv_layer(net.conv3.weight, Tconv3, percentage, quantization_precision)\n",
    "    quantize_conv_layer(net.conv4.weight, Tconv4, percentage, quantization_precision)\n",
    "    \n",
    "    # correct remaining weights by training\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # setam gradienti la 0\n",
    "            Tconv1_aux = torch.ones_like(Tconv1)\n",
    "            Tconv2_aux = torch.ones_like(Tconv2)\n",
    "            Tconv3_aux = torch.ones_like(Tconv3)\n",
    "            Tconv4_aux = torch.ones_like(Tconv4)\n",
    "            net.conv1.weight.grad = net.conv1.weight.grad*(Tconv1_aux - Tconv1)\n",
    "            net.conv2.weight.grad = net.conv2.weight.grad*(Tconv2_aux - Tconv2)\n",
    "            net.conv3.weight.grad = net.conv3.weight.grad*(Tconv3_aux - Tconv3)\n",
    "            net.conv4.weight.grad = net.conv4.weight.grad*(Tconv4_aux - Tconv4)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            iteration = iteration + 1\n",
    "            if iteration % logs_interval == 0:\n",
    "                print('Quantization step: {}/{}, Train epoch:{}, batch index:{}, loss:{}'.format(\n",
    "                    q_stage + 1, len(quantization_percentages),\n",
    "                    epoch, batch_idx, loss.item()/logs_interval))\n",
    "                train_loss.append(loss.item())    \n",
    "\n",
    "quantize_conv_layer(net.conv1.weight, Tconv1, 1, quantization_precision)\n",
    "quantize_conv_layer(net.conv2.weight, Tconv2, 1, quantization_precision)\n",
    "quantize_conv_layer(net.conv3.weight, Tconv3, 1, quantization_precision)\n",
    "quantize_conv_layer(net.conv4.weight, Tconv4, 1, quantization_precision) \n",
    "\n",
    "torch.save(net, \"INQ.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(np.array([\n",
    "    [0.01, 0.02, -0.2, 0.04, 0.33],\n",
    "    [0.17, -0.42, -0.33, 0.02, -0.05], \n",
    "    [0.02, 0.83, -0.03, 0.03, 0.06],\n",
    "    [-0.9, 0.07, 0.11, 0.87, -0.36], \n",
    "    [-0.73, 0.41, 0.42, 0.39, 0.47]]))\n",
    "bit_length = 4\n",
    "n1, n2 = getBounderyExponents(W, bit_length)\n",
    "print(n1, n2)\n",
    "T = torch.Tensor(np.zeros_like(W))\n",
    "T = getQuantizationMask(W, 0.5, T)\n",
    "print(T)\n",
    "_, W = quantizeWeights(W, T, n1, n2)\n",
    "\n",
    "W = torch.Tensor(np.array([\n",
    "    [0.11, 0.04, -0.7, 0.19, -0.25],\n",
    "    [0.15, -0.5, -0.25, -0.09, -0.02],\n",
    "    [-0.02, 1, -0.06, 0.21, 0.15],\n",
    "    [-1, 0.27, -0.09, 1, -0.25],\n",
    "    [-0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "]))\n",
    "\n",
    "T = getQuantizationMask(W, 0.75, T)\n",
    "_, W = quantizeWeights(W, T, n1, n2)\n",
    "print(T)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_bits = 32\n",
    "mb_conv = 1.25e-7\n",
    "number_of_quantized_bits = 5\n",
    "number_of_weights = 0\n",
    "model_size = 0\n",
    "\n",
    "for param in net.parameters():\n",
    "    nr_weight_layer = 1\n",
    "    for n in param.size():\n",
    "        nr_weight_layer *= n\n",
    "    number_of_weights += nr_weight_layer\n",
    "\n",
    "model_size = number_of_weights*number_of_bits\n",
    "\n",
    "print(\"Numer of weights: \"+str(number_of_weights))\n",
    "print(\"Size needed to store the model with float32 precision: {:.3f} MB\".format(model_size*mb_conv))\n",
    "\n",
    "model_size = number_of_weights*number_of_quantized_bits\n",
    "print(\"Size needed to store the model with {} bits quantization: {:.3f} MB\".format(number_of_quantized_bits, model_size*mb_conv))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#time reduction with quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ -6.5783,  16.3643,  -2.1486,   4.3815,  -2.9944,  -6.8572,  11.7657,\n",
       "         -9.4291, -16.7526,  -8.7819])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=VGG7()\n",
    "print(net.fc3.weight.size())\n",
    "net.fc1.weight[:,0]\n",
    "\n",
    "W=torch.Tensor(np.random.randn(10))*10\n",
    "\n",
    "values,index=w_sort\n",
    "\n",
    "values\n",
    "index\n",
    "\n",
    "W.view(-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 1, 6, 7, 9, 5, 0, 3, 4, 2])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "5.0\n",
      "tensor([ -6.5783,  16.3643,  -2.1486,   4.3815,  -2.9944,  -6.8572,  11.7657,\n",
      "         -9.4291, -16.7526,  -8.7819]) tensor([0., 1., 0., 0., 0., 0., 1., 1., 1., 1.])\n",
      "tensor([16.7526, 16.3643, 11.7657,  9.4291,  8.7819,  6.8572,  6.5783,  4.3815,\n",
      "         2.9944,  2.1486])\n"
     ]
    }
   ],
   "source": [
    "w_abs=torch.abs(W)\n",
    "values_sort,indexes_sort=torch.sort(w_abs,descending=True)\n",
    "\n",
    "\n",
    "print(indexes_sort)\n",
    "\n",
    "number=5\n",
    "\n",
    "T= torch.Tensor(np.zeros_like(W))\n",
    "\n",
    "print(T)\n",
    "\n",
    "percentaje=0.5\n",
    "k=np.floor((len(w_abs)*percentaje)) #cate elem luam in fs de %\n",
    "print(k)\n",
    "\n",
    "getindexu=indexes_sort[:int(k)]\n",
    "\n",
    "\n",
    "for i in range(len(getindexu)):\n",
    "    T[getindexu[i]]=1\n",
    "\n",
    "    \n",
    "print(W,T)\n",
    "\n",
    "\n",
    "\n",
    "print(values_sort)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "tensor([0., 1., 0., 0., 0., 1., 1., 1., 1., 0.])\n",
      "tensor([8, 1, 6, 7, 9, 5, 0, 3, 4, 2])\n",
      "tensor([ -6.5783,  16.3643,  -2.1486,   4.3815,  -2.9944,  17.0000,  11.7657,\n",
      "         -9.4291, -16.7526,  -8.7819])\n"
     ]
    }
   ],
   "source": [
    "T= torch.Tensor(np.zeros_like(W))  #  DE AICI IEI BA\n",
    "\n",
    "def GetZaMask(In,percentage,T):\n",
    "    Ax= torch.Tensor(np.zeros_like(W))\n",
    "\n",
    "    for i in range(len(In)):\n",
    "    \n",
    "     Ax[i]=torch.where(T[i]==0,In[i],0)\n",
    "\n",
    "    w_abs=torch.abs(Ax)\n",
    "    values_sort,indexes_sort=torch.sort(w_abs,descending=True)\n",
    "\n",
    "    #k=np.floor((len(w_abs)*percentage)) #cate elem luam in fs de % si de c ati de 1 avem(ce procent am avut anterior)\n",
    "\n",
    "    count = torch.sum(T == 1)\n",
    "    \n",
    "    k=np.floor(len(ex)*perc)-count\n",
    "    k=int(k)\n",
    "\n",
    "    if k <0:\n",
    "      k=0\n",
    "\n",
    "    sorted_indexes=indexes_sort[:int(k)]\n",
    "\n",
    "    for i in range(len(sorted_indexes)):\n",
    "        \n",
    "        T[sorted_indexes[i]]=1\n",
    "\n",
    "    return T\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 1., 1., 0., 1., 1., 0.])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var= torch.Tensor ([2,0,1,5,7,12,3,90,11,4])\n",
    "T= torch.Tensor(np.zeros_like(var))\n",
    "\n",
    "GetZaMask(var,0.5,T)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var= torch.Tensor ([3,4,1,5,7,12,2,90,11,0])\n",
    "GetZaMask(var,1,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  9.,  3., 43.,  6., 22.,  3.,  0.,  0.,  0.])\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 1., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=torch.Tensor([11,9,3,43,6,22,3,0,89,76])\n",
    "B=torch.Tensor([1,0,0,0,0,0,0,0,1,1])\n",
    "Am= torch.Tensor(np.zeros_like(A))\n",
    "Z= torch.Tensor(np.zeros_like(A))\n",
    "\n",
    "\n",
    "for i in range(len(B)):\n",
    "    \n",
    "    Am[i]=torch.where(B[i]==0,A[i],0)\n",
    "\n",
    "print(Am)\n",
    "\n",
    "GetZaMask(Am,0.3,Z)\n",
    "# t ul ultim se aduna la ala initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# nr corect de elem ce trebe facute 1\n",
    "\n",
    "ex=torch.Tensor([1,0,0,0,0,0,1,1,1,1])\n",
    "count = torch.sum(ex == 1)\n",
    "perc=1\n",
    "elems=np.floor(len(ex)*perc)-count\n",
    "elems=int(elems)\n",
    "\n",
    "if elems <0:\n",
    "    elems=0\n",
    "\n",
    "\n",
    "print(elems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 1., 1., 1., 0., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbcefe7cc1b5c8a0efe7bce2a8e6d9d317c4b15f360801b153528886b54c3f98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
