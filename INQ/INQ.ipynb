{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand digit classifier \n",
    "---\n",
    "## Incremental network quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from skimage import io"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root='../', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root=\"../\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_set = [train_data[i] for i in range(50000)]\n",
    "validation_set = [train_data[i] for i in range(50000, 60000)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=64, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definig a VGG-7 inspired architecture model\n",
    "---\n",
    "Featuring 4 convolutional and 3 fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG7, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=\"same\", stride=1, bias=False)\n",
    "        \n",
    "        self.fc1 = nn.Linear(7*7*128, 512, bias=False)\n",
    "        self.fc2 = nn.Linear(512, 256, bias=False)\n",
    "        self.fc3 = nn.Linear(256, 10, bias=False)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, stride=2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, stride=2)  \n",
    "\n",
    "        x = x.view(-1, 7*7*128)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)  \n",
    "\n",
    "        x = F.log_softmax(x, dim=1)  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Quantization operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBounderyExponents(W, b):\n",
    "    s = torch.max(torch.abs(W)).item()\n",
    "    n1 = np.floor(np.log2(4*(s/3)))\n",
    "    n2 = n1 + 1 - (2**(b - 1))/2\n",
    "    return n1, n2\n",
    "\n",
    "def getQuantizationMask(W, percentage, T):\n",
    "    w = W.view(-1)\n",
    "    t = T.view(-1)\n",
    "    idx = t == 1\n",
    "\n",
    "    numberOfWeights = w.size(dim=0)\n",
    "    numberOfQWeights = int(percentage*numberOfWeights - t[idx].size(dim=0))\n",
    "\n",
    "    t_aux = torch.Tensor(np.ones_like(T)).view(-1)\n",
    "    w = w*(t_aux - t)\n",
    "    w = torch.abs(w)\n",
    "    sorted_w, indices_w = w.sort()\n",
    "    t[indices_w[-numberOfQWeights:]] = 1\n",
    "    \n",
    "    return t.view(T.size())\n",
    "\n",
    "def quantizeWeights(W, T, n1, n2):\n",
    "    T_aux = torch.Tensor(np.ones_like(T))\n",
    "    eps = 1e-6\n",
    "    W1 = W*(T_aux - T)\n",
    "    idx = W == 0\n",
    "    W.data[idx] = eps\n",
    "\n",
    "    closestExp = torch.floor(torch.log2(torch.abs(W*4/3)))\n",
    "    Q = W1 + torch.sign(W)*(2**closestExp)*T\n",
    "\n",
    "    idx = closestExp*T < n2\n",
    "    Q[idx] = 0\n",
    "    idx = closestExp*T > n1\n",
    "    Q[idx] = 2**n1\n",
    "\n",
    "    return closestExp, Q\n",
    "\n",
    "def quantize_conv_layer(W, T, percentage):\n",
    "    n = T.size(dim=0)\n",
    "    m = T.size(dim=1)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            T[i, j, :, :] = getQuantizationMask(W[i, j, :, :], percentage, T[i, j, :, :])\n",
    "            _, W.data[i, j, :, :] = quantizeWeights(W[i, j, :, :], T[i, j, :, :], 2, -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(np.array([\n",
    "    [0.01, 0.02, -0.2, 0.04, 0.33],\n",
    "    [0.17, -0.42, -0.33, 0.02, -0.05], \n",
    "    [0.02, 0.83, -0.03, 0.03, 0.06],\n",
    "    [-0.9, 0.07, 0.11, 0.87, -0.36], \n",
    "    [-0.73, 0.41, 0.42, 0.39, 0.47]]))\n",
    "bit_length = 4\n",
    "n1, n2 = getBounderyExponents(W, bit_length)\n",
    "print(n1, n2)\n",
    "T = torch.Tensor(np.zeros_like(W))\n",
    "T = getQuantizationMask(W, 0.5, T)\n",
    "print(T)\n",
    "_, W = quantizeWeights(W, T, n1, n2)\n",
    "\n",
    "W = torch.Tensor(np.array([\n",
    "    [0.11, 0.04, -0.7, 0.19, -0.25],\n",
    "    [0.15, -0.5, -0.25, -0.09, -0.02],\n",
    "    [-0.02, 1, -0.06, 0.21, 0.15],\n",
    "    [-1, 0.27, -0.09, 1, -0.25],\n",
    "    [-0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "]))\n",
    "\n",
    "T = getQuantizationMask(W, 0.75, T)\n",
    "_, W = quantizeWeights(W, T, n1, n2)\n",
    "print(T)\n",
    "print(W)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device initialization for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(\"../Baseline/baseline.pth\")\n",
    "net.to(device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-2, weight_decay=0)\n",
    "\n",
    "quantization_percentages = [0.5, 0.75, 0.875]\n",
    "\n",
    "epochs = 5\n",
    "logs_interval = 100\n",
    "number_of_iterations = 0\n",
    "train_loss = []\n",
    "\n",
    "Tconv1 = torch.zeros_like(net.conv1.weight)\n",
    "Tconv2 = torch.zeros_like(net.conv2.weight)\n",
    "Tconv3 = torch.zeros_like(net.conv3.weight)\n",
    "Tconv4 = torch.zeros_like(net.conv4.weight)\n",
    "\n",
    "net.train()\n",
    "\n",
    "for percentage in quantization_percentages:\n",
    "    # quantize layers\n",
    "    quantize_conv_layer(net.conv1.weight, Tconv1, percentage)\n",
    "    quantize_conv_layer(net.conv2.weight, Tconv2, percentage)\n",
    "    quantize_conv_layer(net.conv3.weight, Tconv3, percentage)\n",
    "    quantize_conv_layer(net.conv4.weight, Tconv4, percentage)\n",
    "    \n",
    "    # correct remaining weights by training\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # setam gradienti la 0\n",
    "            Tconv1_aux = torch.ones_like(Tconv1)\n",
    "            Tconv2_aux = torch.ones_like(Tconv2)\n",
    "            Tconv3_aux = torch.ones_like(Tconv3)\n",
    "            Tconv4_aux = torch.ones_like(Tconv4)\n",
    "            net.conv1.weight.grad = net.conv1.weight.grad*(Tconv1_aux - Tconv1)\n",
    "            net.conv2.weight.grad = net.conv2.weight.grad*(Tconv2_aux - Tconv2)\n",
    "            net.conv3.weight.grad = net.conv3.weight.grad*(Tconv3_aux - Tconv3)\n",
    "            net.conv4.weight.grad = net.conv4.weight.grad*(Tconv4_aux - Tconv4)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            iteration = iteration + 1\n",
    "            if iteration % logs_interval == 0:\n",
    "                print('Train epoch:{}, batch index:{}, loss:{}'.format(epoch, batch_idx, loss.item()/logs_interval))\n",
    "                train_loss.append(loss.item())    \n",
    "\n",
    "quantize_conv_layer(net.conv1.weight, Tconv1, 1)\n",
    "quantize_conv_layer(net.conv2.weight, Tconv2, 1)\n",
    "quantize_conv_layer(net.conv3.weight, Tconv3, 1)\n",
    "quantize_conv_layer(net.conv4.weight, Tconv4, 1)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "loss = 0\n",
    "correct = 0\n",
    "\n",
    "confusion_matrix = np.zeros((10, 10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = net(data)\n",
    "        loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        confusion_matrix[pred.cpu()[:, 0], target.cpu()] += 1\n",
    "\n",
    "    loss = loss/len(test_loader.dataset)\n",
    "    accuracy = 100.*correct/len(test_loader.dataset)\n",
    "    print('Test set average loss: {}, accuracy: {}%'.format(loss, accuracy))\n",
    "\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = VGG7()\n",
    "# net.to(device)\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.1, weight_decay=0)\n",
    "# wei = torch.clone(net.conv1.weight)\n",
    "# Tconv1 = torch.zeros_like(net.conv1.weight)\n",
    "# quantize_conv_layer(net.conv1.weight, Tconv1, 0.5)\n",
    "# wei_q = torch.clone(net.conv1.weight)\n",
    "\n",
    "# net.train()\n",
    "\n",
    "# sample, lable = train_data[0]\n",
    "# lable = torch.Tensor([lable]).long()\n",
    "# optimizer.zero_grad()\n",
    "# output = net(sample)\n",
    "# loss = F.nll_loss(output, lable)\n",
    "# loss.backward()\n",
    "\n",
    "# Tconv1_aux = torch.ones_like(Tconv1)\n",
    "# net.conv1.weight.grad = net.conv1.weight.grad*(Tconv1_aux - Tconv1)\n",
    "# optimizer.step()\n",
    "\n",
    "\n",
    "# print(wei[0, 0, :, :])\n",
    "# print(Tconv1[0, 0, :, :])\n",
    "# print(wei_q[0, 0, :, :])\n",
    "# print(net.conv1.weight[0, 0, :, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da9c6a45712c6b513c4a80738271fc65c25531d6aa017375b96d9ba6472bbc68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
