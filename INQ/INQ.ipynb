{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand digit classifier\n",
    "---\n",
    "## Incremental network quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root='../', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root=\"../\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_set = [train_data[i] for i in range(50000)]\n",
    "validation_set = [train_data[i] for i in range(50000, 60000)]\n",
    "# test_set = [test_set[i] for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=64, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definig a VGG-7 inspired architecture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG7, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=\"same\", stride=1, bias=False)\n",
    "        \n",
    "        self.fc1 = nn.Linear(7*7*128, 512, bias=False)\n",
    "        self.fc2 = nn.Linear(512, 256, bias=False)\n",
    "        self.fc3 = nn.Linear(256, 10, bias=False)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, stride=2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, stride=2)  \n",
    "\n",
    "        x = x.view(-1, 7*7*128)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)  \n",
    "\n",
    "        x = F.log_softmax(x, dim=1)  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device initialization for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch:0, batch index:0, loss:0.01151275634765625\n",
      "Train epoch:0, batch index:1, loss:0.011513707637786865\n",
      "Train epoch:0, batch index:2, loss:0.011514501571655273\n",
      "Train epoch:0, batch index:3, loss:0.011514372825622558\n",
      "Train epoch:0, batch index:4, loss:0.011511448621749878\n",
      "Train epoch:0, batch index:5, loss:0.011510716676712036\n",
      "Train epoch:0, batch index:6, loss:0.011512809991836548\n",
      "Train epoch:0, batch index:7, loss:0.011513093709945679\n",
      "Train epoch:0, batch index:8, loss:0.01151338815689087\n",
      "Train epoch:0, batch index:9, loss:0.011514453887939454\n",
      "Train epoch:0, batch index:10, loss:0.011512792110443116\n",
      "Train epoch:0, batch index:11, loss:0.011513731479644775\n",
      "Train epoch:0, batch index:12, loss:0.01151130199432373\n",
      "Train epoch:0, batch index:13, loss:0.0115126371383667\n",
      "Train epoch:0, batch index:14, loss:0.0115156090259552\n",
      "Train epoch:0, batch index:15, loss:0.011512694358825683\n",
      "Train epoch:0, batch index:16, loss:0.011512682437896729\n",
      "Train epoch:0, batch index:17, loss:0.011515322923660278\n",
      "Train epoch:0, batch index:18, loss:0.01151421308517456\n",
      "Train epoch:0, batch index:19, loss:0.011511105298995971\n",
      "Train epoch:0, batch index:20, loss:0.011512117385864258\n",
      "Train epoch:0, batch index:21, loss:0.011514334678649903\n",
      "Train epoch:0, batch index:22, loss:0.011512504816055298\n",
      "Train epoch:0, batch index:23, loss:0.011511005163192749\n",
      "Train epoch:0, batch index:24, loss:0.011512024402618408\n",
      "Train epoch:0, batch index:25, loss:0.011513288021087647\n",
      "Train epoch:0, batch index:26, loss:0.011512012481689453\n",
      "Train epoch:0, batch index:27, loss:0.01151039481163025\n",
      "Train epoch:0, batch index:28, loss:0.011512945890426635\n",
      "Train epoch:0, batch index:29, loss:0.011512868404388428\n",
      "Train epoch:0, batch index:30, loss:0.011513237953186034\n",
      "Train epoch:0, batch index:31, loss:0.01151113748550415\n",
      "Train epoch:0, batch index:32, loss:0.011512521505355835\n",
      "Train epoch:0, batch index:33, loss:0.011512240171432495\n",
      "Train epoch:0, batch index:34, loss:0.011513670682907104\n",
      "Train epoch:0, batch index:35, loss:0.01151226282119751\n",
      "Train epoch:0, batch index:36, loss:0.01151189923286438\n",
      "Train epoch:0, batch index:37, loss:0.011511234045028686\n",
      "Train epoch:0, batch index:38, loss:0.011513569355010987\n",
      "Train epoch:0, batch index:39, loss:0.011511167287826538\n",
      "Train epoch:0, batch index:40, loss:0.011510425806045532\n",
      "Train epoch:0, batch index:41, loss:0.011512361764907837\n",
      "Train epoch:0, batch index:42, loss:0.011513512134552002\n",
      "Train epoch:0, batch index:43, loss:0.011513283252716064\n",
      "Train epoch:0, batch index:44, loss:0.011512218713760377\n",
      "Train epoch:0, batch index:45, loss:0.01151565670967102\n",
      "Train epoch:0, batch index:46, loss:0.011513068675994872\n",
      "Train epoch:0, batch index:47, loss:0.011512161493301391\n",
      "Train epoch:0, batch index:48, loss:0.011509913206100463\n",
      "Train epoch:0, batch index:49, loss:0.011512596607208252\n",
      "Train epoch:0, batch index:50, loss:0.011510446071624755\n",
      "Train epoch:0, batch index:51, loss:0.01151398777961731\n",
      "Train epoch:0, batch index:52, loss:0.011510928869247436\n",
      "Train epoch:0, batch index:53, loss:0.011512550115585328\n",
      "Train epoch:0, batch index:54, loss:0.01151345729827881\n",
      "Train epoch:0, batch index:55, loss:0.011512322425842285\n",
      "Train epoch:0, batch index:56, loss:0.011514171361923217\n",
      "Train epoch:0, batch index:57, loss:0.011512786149978638\n",
      "Train epoch:0, batch index:58, loss:0.011512721776962281\n",
      "Train epoch:0, batch index:59, loss:0.011511790752410888\n",
      "Train epoch:0, batch index:60, loss:0.011509406566619872\n",
      "Train epoch:0, batch index:61, loss:0.011514036655426026\n",
      "Train epoch:0, batch index:62, loss:0.011510447263717652\n",
      "Train epoch:0, batch index:63, loss:0.011511572599411012\n",
      "Train epoch:0, batch index:64, loss:0.011511058807373046\n",
      "Train epoch:0, batch index:65, loss:0.011509966850280762\n",
      "Train epoch:0, batch index:66, loss:0.011511400938034058\n",
      "Train epoch:0, batch index:67, loss:0.011510604619979858\n",
      "Train epoch:0, batch index:68, loss:0.011512460708618165\n",
      "Train epoch:0, batch index:69, loss:0.01151132583618164\n",
      "Train epoch:0, batch index:70, loss:0.011512897014617919\n",
      "Train epoch:0, batch index:71, loss:0.011511323451995849\n",
      "Train epoch:0, batch index:72, loss:0.01151145100593567\n",
      "Train epoch:0, batch index:73, loss:0.01151618242263794\n",
      "Train epoch:0, batch index:74, loss:0.011512734889984132\n",
      "Train epoch:0, batch index:75, loss:0.011512529850006104\n",
      "Train epoch:0, batch index:76, loss:0.01151341676712036\n",
      "Train epoch:0, batch index:77, loss:0.011513880491256713\n",
      "Train epoch:0, batch index:78, loss:0.011510813236236572\n",
      "Train epoch:0, batch index:79, loss:0.011510988473892212\n",
      "Train epoch:0, batch index:80, loss:0.011511749029159546\n",
      "Train epoch:0, batch index:81, loss:0.01151139497756958\n",
      "Train epoch:0, batch index:82, loss:0.011513618230819702\n",
      "Train epoch:0, batch index:83, loss:0.011510344743728638\n",
      "Train epoch:0, batch index:84, loss:0.011512914896011353\n",
      "Train epoch:0, batch index:85, loss:0.011510189771652222\n",
      "Train epoch:0, batch index:86, loss:0.011513252258300782\n",
      "Train epoch:0, batch index:87, loss:0.011513543128967286\n",
      "Train epoch:0, batch index:88, loss:0.011512610912322998\n",
      "Train epoch:0, batch index:89, loss:0.011514219045639039\n",
      "Train epoch:0, batch index:90, loss:0.011512088775634765\n",
      "Train epoch:0, batch index:91, loss:0.011511808633804322\n",
      "Train epoch:0, batch index:92, loss:0.011511976718902589\n",
      "Train epoch:0, batch index:93, loss:0.01150962233543396\n",
      "Train epoch:0, batch index:94, loss:0.011513427495956421\n",
      "Train epoch:0, batch index:95, loss:0.011514136791229248\n",
      "Train epoch:0, batch index:96, loss:0.011513116359710694\n",
      "Train epoch:0, batch index:97, loss:0.011514546871185303\n",
      "Train epoch:0, batch index:98, loss:0.011512176990509033\n",
      "Train epoch:0, batch index:99, loss:0.011512867212295531\n",
      "Train epoch:0, batch index:100, loss:0.011513867378234864\n",
      "Train epoch:0, batch index:101, loss:0.01151418685913086\n",
      "Train epoch:0, batch index:102, loss:0.011514285802841187\n",
      "Train epoch:0, batch index:103, loss:0.011511400938034058\n",
      "Train epoch:0, batch index:104, loss:0.011512236595153809\n",
      "Train epoch:0, batch index:105, loss:0.01151253581047058\n",
      "Train epoch:0, batch index:106, loss:0.01151219367980957\n",
      "Train epoch:0, batch index:107, loss:0.01151391863822937\n",
      "Train epoch:0, batch index:108, loss:0.011512236595153809\n",
      "Train epoch:0, batch index:109, loss:0.011513030529022217\n",
      "Train epoch:0, batch index:110, loss:0.011510860919952393\n",
      "Train epoch:0, batch index:111, loss:0.01151128053665161\n",
      "Train epoch:0, batch index:112, loss:0.011512584686279297\n",
      "Train epoch:0, batch index:113, loss:0.011513646841049194\n",
      "Train epoch:0, batch index:114, loss:0.01151119828224182\n",
      "Train epoch:0, batch index:115, loss:0.011512224674224853\n",
      "Train epoch:0, batch index:116, loss:0.011511558294296264\n",
      "Train epoch:0, batch index:117, loss:0.011512410640716553\n",
      "Train epoch:0, batch index:118, loss:0.011511993408203126\n",
      "Train epoch:0, batch index:119, loss:0.011511855125427246\n",
      "Train epoch:0, batch index:120, loss:0.011512925624847412\n",
      "Train epoch:0, batch index:121, loss:0.011510913372039794\n",
      "Train epoch:0, batch index:122, loss:0.01151193380355835\n",
      "Train epoch:0, batch index:123, loss:0.011509891748428345\n",
      "Train epoch:0, batch index:124, loss:0.011511893272399902\n",
      "Train epoch:0, batch index:125, loss:0.01151326298713684\n",
      "Train epoch:0, batch index:126, loss:0.011513142585754395\n",
      "Train epoch:0, batch index:127, loss:0.011511514186859131\n",
      "Train epoch:0, batch index:128, loss:0.011511648893356324\n",
      "Train epoch:0, batch index:129, loss:0.011511391401290894\n",
      "Train epoch:0, batch index:130, loss:0.011512897014617919\n",
      "Train epoch:0, batch index:131, loss:0.011510519981384278\n",
      "Train epoch:0, batch index:132, loss:0.011510936021804809\n",
      "Train epoch:0, batch index:133, loss:0.011513046026229858\n",
      "Train epoch:0, batch index:134, loss:0.01151224136352539\n",
      "Train epoch:0, batch index:135, loss:0.011512761116027831\n",
      "Train epoch:0, batch index:136, loss:0.011511154174804687\n",
      "Train epoch:0, batch index:137, loss:0.011512722969055176\n",
      "Train epoch:0, batch index:138, loss:0.011513178348541259\n",
      "Train epoch:0, batch index:139, loss:0.011512432098388672\n",
      "Train epoch:0, batch index:140, loss:0.011511870622634889\n",
      "Train epoch:0, batch index:141, loss:0.011512898206710816\n",
      "Train epoch:0, batch index:142, loss:0.011511069536209107\n",
      "Train epoch:0, batch index:143, loss:0.011512274742126466\n",
      "Train epoch:0, batch index:144, loss:0.011512529850006104\n",
      "Train epoch:0, batch index:145, loss:0.011513323783874511\n",
      "Train epoch:0, batch index:146, loss:0.011512290239334106\n",
      "Train epoch:0, batch index:147, loss:0.011511234045028686\n",
      "Train epoch:0, batch index:148, loss:0.011511176824569702\n",
      "Train epoch:0, batch index:149, loss:0.011511992216110229\n",
      "Train epoch:0, batch index:150, loss:0.011512830257415771\n",
      "Train epoch:0, batch index:151, loss:0.011509432792663574\n",
      "Train epoch:0, batch index:152, loss:0.011513410806655884\n",
      "Train epoch:0, batch index:153, loss:0.011510850191116332\n",
      "Train epoch:0, batch index:154, loss:0.011510336399078369\n",
      "Train epoch:0, batch index:155, loss:0.011513181924819947\n",
      "Train epoch:0, batch index:156, loss:0.011509735584259034\n",
      "Train epoch:0, batch index:157, loss:0.01151208758354187\n",
      "Train epoch:0, batch index:158, loss:0.011511781215667725\n",
      "Train epoch:0, batch index:159, loss:0.011512721776962281\n",
      "Train epoch:0, batch index:160, loss:0.011509895324707031\n",
      "Train epoch:0, batch index:161, loss:0.011512527465820313\n",
      "Train epoch:0, batch index:162, loss:0.011513427495956421\n",
      "Train epoch:0, batch index:163, loss:0.011516505479812622\n",
      "Train epoch:0, batch index:164, loss:0.011514290571212768\n",
      "Train epoch:0, batch index:165, loss:0.01151445746421814\n",
      "Train epoch:0, batch index:166, loss:0.011512091159820556\n",
      "Train epoch:0, batch index:167, loss:0.011512203216552734\n",
      "Train epoch:0, batch index:168, loss:0.011511613130569458\n",
      "Train epoch:0, batch index:169, loss:0.011511902809143066\n",
      "Train epoch:0, batch index:170, loss:0.011513060331344605\n",
      "Train epoch:0, batch index:171, loss:0.011510895490646362\n",
      "Train epoch:0, batch index:172, loss:0.011514668464660644\n",
      "Train epoch:0, batch index:173, loss:0.01151563048362732\n",
      "Train epoch:0, batch index:174, loss:0.011512531042098999\n",
      "Train epoch:0, batch index:175, loss:0.011512371301651001\n",
      "Train epoch:0, batch index:176, loss:0.011512603759765625\n",
      "Train epoch:0, batch index:177, loss:0.011511240005493164\n",
      "Train epoch:0, batch index:178, loss:0.011513841152191163\n",
      "Train epoch:0, batch index:179, loss:0.011511644124984741\n",
      "Train epoch:0, batch index:180, loss:0.011512441635131836\n",
      "Train epoch:0, batch index:181, loss:0.011510792970657348\n",
      "Train epoch:0, batch index:182, loss:0.0115154767036438\n",
      "Train epoch:0, batch index:183, loss:0.011513140201568604\n",
      "Train epoch:0, batch index:184, loss:0.011513372659683227\n",
      "Train epoch:0, batch index:185, loss:0.011513907909393311\n",
      "Train epoch:0, batch index:186, loss:0.011514942646026611\n",
      "Train epoch:0, batch index:187, loss:0.011509921550750733\n",
      "Train epoch:0, batch index:188, loss:0.011512554883956909\n",
      "Train epoch:0, batch index:189, loss:0.011511893272399902\n",
      "Train epoch:0, batch index:190, loss:0.011513110399246216\n",
      "Train epoch:0, batch index:191, loss:0.011512118577957153\n",
      "Train epoch:0, batch index:192, loss:0.011513235569000245\n",
      "Train epoch:0, batch index:193, loss:0.01151124358177185\n",
      "Train epoch:0, batch index:194, loss:0.011511448621749878\n",
      "Train epoch:0, batch index:195, loss:0.011513164043426513\n",
      "Train epoch:0, batch index:196, loss:0.011512891054153443\n",
      "Train epoch:0, batch index:197, loss:0.011512509584426879\n",
      "Train epoch:0, batch index:198, loss:0.011512753963470458\n",
      "Train epoch:0, batch index:200, loss:0.011512434482574463\n",
      "Train epoch:0, batch index:201, loss:0.011510008573532104\n",
      "Train epoch:0, batch index:202, loss:0.011514102220535278\n",
      "Train epoch:0, batch index:203, loss:0.011513395309448242\n",
      "Train epoch:0, batch index:204, loss:0.011513621807098388\n",
      "Train epoch:0, batch index:205, loss:0.011513772010803223\n",
      "Train epoch:0, batch index:206, loss:0.011512625217437743\n",
      "Train epoch:0, batch index:207, loss:0.011511329412460327\n",
      "Train epoch:0, batch index:208, loss:0.011513022184371948\n",
      "Train epoch:0, batch index:209, loss:0.011511392593383789\n",
      "Train epoch:0, batch index:210, loss:0.011511886119842529\n",
      "Train epoch:0, batch index:211, loss:0.011512660980224609\n",
      "Train epoch:0, batch index:212, loss:0.011510142087936402\n",
      "Train epoch:0, batch index:213, loss:0.01151235818862915\n",
      "Train epoch:0, batch index:214, loss:0.01151134490966797\n",
      "Train epoch:0, batch index:215, loss:0.011513210535049438\n",
      "Train epoch:0, batch index:216, loss:0.011510186195373535\n",
      "Train epoch:0, batch index:217, loss:0.011512696743011475\n",
      "Train epoch:0, batch index:218, loss:0.011511385440826416\n",
      "Train epoch:0, batch index:219, loss:0.011513309478759766\n",
      "Train epoch:0, batch index:220, loss:0.011512939929962157\n",
      "Train epoch:0, batch index:221, loss:0.011511062383651733\n",
      "Train epoch:0, batch index:222, loss:0.011514630317687988\n",
      "Train epoch:0, batch index:223, loss:0.011511462926864623\n",
      "Train epoch:0, batch index:224, loss:0.011508967876434326\n",
      "Train epoch:0, batch index:225, loss:0.011510831117630006\n",
      "Train epoch:0, batch index:226, loss:0.011513900756835938\n",
      "Train epoch:0, batch index:227, loss:0.011512860059738159\n",
      "Train epoch:0, batch index:228, loss:0.01151214599609375\n",
      "Train epoch:0, batch index:229, loss:0.011512130498886108\n",
      "Train epoch:0, batch index:230, loss:0.011512287855148316\n",
      "Train epoch:0, batch index:231, loss:0.011510993242263795\n",
      "Train epoch:0, batch index:232, loss:0.011510813236236572\n",
      "Train epoch:0, batch index:233, loss:0.011510462760925292\n",
      "Train epoch:0, batch index:234, loss:0.011513528823852539\n",
      "Train epoch:0, batch index:235, loss:0.011511552333831786\n",
      "Train epoch:0, batch index:236, loss:0.011509805917739868\n",
      "Train epoch:0, batch index:237, loss:0.011508731842041016\n",
      "Train epoch:0, batch index:238, loss:0.011513375043869019\n",
      "Train epoch:0, batch index:239, loss:0.011510435342788696\n",
      "Train epoch:0, batch index:240, loss:0.011511934995651245\n",
      "Train epoch:0, batch index:241, loss:0.01151288390159607\n",
      "Train epoch:0, batch index:242, loss:0.011512277126312255\n",
      "Train epoch:0, batch index:243, loss:0.011510300636291503\n",
      "Train epoch:0, batch index:244, loss:0.01151211142539978\n",
      "Train epoch:0, batch index:245, loss:0.011510617733001709\n",
      "Train epoch:0, batch index:246, loss:0.011511799097061157\n",
      "Train epoch:0, batch index:247, loss:0.011514158248901367\n",
      "Train epoch:0, batch index:248, loss:0.011510086059570313\n",
      "Train epoch:0, batch index:249, loss:0.01151115894317627\n",
      "Train epoch:0, batch index:250, loss:0.011514917612075806\n",
      "Train epoch:0, batch index:251, loss:0.0115138578414917\n",
      "Train epoch:0, batch index:252, loss:0.011510848999023438\n",
      "Train epoch:0, batch index:253, loss:0.011511771678924561\n",
      "Train epoch:0, batch index:254, loss:0.01151398777961731\n",
      "Train epoch:0, batch index:255, loss:0.011510032415390014\n",
      "Train epoch:0, batch index:256, loss:0.01151193618774414\n",
      "Train epoch:0, batch index:257, loss:0.011512490510940552\n",
      "Train epoch:0, batch index:258, loss:0.011511576175689698\n",
      "Train epoch:0, batch index:259, loss:0.011510798931121826\n",
      "Train epoch:0, batch index:260, loss:0.011512205600738526\n",
      "Train epoch:0, batch index:261, loss:0.011512918472290039\n",
      "Train epoch:0, batch index:262, loss:0.011512750387191772\n",
      "Train epoch:0, batch index:263, loss:0.011512205600738526\n",
      "Train epoch:0, batch index:264, loss:0.011512942314147949\n",
      "Train epoch:0, batch index:265, loss:0.011510108709335326\n",
      "Train epoch:0, batch index:266, loss:0.011511474847793579\n",
      "Train epoch:0, batch index:267, loss:0.011510924100875855\n",
      "Train epoch:0, batch index:268, loss:0.011514050960540772\n",
      "Train epoch:0, batch index:269, loss:0.011509003639221192\n",
      "Train epoch:0, batch index:270, loss:0.011511416435241699\n",
      "Train epoch:0, batch index:271, loss:0.011511180400848389\n",
      "Train epoch:0, batch index:272, loss:0.011512032747268676\n",
      "Train epoch:0, batch index:273, loss:0.011511651277542114\n",
      "Train epoch:0, batch index:274, loss:0.011513751745224\n",
      "Train epoch:0, batch index:275, loss:0.011510823965072631\n",
      "Train epoch:0, batch index:276, loss:0.011512991189956665\n",
      "Train epoch:0, batch index:277, loss:0.011512655019760131\n",
      "Train epoch:0, batch index:278, loss:0.011509044170379639\n",
      "Train epoch:0, batch index:279, loss:0.011511555910110473\n",
      "Train epoch:0, batch index:280, loss:0.01151132106781006\n",
      "Train epoch:0, batch index:281, loss:0.01151113748550415\n",
      "Train epoch:0, batch index:282, loss:0.011511650085449219\n",
      "Train epoch:0, batch index:283, loss:0.011512365341186524\n",
      "Train epoch:0, batch index:284, loss:0.011509948968887329\n",
      "Train epoch:0, batch index:285, loss:0.011510548591613769\n",
      "Train epoch:0, batch index:286, loss:0.011511309146881104\n",
      "Train epoch:0, batch index:287, loss:0.011510764360427856\n",
      "Train epoch:0, batch index:288, loss:0.011511671543121337\n",
      "Train epoch:0, batch index:289, loss:0.011512076854705811\n",
      "Train epoch:0, batch index:290, loss:0.011513032913208009\n",
      "Train epoch:0, batch index:291, loss:0.011511584520339966\n",
      "Train epoch:0, batch index:292, loss:0.011512309312820435\n",
      "Train epoch:0, batch index:293, loss:0.011511116027832032\n",
      "Train epoch:0, batch index:294, loss:0.011512634754180908\n",
      "Train epoch:0, batch index:295, loss:0.01151378035545349\n",
      "Train epoch:0, batch index:296, loss:0.011510142087936402\n",
      "Train epoch:0, batch index:297, loss:0.011511342525482178\n",
      "Train epoch:0, batch index:298, loss:0.01151029348373413\n",
      "Train epoch:0, batch index:299, loss:0.011512172222137452\n",
      "Train epoch:0, batch index:300, loss:0.011514382362365723\n",
      "Train epoch:0, batch index:301, loss:0.011509644985198974\n",
      "Train epoch:0, batch index:302, loss:0.011511945724487304\n",
      "Train epoch:0, batch index:303, loss:0.011512706279754639\n",
      "Train epoch:0, batch index:304, loss:0.01151055932044983\n",
      "Train epoch:0, batch index:305, loss:0.01151142954826355\n",
      "Train epoch:0, batch index:306, loss:0.011510884761810303\n",
      "Train epoch:0, batch index:307, loss:0.011513350009918212\n",
      "Train epoch:0, batch index:308, loss:0.01151156187057495\n",
      "Train epoch:0, batch index:309, loss:0.01150818943977356\n",
      "Train epoch:0, batch index:310, loss:0.011513921022415162\n",
      "Train epoch:0, batch index:311, loss:0.011512153148651123\n",
      "Train epoch:0, batch index:312, loss:0.011512738466262818\n",
      "Train epoch:0, batch index:313, loss:0.011511733531951904\n",
      "Train epoch:0, batch index:314, loss:0.011513363122940063\n",
      "Train epoch:0, batch index:315, loss:0.011510194540023803\n",
      "Train epoch:0, batch index:316, loss:0.011513322591781616\n",
      "Train epoch:0, batch index:317, loss:0.011511002779006957\n",
      "Train epoch:0, batch index:318, loss:0.011514136791229248\n",
      "Train epoch:0, batch index:319, loss:0.011512933969497681\n",
      "Train epoch:0, batch index:320, loss:0.011515095233917236\n",
      "Train epoch:0, batch index:321, loss:0.011512070894241333\n",
      "Train epoch:0, batch index:322, loss:0.011511307954788209\n",
      "Train epoch:0, batch index:323, loss:0.011513224840164184\n",
      "Train epoch:0, batch index:324, loss:0.011513721942901612\n",
      "Train epoch:0, batch index:325, loss:0.011511108875274657\n",
      "Train epoch:0, batch index:326, loss:0.011510614156723022\n",
      "Train epoch:0, batch index:327, loss:0.011509991884231567\n",
      "Train epoch:0, batch index:328, loss:0.011511595249176025\n",
      "Train epoch:0, batch index:329, loss:0.011512734889984132\n",
      "Train epoch:0, batch index:330, loss:0.01151117444038391\n",
      "Train epoch:0, batch index:331, loss:0.01150998830795288\n",
      "Train epoch:0, batch index:332, loss:0.011512292623519898\n",
      "Train epoch:0, batch index:333, loss:0.011510142087936402\n",
      "Train epoch:0, batch index:334, loss:0.011511474847793579\n",
      "Train epoch:0, batch index:335, loss:0.011510666608810425\n",
      "Train epoch:0, batch index:336, loss:0.011511462926864623\n",
      "Train epoch:0, batch index:337, loss:0.011512596607208252\n",
      "Train epoch:0, batch index:338, loss:0.011510457992553711\n",
      "Train epoch:0, batch index:339, loss:0.011512590646743774\n",
      "Train epoch:0, batch index:340, loss:0.011510695219039918\n",
      "Train epoch:0, batch index:341, loss:0.011513253450393677\n",
      "Train epoch:0, batch index:342, loss:0.011509431600570679\n",
      "Train epoch:0, batch index:343, loss:0.011510595083236694\n",
      "Train epoch:0, batch index:344, loss:0.011510437726974488\n",
      "Train epoch:0, batch index:345, loss:0.01150954008102417\n",
      "Train epoch:0, batch index:346, loss:0.011511881351470948\n",
      "Train epoch:0, batch index:347, loss:0.01150983691215515\n",
      "Train epoch:0, batch index:348, loss:0.011511651277542114\n",
      "Train epoch:0, batch index:349, loss:0.011511526107788085\n",
      "Train epoch:0, batch index:350, loss:0.011510699987411499\n",
      "Train epoch:0, batch index:351, loss:0.011511914730072022\n",
      "Train epoch:0, batch index:352, loss:0.011511918306350708\n",
      "Train epoch:0, batch index:353, loss:0.011511490345001221\n",
      "Train epoch:0, batch index:354, loss:0.011511245965957642\n",
      "Train epoch:0, batch index:355, loss:0.011511241197586059\n",
      "Train epoch:0, batch index:356, loss:0.01151414155960083\n",
      "Train epoch:0, batch index:357, loss:0.011511127948760986\n",
      "Train epoch:0, batch index:358, loss:0.01151483178138733\n",
      "Train epoch:0, batch index:359, loss:0.011512715816497803\n",
      "Train epoch:0, batch index:360, loss:0.011512789726257324\n",
      "Train epoch:0, batch index:361, loss:0.011511664390563964\n",
      "Train epoch:0, batch index:362, loss:0.011512526273727418\n",
      "Train epoch:0, batch index:363, loss:0.011510869264602661\n",
      "Train epoch:0, batch index:364, loss:0.011511151790618896\n",
      "Train epoch:0, batch index:365, loss:0.011511083841323853\n",
      "Train epoch:0, batch index:366, loss:0.011513228416442872\n",
      "Train epoch:0, batch index:367, loss:0.01150903582572937\n",
      "Train epoch:0, batch index:368, loss:0.011512506008148193\n",
      "Train epoch:0, batch index:369, loss:0.011511379480361938\n",
      "Train epoch:0, batch index:370, loss:0.011511176824569702\n",
      "Train epoch:0, batch index:371, loss:0.011511600017547608\n",
      "Train epoch:0, batch index:372, loss:0.01151258945465088\n",
      "Train epoch:0, batch index:373, loss:0.011511559486389161\n",
      "Train epoch:0, batch index:374, loss:0.011512227058410644\n",
      "Train epoch:0, batch index:375, loss:0.011512395143508911\n",
      "Train epoch:0, batch index:376, loss:0.011512353420257568\n",
      "Train epoch:0, batch index:377, loss:0.01151263952255249\n",
      "Train epoch:0, batch index:378, loss:0.011511811017990113\n",
      "Train epoch:0, batch index:379, loss:0.011512280702590942\n",
      "Train epoch:0, batch index:380, loss:0.011511645317077636\n",
      "Train epoch:0, batch index:381, loss:0.011511318683624268\n",
      "Train epoch:0, batch index:382, loss:0.011512546539306641\n",
      "Train epoch:0, batch index:383, loss:0.0115098774433136\n",
      "Train epoch:0, batch index:384, loss:0.011511538028717041\n",
      "Train epoch:0, batch index:385, loss:0.011512093544006348\n",
      "Train epoch:0, batch index:386, loss:0.011511313915252685\n",
      "Train epoch:0, batch index:387, loss:0.011511154174804687\n",
      "Train epoch:0, batch index:388, loss:0.011509273052215576\n",
      "Train epoch:0, batch index:389, loss:0.011509814262390138\n",
      "Train epoch:0, batch index:390, loss:0.011514157056808472\n",
      "Train epoch:0, batch index:391, loss:0.011509335041046143\n",
      "Train epoch:0, batch index:392, loss:0.01151288390159607\n",
      "Train epoch:0, batch index:393, loss:0.01151111364364624\n",
      "Train epoch:0, batch index:394, loss:0.011511691808700562\n",
      "Train epoch:0, batch index:395, loss:0.011512384414672852\n",
      "Train epoch:0, batch index:396, loss:0.011510103940963745\n",
      "Train epoch:0, batch index:397, loss:0.011508786678314209\n",
      "Train epoch:0, batch index:398, loss:0.011509455442428589\n",
      "Train epoch:0, batch index:400, loss:0.011509797573089599\n",
      "Train epoch:0, batch index:401, loss:0.011510698795318604\n",
      "Train epoch:0, batch index:402, loss:0.011511460542678834\n",
      "Train epoch:0, batch index:403, loss:0.011507737636566161\n",
      "Train epoch:0, batch index:404, loss:0.011511242389678955\n",
      "Train epoch:0, batch index:405, loss:0.011511963605880738\n",
      "Train epoch:0, batch index:406, loss:0.011510028839111328\n",
      "Train epoch:0, batch index:407, loss:0.01150985598564148\n",
      "Train epoch:0, batch index:408, loss:0.011512433290481567\n",
      "Train epoch:0, batch index:409, loss:0.011512671709060668\n",
      "Train epoch:0, batch index:410, loss:0.011513960361480714\n",
      "Train epoch:0, batch index:411, loss:0.011510233879089355\n",
      "Train epoch:0, batch index:412, loss:0.011511421203613282\n",
      "Train epoch:0, batch index:413, loss:0.011510591506958007\n",
      "Train epoch:0, batch index:414, loss:0.011510004997253418\n",
      "Train epoch:0, batch index:415, loss:0.011512038707733154\n",
      "Train epoch:0, batch index:416, loss:0.011513683795928955\n",
      "Train epoch:0, batch index:417, loss:0.011511971950531006\n",
      "Train epoch:0, batch index:418, loss:0.01151168942451477\n",
      "Train epoch:0, batch index:419, loss:0.011512949466705322\n",
      "Train epoch:0, batch index:420, loss:0.01151124358177185\n",
      "Train epoch:0, batch index:421, loss:0.01151214838027954\n",
      "Train epoch:0, batch index:422, loss:0.011513912677764892\n",
      "Train epoch:0, batch index:423, loss:0.01151010513305664\n",
      "Train epoch:0, batch index:424, loss:0.011511757373809814\n",
      "Train epoch:0, batch index:425, loss:0.011511925458908081\n",
      "Train epoch:0, batch index:426, loss:0.011512655019760131\n",
      "Train epoch:0, batch index:427, loss:0.011510752439498902\n",
      "Train epoch:0, batch index:428, loss:0.011511688232421874\n",
      "Train epoch:0, batch index:429, loss:0.011513164043426513\n",
      "Train epoch:0, batch index:430, loss:0.01151236891746521\n",
      "Train epoch:0, batch index:431, loss:0.01150882601737976\n",
      "Train epoch:0, batch index:432, loss:0.011512439250946045\n",
      "Train epoch:0, batch index:433, loss:0.011511600017547608\n",
      "Train epoch:0, batch index:434, loss:0.011512348651885987\n",
      "Train epoch:0, batch index:435, loss:0.011511181592941283\n",
      "Train epoch:0, batch index:436, loss:0.011509895324707031\n",
      "Train epoch:0, batch index:437, loss:0.011512566804885865\n",
      "Train epoch:0, batch index:438, loss:0.01150944948196411\n",
      "Train epoch:0, batch index:439, loss:0.011509320735931396\n",
      "Train epoch:0, batch index:440, loss:0.011511812210083008\n",
      "Train epoch:0, batch index:441, loss:0.011513080596923828\n",
      "Train epoch:0, batch index:442, loss:0.011510534286499024\n",
      "Train epoch:0, batch index:443, loss:0.011509127616882324\n",
      "Train epoch:0, batch index:444, loss:0.01151057481765747\n",
      "Train epoch:0, batch index:445, loss:0.011511410474777222\n",
      "Train epoch:0, batch index:446, loss:0.01151172399520874\n",
      "Train epoch:0, batch index:447, loss:0.01151419997215271\n",
      "Train epoch:0, batch index:448, loss:0.011511222124099732\n",
      "Train epoch:0, batch index:449, loss:0.011511541604995727\n",
      "Train epoch:0, batch index:450, loss:0.011511294841766358\n",
      "Train epoch:0, batch index:451, loss:0.01150993824005127\n",
      "Train epoch:0, batch index:452, loss:0.01151075005531311\n",
      "Train epoch:0, batch index:453, loss:0.011509560346603394\n",
      "Train epoch:0, batch index:454, loss:0.011511050462722779\n",
      "Train epoch:0, batch index:455, loss:0.0115109121799469\n",
      "Train epoch:0, batch index:456, loss:0.011510878801345825\n",
      "Train epoch:0, batch index:457, loss:0.011511372327804565\n",
      "Train epoch:0, batch index:458, loss:0.011513060331344605\n",
      "Train epoch:0, batch index:459, loss:0.011510984897613525\n",
      "Train epoch:0, batch index:460, loss:0.011510410308837891\n",
      "Train epoch:0, batch index:461, loss:0.011510897874832154\n",
      "Train epoch:0, batch index:462, loss:0.011511895656585693\n",
      "Train epoch:0, batch index:463, loss:0.011510663032531739\n",
      "Train epoch:0, batch index:464, loss:0.01151303768157959\n",
      "Train epoch:0, batch index:465, loss:0.011511114835739135\n",
      "Train epoch:0, batch index:466, loss:0.011509509086608886\n",
      "Train epoch:0, batch index:467, loss:0.011510422229766845\n",
      "Train epoch:0, batch index:468, loss:0.011508063077926636\n",
      "Train epoch:0, batch index:469, loss:0.011512298583984375\n",
      "Train epoch:0, batch index:470, loss:0.011512328386306763\n",
      "Train epoch:0, batch index:471, loss:0.011511833667755126\n",
      "Train epoch:0, batch index:472, loss:0.011511780023574829\n",
      "Train epoch:0, batch index:473, loss:0.011512374877929688\n",
      "Train epoch:0, batch index:474, loss:0.011513003110885621\n",
      "Train epoch:0, batch index:475, loss:0.011512402296066284\n",
      "Train epoch:0, batch index:476, loss:0.011511036157608033\n",
      "Train epoch:0, batch index:477, loss:0.011511577367782593\n",
      "Train epoch:0, batch index:478, loss:0.011510696411132813\n",
      "Train epoch:0, batch index:479, loss:0.011510453224182128\n",
      "Train epoch:0, batch index:480, loss:0.011511173248291016\n",
      "Train epoch:0, batch index:481, loss:0.01151297926902771\n",
      "Train epoch:0, batch index:482, loss:0.011510418653488159\n",
      "Train epoch:0, batch index:483, loss:0.011510217189788818\n",
      "Train epoch:0, batch index:484, loss:0.0115093195438385\n",
      "Train epoch:0, batch index:485, loss:0.01150971531867981\n",
      "Train epoch:0, batch index:486, loss:0.011509819030761719\n",
      "Train epoch:0, batch index:487, loss:0.01151159405708313\n",
      "Train epoch:0, batch index:488, loss:0.011511454582214356\n",
      "Train epoch:0, batch index:489, loss:0.011510443687438966\n",
      "Train epoch:0, batch index:490, loss:0.011511485576629638\n",
      "Train epoch:0, batch index:491, loss:0.011510415077209473\n",
      "Train epoch:0, batch index:492, loss:0.011513177156448364\n",
      "Train epoch:0, batch index:493, loss:0.011510515213012695\n",
      "Train epoch:0, batch index:494, loss:0.01151146650314331\n",
      "Train epoch:0, batch index:495, loss:0.01150976300239563\n",
      "Train epoch:0, batch index:496, loss:0.011511404514312745\n",
      "Train epoch:0, batch index:497, loss:0.01151174545288086\n",
      "Train epoch:0, batch index:498, loss:0.011510456800460816\n",
      "Train epoch:0, batch index:499, loss:0.011508862972259521\n",
      "Train epoch:0, batch index:500, loss:0.01151218056678772\n",
      "Train epoch:0, batch index:501, loss:0.011510188579559327\n",
      "Train epoch:0, batch index:502, loss:0.01151265025138855\n",
      "Train epoch:0, batch index:503, loss:0.011510144472122192\n",
      "Train epoch:0, batch index:504, loss:0.011510781049728393\n",
      "Train epoch:0, batch index:505, loss:0.011512974500656128\n",
      "Train epoch:0, batch index:506, loss:0.011509772539138794\n",
      "Train epoch:0, batch index:507, loss:0.011509820222854614\n",
      "Train epoch:0, batch index:508, loss:0.011512038707733154\n",
      "Train epoch:0, batch index:509, loss:0.011512178182601928\n",
      "Train epoch:0, batch index:510, loss:0.011510347127914428\n",
      "Train epoch:0, batch index:511, loss:0.0115108060836792\n",
      "Train epoch:0, batch index:512, loss:0.011509742736816406\n",
      "Train epoch:0, batch index:513, loss:0.011509325504302979\n",
      "Train epoch:0, batch index:514, loss:0.011509802341461182\n",
      "Train epoch:0, batch index:515, loss:0.01151078224182129\n",
      "Train epoch:0, batch index:516, loss:0.011511242389678955\n",
      "Train epoch:0, batch index:517, loss:0.011509265899658203\n",
      "Train epoch:0, batch index:518, loss:0.011512196063995362\n",
      "Train epoch:0, batch index:519, loss:0.011511056423187256\n",
      "Train epoch:0, batch index:520, loss:0.0115098237991333\n",
      "Train epoch:0, batch index:521, loss:0.011512467861175537\n",
      "Train epoch:0, batch index:522, loss:0.01151043176651001\n",
      "Train epoch:0, batch index:523, loss:0.01151055097579956\n",
      "Train epoch:0, batch index:524, loss:0.0115110182762146\n",
      "Train epoch:0, batch index:525, loss:0.011511279344558716\n",
      "Train epoch:0, batch index:526, loss:0.011511462926864623\n",
      "Train epoch:0, batch index:527, loss:0.011512203216552734\n",
      "Train epoch:0, batch index:528, loss:0.01151172399520874\n",
      "Train epoch:0, batch index:529, loss:0.011511222124099732\n",
      "Train epoch:0, batch index:530, loss:0.01151059865951538\n",
      "Train epoch:0, batch index:531, loss:0.011511087417602539\n",
      "Train epoch:0, batch index:532, loss:0.011513148546218871\n",
      "Train epoch:0, batch index:533, loss:0.01151125192642212\n",
      "Train epoch:0, batch index:534, loss:0.011510210037231445\n",
      "Train epoch:0, batch index:535, loss:0.011509113311767578\n",
      "Train epoch:0, batch index:536, loss:0.011512572765350343\n",
      "Train epoch:0, batch index:537, loss:0.011512407064437867\n",
      "Train epoch:0, batch index:538, loss:0.011510279178619385\n",
      "Train epoch:0, batch index:539, loss:0.011509863138198852\n",
      "Train epoch:0, batch index:540, loss:0.011511119604110718\n",
      "Train epoch:0, batch index:541, loss:0.01150993824005127\n",
      "Train epoch:0, batch index:542, loss:0.011511754989624024\n",
      "Train epoch:0, batch index:543, loss:0.011512818336486817\n",
      "Train epoch:0, batch index:544, loss:0.011512194871902465\n",
      "Train epoch:0, batch index:545, loss:0.011512444019317628\n",
      "Train epoch:0, batch index:546, loss:0.011511106491088868\n",
      "Train epoch:0, batch index:547, loss:0.011515191793441772\n",
      "Train epoch:0, batch index:548, loss:0.011511186361312866\n",
      "Train epoch:0, batch index:549, loss:0.011510050296783448\n",
      "Train epoch:0, batch index:550, loss:0.011509588956832885\n",
      "Train epoch:0, batch index:551, loss:0.011511448621749878\n",
      "Train epoch:0, batch index:552, loss:0.011510530710220337\n",
      "Train epoch:0, batch index:553, loss:0.011508471965789795\n",
      "Train epoch:0, batch index:554, loss:0.011511147022247314\n",
      "Train epoch:0, batch index:555, loss:0.011509350538253783\n",
      "Train epoch:0, batch index:556, loss:0.011510227918624877\n",
      "Train epoch:0, batch index:557, loss:0.011509283781051635\n",
      "Train epoch:0, batch index:558, loss:0.011509075164794921\n",
      "Train epoch:0, batch index:559, loss:0.011511600017547608\n",
      "Train epoch:0, batch index:560, loss:0.011510267257690429\n",
      "Train epoch:0, batch index:561, loss:0.011510884761810303\n",
      "Train epoch:0, batch index:562, loss:0.011512104272842407\n",
      "Train epoch:0, batch index:563, loss:0.011512762308120728\n",
      "Train epoch:0, batch index:564, loss:0.011511156558990479\n",
      "Train epoch:0, batch index:565, loss:0.01151021957397461\n",
      "Train epoch:0, batch index:566, loss:0.011512755155563355\n",
      "Train epoch:0, batch index:567, loss:0.011508959531784057\n",
      "Train epoch:0, batch index:568, loss:0.01151193380355835\n",
      "Train epoch:0, batch index:569, loss:0.011511183977127075\n",
      "Train epoch:0, batch index:570, loss:0.011509006023406982\n",
      "Train epoch:0, batch index:571, loss:0.01151322603225708\n",
      "Train epoch:0, batch index:572, loss:0.01151265025138855\n",
      "Train epoch:0, batch index:573, loss:0.011509466171264648\n",
      "Train epoch:0, batch index:574, loss:0.011512181758880614\n",
      "Train epoch:0, batch index:575, loss:0.011511359214782715\n",
      "Train epoch:0, batch index:576, loss:0.011513338088989258\n",
      "Train epoch:0, batch index:577, loss:0.01151103973388672\n",
      "Train epoch:0, batch index:578, loss:0.011512587070465088\n",
      "Train epoch:0, batch index:579, loss:0.01151021957397461\n",
      "Train epoch:0, batch index:580, loss:0.011510282754898071\n",
      "Train epoch:0, batch index:581, loss:0.011511952877044677\n",
      "Train epoch:0, batch index:582, loss:0.011510134935379028\n",
      "Train epoch:0, batch index:583, loss:0.01151049256324768\n",
      "Train epoch:0, batch index:584, loss:0.011512517929077148\n",
      "Train epoch:0, batch index:585, loss:0.011510534286499024\n",
      "Train epoch:0, batch index:586, loss:0.011509006023406982\n",
      "Train epoch:0, batch index:587, loss:0.011511210203170776\n",
      "Train epoch:0, batch index:588, loss:0.01151154637336731\n",
      "Train epoch:0, batch index:589, loss:0.011509528160095215\n",
      "Train epoch:0, batch index:590, loss:0.011508891582489014\n",
      "Train epoch:0, batch index:591, loss:0.011511846780776977\n",
      "Train epoch:0, batch index:592, loss:0.011511390209197997\n",
      "Train epoch:0, batch index:593, loss:0.011509873867034913\n",
      "Train epoch:0, batch index:594, loss:0.011510037183761597\n",
      "Train epoch:0, batch index:595, loss:0.011510683298110962\n",
      "Train epoch:0, batch index:596, loss:0.011511995792388915\n",
      "Train epoch:0, batch index:597, loss:0.011511125564575196\n",
      "Train epoch:0, batch index:598, loss:0.011512665748596192\n",
      "Train epoch:0, batch index:600, loss:0.011508538722991943\n",
      "Train epoch:0, batch index:601, loss:0.011511894464492798\n",
      "Train epoch:0, batch index:602, loss:0.011511363983154298\n",
      "Train epoch:0, batch index:603, loss:0.011511476039886474\n",
      "Train epoch:0, batch index:604, loss:0.011510567665100098\n",
      "Train epoch:0, batch index:605, loss:0.011509279012680054\n",
      "Train epoch:0, batch index:606, loss:0.011509377956390381\n",
      "Train epoch:0, batch index:607, loss:0.011509895324707031\n",
      "Train epoch:0, batch index:608, loss:0.011510417461395264\n",
      "Train epoch:0, batch index:609, loss:0.011511634588241577\n",
      "Train epoch:0, batch index:610, loss:0.011508564949035644\n",
      "Train epoch:0, batch index:611, loss:0.011511247158050537\n",
      "Train epoch:0, batch index:612, loss:0.011510571241378784\n",
      "Train epoch:0, batch index:613, loss:0.011511064767837524\n",
      "Train epoch:0, batch index:614, loss:0.011508097648620605\n",
      "Train epoch:0, batch index:615, loss:0.011508938074111939\n",
      "Train epoch:0, batch index:616, loss:0.011508489847183228\n",
      "Train epoch:0, batch index:617, loss:0.011510515213012695\n",
      "Train epoch:0, batch index:618, loss:0.01150776982307434\n",
      "Train epoch:0, batch index:619, loss:0.011510100364685059\n",
      "Train epoch:0, batch index:620, loss:0.011511578559875488\n",
      "Train epoch:0, batch index:621, loss:0.011510472297668457\n",
      "Train epoch:0, batch index:622, loss:0.011512510776519776\n",
      "Train epoch:0, batch index:623, loss:0.011510236263275147\n",
      "Train epoch:0, batch index:624, loss:0.011510285139083863\n",
      "Train epoch:0, batch index:625, loss:0.011511552333831786\n",
      "Train epoch:0, batch index:626, loss:0.01150891900062561\n",
      "Train epoch:0, batch index:627, loss:0.011509348154067994\n",
      "Train epoch:0, batch index:628, loss:0.011511350870132447\n",
      "Train epoch:0, batch index:629, loss:0.011510154008865356\n",
      "Train epoch:0, batch index:630, loss:0.01151146411895752\n",
      "Train epoch:0, batch index:631, loss:0.011510260105133056\n",
      "Train epoch:0, batch index:632, loss:0.011508704423904418\n",
      "Train epoch:0, batch index:633, loss:0.011509898900985718\n",
      "Train epoch:0, batch index:634, loss:0.011511449813842773\n",
      "Train epoch:0, batch index:635, loss:0.011508859395980835\n",
      "Train epoch:0, batch index:636, loss:0.011512812376022339\n",
      "Train epoch:0, batch index:637, loss:0.011509342193603516\n",
      "Train epoch:0, batch index:638, loss:0.011509723663330078\n",
      "Train epoch:0, batch index:639, loss:0.011510821580886841\n",
      "Train epoch:0, batch index:640, loss:0.01151063084602356\n",
      "Train epoch:0, batch index:641, loss:0.01151368260383606\n",
      "Train epoch:0, batch index:642, loss:0.011508326530456543\n",
      "Train epoch:0, batch index:643, loss:0.01150995135307312\n",
      "Train epoch:0, batch index:644, loss:0.011511303186416626\n",
      "Train epoch:0, batch index:645, loss:0.011511136293411255\n",
      "Train epoch:0, batch index:646, loss:0.011507989168167114\n",
      "Train epoch:0, batch index:647, loss:0.011510579586029053\n",
      "Train epoch:0, batch index:648, loss:0.011510298252105713\n",
      "Train epoch:0, batch index:649, loss:0.011511449813842773\n",
      "Train epoch:0, batch index:650, loss:0.011510065793991088\n",
      "Train epoch:0, batch index:651, loss:0.011510813236236572\n",
      "Train epoch:0, batch index:652, loss:0.011510398387908936\n",
      "Train epoch:0, batch index:653, loss:0.011510289907455444\n",
      "Train epoch:0, batch index:654, loss:0.011508771181106568\n",
      "Train epoch:0, batch index:655, loss:0.011510928869247436\n",
      "Train epoch:0, batch index:656, loss:0.01151187539100647\n",
      "Train epoch:0, batch index:657, loss:0.011510834693908692\n",
      "Train epoch:0, batch index:658, loss:0.011510300636291503\n",
      "Train epoch:0, batch index:659, loss:0.011510939598083495\n",
      "Train epoch:0, batch index:660, loss:0.011509741544723512\n",
      "Train epoch:0, batch index:661, loss:0.011511361598968506\n",
      "Train epoch:0, batch index:662, loss:0.01151002049446106\n",
      "Train epoch:0, batch index:663, loss:0.01150970458984375\n",
      "Train epoch:0, batch index:664, loss:0.011510858535766602\n",
      "Train epoch:0, batch index:665, loss:0.011508857011795043\n",
      "Train epoch:0, batch index:666, loss:0.01150936484336853\n",
      "Train epoch:0, batch index:667, loss:0.011512624025344849\n",
      "Train epoch:0, batch index:668, loss:0.011510922908782958\n",
      "Train epoch:0, batch index:669, loss:0.011511489152908325\n",
      "Train epoch:0, batch index:670, loss:0.011512143611907959\n",
      "Train epoch:0, batch index:671, loss:0.011511821746826172\n",
      "Train epoch:0, batch index:672, loss:0.011510601043701172\n",
      "Train epoch:0, batch index:673, loss:0.011510210037231445\n",
      "Train epoch:0, batch index:674, loss:0.01151061177253723\n",
      "Train epoch:0, batch index:675, loss:0.011510820388793945\n",
      "Train epoch:0, batch index:676, loss:0.011510632038116454\n",
      "Train epoch:0, batch index:677, loss:0.011510615348815917\n",
      "Train epoch:0, batch index:678, loss:0.011509506702423096\n",
      "Train epoch:0, batch index:679, loss:0.011511613130569458\n",
      "Train epoch:0, batch index:680, loss:0.011509881019592286\n",
      "Train epoch:0, batch index:681, loss:0.011508936882019044\n",
      "Train epoch:0, batch index:682, loss:0.011511610746383667\n",
      "Train epoch:0, batch index:683, loss:0.011508008241653442\n",
      "Train epoch:0, batch index:684, loss:0.011510910987854004\n",
      "Train epoch:0, batch index:685, loss:0.011510263681411743\n",
      "Train epoch:0, batch index:686, loss:0.011508316993713378\n",
      "Train epoch:0, batch index:687, loss:0.01150915265083313\n",
      "Train epoch:0, batch index:688, loss:0.011510570049285889\n",
      "Train epoch:0, batch index:689, loss:0.01150867462158203\n",
      "Train epoch:0, batch index:690, loss:0.011508526802062989\n",
      "Train epoch:0, batch index:691, loss:0.011510379314422607\n",
      "Train epoch:0, batch index:692, loss:0.011510752439498902\n",
      "Train epoch:0, batch index:693, loss:0.011510957479476929\n",
      "Train epoch:0, batch index:694, loss:0.011509891748428345\n",
      "Train epoch:0, batch index:695, loss:0.011509997844696045\n",
      "Train epoch:0, batch index:696, loss:0.011509243249893188\n",
      "Train epoch:0, batch index:697, loss:0.01150962233543396\n",
      "Train epoch:0, batch index:698, loss:0.011509734392166137\n",
      "Train epoch:0, batch index:699, loss:0.011512069702148438\n",
      "Train epoch:0, batch index:700, loss:0.011509194374084472\n",
      "Train epoch:0, batch index:701, loss:0.011511573791503906\n",
      "Train epoch:0, batch index:702, loss:0.011511901617050171\n",
      "Train epoch:0, batch index:703, loss:0.011508251428604127\n",
      "Train epoch:0, batch index:704, loss:0.01150933027267456\n",
      "Train epoch:0, batch index:705, loss:0.011512467861175537\n",
      "Train epoch:0, batch index:706, loss:0.011509683132171631\n",
      "Train epoch:0, batch index:707, loss:0.011507472991943359\n",
      "Train epoch:0, batch index:708, loss:0.011510133743286133\n",
      "Train epoch:0, batch index:709, loss:0.011510339975357055\n",
      "Train epoch:0, batch index:710, loss:0.011511825323104859\n",
      "Train epoch:0, batch index:711, loss:0.011512178182601928\n",
      "Train epoch:0, batch index:712, loss:0.011510803699493408\n",
      "Train epoch:0, batch index:713, loss:0.011510374546051026\n",
      "Train epoch:0, batch index:714, loss:0.011509318351745606\n",
      "Train epoch:0, batch index:715, loss:0.011510064601898193\n",
      "Train epoch:0, batch index:716, loss:0.011507132053375245\n",
      "Train epoch:0, batch index:717, loss:0.01151133418083191\n",
      "Train epoch:0, batch index:718, loss:0.011508426666259765\n",
      "Train epoch:0, batch index:719, loss:0.011510344743728638\n",
      "Train epoch:0, batch index:720, loss:0.011511338949203491\n",
      "Train epoch:0, batch index:721, loss:0.011509066820144654\n",
      "Train epoch:0, batch index:722, loss:0.011511492729187011\n",
      "Train epoch:0, batch index:723, loss:0.011508892774581909\n",
      "Train epoch:0, batch index:724, loss:0.011511522531509399\n",
      "Train epoch:0, batch index:725, loss:0.011509219408035279\n",
      "Train epoch:0, batch index:726, loss:0.011510858535766602\n",
      "Train epoch:0, batch index:727, loss:0.011510059833526612\n",
      "Train epoch:0, batch index:728, loss:0.01151038408279419\n",
      "Train epoch:0, batch index:729, loss:0.011509463787078858\n",
      "Train epoch:0, batch index:730, loss:0.01150838851928711\n",
      "Train epoch:0, batch index:731, loss:0.011510251760482789\n",
      "Train epoch:0, batch index:732, loss:0.011509705781936646\n",
      "Train epoch:0, batch index:733, loss:0.01151068091392517\n",
      "Train epoch:0, batch index:734, loss:0.011511306762695312\n",
      "Train epoch:0, batch index:735, loss:0.011508342027664185\n",
      "Train epoch:0, batch index:736, loss:0.011510747671127319\n",
      "Train epoch:0, batch index:737, loss:0.011509891748428345\n",
      "Train epoch:0, batch index:738, loss:0.011508685350418092\n",
      "Train epoch:0, batch index:739, loss:0.01151038408279419\n",
      "Train epoch:0, batch index:740, loss:0.01150858998298645\n",
      "Train epoch:0, batch index:741, loss:0.011508313417434692\n",
      "Train epoch:0, batch index:742, loss:0.011508756875991821\n",
      "Train epoch:0, batch index:743, loss:0.011509345769882202\n",
      "Train epoch:0, batch index:744, loss:0.01150887131690979\n",
      "Train epoch:0, batch index:745, loss:0.011511521339416504\n",
      "Train epoch:0, batch index:746, loss:0.011508209705352783\n",
      "Train epoch:0, batch index:747, loss:0.011507920026779174\n",
      "Train epoch:0, batch index:748, loss:0.011509920358657838\n",
      "Train epoch:0, batch index:749, loss:0.011510024070739746\n",
      "Train epoch:0, batch index:750, loss:0.011509881019592286\n",
      "Train epoch:0, batch index:751, loss:0.011509736776351929\n",
      "Train epoch:0, batch index:752, loss:0.011511228084564208\n",
      "Train epoch:0, batch index:753, loss:0.011512320041656494\n",
      "Train epoch:0, batch index:754, loss:0.011510839462280273\n",
      "Train epoch:0, batch index:755, loss:0.011508508920669555\n",
      "Train epoch:0, batch index:756, loss:0.01151164174079895\n",
      "Train epoch:0, batch index:757, loss:0.011509090662002563\n",
      "Train epoch:0, batch index:758, loss:0.011509765386581421\n",
      "Train epoch:0, batch index:759, loss:0.011508750915527343\n",
      "Train epoch:0, batch index:760, loss:0.011510730981826782\n",
      "Train epoch:0, batch index:761, loss:0.01151034951210022\n",
      "Train epoch:0, batch index:762, loss:0.011509907245635987\n",
      "Train epoch:0, batch index:763, loss:0.01151227593421936\n",
      "Train epoch:0, batch index:764, loss:0.011509110927581787\n",
      "Train epoch:0, batch index:765, loss:0.01151158571243286\n",
      "Train epoch:0, batch index:766, loss:0.011510553359985352\n",
      "Train epoch:0, batch index:767, loss:0.01150928258895874\n",
      "Train epoch:0, batch index:768, loss:0.011509591341018676\n",
      "Train epoch:0, batch index:769, loss:0.011509984731674194\n",
      "Train epoch:0, batch index:770, loss:0.011510381698608399\n",
      "Train epoch:0, batch index:771, loss:0.011508257389068603\n",
      "Train epoch:0, batch index:772, loss:0.01151015877723694\n",
      "Train epoch:0, batch index:773, loss:0.011508883237838744\n",
      "Train epoch:0, batch index:774, loss:0.01150954008102417\n",
      "Train epoch:0, batch index:775, loss:0.011508039236068725\n",
      "Train epoch:0, batch index:776, loss:0.011509217023849487\n",
      "Train epoch:0, batch index:777, loss:0.011511877775192261\n",
      "Train epoch:0, batch index:778, loss:0.011510390043258666\n",
      "Train epoch:0, batch index:779, loss:0.011510572433471679\n",
      "Train epoch:0, batch index:780, loss:0.011510504484176636\n",
      "Train epoch:0, batch index:781, loss:0.011510542631149291\n",
      "Train epoch:1, batch index:0, loss:0.011509833335876464\n",
      "Train epoch:1, batch index:1, loss:0.01150825023651123\n",
      "Train epoch:1, batch index:2, loss:0.011510096788406372\n",
      "Train epoch:1, batch index:3, loss:0.011509250402450561\n",
      "Train epoch:1, batch index:4, loss:0.011508543491363526\n",
      "Train epoch:1, batch index:5, loss:0.011509705781936646\n",
      "Train epoch:1, batch index:6, loss:0.011510624885559081\n",
      "Train epoch:1, batch index:7, loss:0.011510387659072877\n",
      "Train epoch:1, batch index:8, loss:0.011510432958602905\n",
      "Train epoch:1, batch index:9, loss:0.011512771844863892\n",
      "Train epoch:1, batch index:10, loss:0.011510059833526612\n",
      "Train epoch:1, batch index:11, loss:0.011508727073669433\n",
      "Train epoch:1, batch index:12, loss:0.011510434150695801\n",
      "Train epoch:1, batch index:13, loss:0.011508129835128784\n",
      "Train epoch:1, batch index:14, loss:0.011509891748428345\n",
      "Train epoch:1, batch index:15, loss:0.01151039958000183\n",
      "Train epoch:1, batch index:16, loss:0.011511204242706298\n",
      "Train epoch:1, batch index:18, loss:0.011513500213623047\n",
      "Train epoch:1, batch index:19, loss:0.01150923252105713\n",
      "Train epoch:1, batch index:20, loss:0.011509873867034913\n",
      "Train epoch:1, batch index:21, loss:0.011509109735488892\n",
      "Train epoch:1, batch index:22, loss:0.011506601572036743\n",
      "Train epoch:1, batch index:23, loss:0.01151086688041687\n",
      "Train epoch:1, batch index:24, loss:0.01150973916053772\n",
      "Train epoch:1, batch index:25, loss:0.011510984897613525\n",
      "Train epoch:1, batch index:26, loss:0.011508290767669677\n",
      "Train epoch:1, batch index:27, loss:0.011511541604995727\n",
      "Train epoch:1, batch index:28, loss:0.011507821083068848\n",
      "Train epoch:1, batch index:29, loss:0.011510556936264038\n",
      "Train epoch:1, batch index:30, loss:0.011511675119400024\n",
      "Train epoch:1, batch index:31, loss:0.01150978684425354\n",
      "Train epoch:1, batch index:32, loss:0.011509130001068115\n",
      "Train epoch:1, batch index:33, loss:0.011511002779006957\n",
      "Train epoch:1, batch index:34, loss:0.011510065793991088\n",
      "Train epoch:1, batch index:35, loss:0.01150814414024353\n",
      "Train epoch:1, batch index:36, loss:0.011512516736984254\n",
      "Train epoch:1, batch index:37, loss:0.011510015726089477\n",
      "Train epoch:1, batch index:38, loss:0.011509603261947632\n",
      "Train epoch:1, batch index:39, loss:0.011510852575302124\n",
      "Train epoch:1, batch index:40, loss:0.011509304046630859\n",
      "Train epoch:1, batch index:41, loss:0.011508967876434326\n",
      "Train epoch:1, batch index:42, loss:0.011507090330123902\n",
      "Train epoch:1, batch index:43, loss:0.011510628461837768\n",
      "Train epoch:1, batch index:44, loss:0.011509475708007812\n",
      "Train epoch:1, batch index:45, loss:0.01151042103767395\n",
      "Train epoch:1, batch index:46, loss:0.011510878801345825\n",
      "Train epoch:1, batch index:47, loss:0.011509408950805664\n",
      "Train epoch:1, batch index:48, loss:0.011509759426116943\n",
      "Train epoch:1, batch index:49, loss:0.011509095430374145\n",
      "Train epoch:1, batch index:50, loss:0.011511392593383789\n",
      "Train epoch:1, batch index:51, loss:0.011511785984039307\n",
      "Train epoch:1, batch index:52, loss:0.011511006355285645\n",
      "Train epoch:1, batch index:53, loss:0.01150964617729187\n",
      "Train epoch:1, batch index:54, loss:0.011510623693466187\n",
      "Train epoch:1, batch index:55, loss:0.011508218050003051\n",
      "Train epoch:1, batch index:56, loss:0.011507422924041747\n",
      "Train epoch:1, batch index:57, loss:0.011509054899215698\n",
      "Train epoch:1, batch index:58, loss:0.011508476734161378\n",
      "Train epoch:1, batch index:59, loss:0.011510336399078369\n",
      "Train epoch:1, batch index:60, loss:0.011510483026504516\n",
      "Train epoch:1, batch index:61, loss:0.01151045560836792\n",
      "Train epoch:1, batch index:62, loss:0.01151063323020935\n",
      "Train epoch:1, batch index:63, loss:0.011508291959762574\n",
      "Train epoch:1, batch index:64, loss:0.0115116024017334\n",
      "Train epoch:1, batch index:65, loss:0.011512149572372437\n",
      "Train epoch:1, batch index:66, loss:0.0115092134475708\n",
      "Train epoch:1, batch index:67, loss:0.011509795188903809\n",
      "Train epoch:1, batch index:68, loss:0.011510097980499267\n",
      "Train epoch:1, batch index:69, loss:0.011508489847183228\n",
      "Train epoch:1, batch index:70, loss:0.011509448289871216\n",
      "Train epoch:1, batch index:71, loss:0.011508551836013793\n",
      "Train epoch:1, batch index:72, loss:0.01151155710220337\n",
      "Train epoch:1, batch index:73, loss:0.011509912014007568\n",
      "Train epoch:1, batch index:74, loss:0.011511446237564086\n",
      "Train epoch:1, batch index:75, loss:0.011510502099990844\n",
      "Train epoch:1, batch index:76, loss:0.011510498523712158\n",
      "Train epoch:1, batch index:77, loss:0.011508423089981078\n",
      "Train epoch:1, batch index:78, loss:0.011509585380554199\n",
      "Train epoch:1, batch index:79, loss:0.011510756015777588\n",
      "Train epoch:1, batch index:80, loss:0.011510241031646728\n",
      "Train epoch:1, batch index:81, loss:0.011509028673171997\n",
      "Train epoch:1, batch index:82, loss:0.01150895118713379\n",
      "Train epoch:1, batch index:83, loss:0.011508861780166626\n",
      "Train epoch:1, batch index:84, loss:0.01151028037071228\n",
      "Train epoch:1, batch index:85, loss:0.01151188850402832\n",
      "Train epoch:1, batch index:86, loss:0.011509894132614136\n",
      "Train epoch:1, batch index:87, loss:0.011507458686828613\n",
      "Train epoch:1, batch index:88, loss:0.011508599519729615\n",
      "Train epoch:1, batch index:89, loss:0.01151188850402832\n",
      "Train epoch:1, batch index:90, loss:0.011510887145996095\n",
      "Train epoch:1, batch index:91, loss:0.011508150100708008\n",
      "Train epoch:1, batch index:92, loss:0.011507561206817627\n",
      "Train epoch:1, batch index:93, loss:0.011510359048843384\n",
      "Train epoch:1, batch index:94, loss:0.011506712436676026\n",
      "Train epoch:1, batch index:95, loss:0.011509658098220825\n",
      "Train epoch:1, batch index:96, loss:0.011510009765625\n",
      "Train epoch:1, batch index:97, loss:0.011510876417160034\n",
      "Train epoch:1, batch index:98, loss:0.011509581804275512\n",
      "Train epoch:1, batch index:99, loss:0.011508830785751344\n",
      "Train epoch:1, batch index:100, loss:0.011510673761367798\n",
      "Train epoch:1, batch index:101, loss:0.011506984233856201\n",
      "Train epoch:1, batch index:102, loss:0.01150687575340271\n",
      "Train epoch:1, batch index:103, loss:0.011508901119232178\n",
      "Train epoch:1, batch index:104, loss:0.011511248350143433\n",
      "Train epoch:1, batch index:105, loss:0.011511811017990113\n",
      "Train epoch:1, batch index:106, loss:0.011510130167007446\n",
      "Train epoch:1, batch index:107, loss:0.011508631706237792\n",
      "Train epoch:1, batch index:108, loss:0.011510297060012817\n",
      "Train epoch:1, batch index:109, loss:0.011510257720947265\n",
      "Train epoch:1, batch index:110, loss:0.011512367725372315\n",
      "Train epoch:1, batch index:111, loss:0.011510981321334839\n",
      "Train epoch:1, batch index:112, loss:0.011512122154235839\n",
      "Train epoch:1, batch index:113, loss:0.011508572101593017\n",
      "Train epoch:1, batch index:114, loss:0.0115092134475708\n",
      "Train epoch:1, batch index:115, loss:0.011507400274276734\n",
      "Train epoch:1, batch index:116, loss:0.011508021354675293\n",
      "Train epoch:1, batch index:117, loss:0.011509422063827515\n",
      "Train epoch:1, batch index:118, loss:0.01150787353515625\n",
      "Train epoch:1, batch index:119, loss:0.01150820255279541\n",
      "Train epoch:1, batch index:120, loss:0.011509459018707275\n",
      "Train epoch:1, batch index:121, loss:0.011508097648620605\n",
      "Train epoch:1, batch index:122, loss:0.011509172916412354\n",
      "Train epoch:1, batch index:123, loss:0.011511006355285645\n",
      "Train epoch:1, batch index:124, loss:0.011508011817932129\n",
      "Train epoch:1, batch index:125, loss:0.01151010513305664\n",
      "Train epoch:1, batch index:126, loss:0.011509743928909301\n",
      "Train epoch:1, batch index:127, loss:0.01151050090789795\n",
      "Train epoch:1, batch index:128, loss:0.011510841846466065\n",
      "Train epoch:1, batch index:129, loss:0.011510425806045532\n",
      "Train epoch:1, batch index:130, loss:0.011510252952575684\n",
      "Train epoch:1, batch index:131, loss:0.011510729789733887\n",
      "Train epoch:1, batch index:132, loss:0.011507728099822999\n",
      "Train epoch:1, batch index:133, loss:0.011509169340133667\n",
      "Train epoch:1, batch index:134, loss:0.01150766372680664\n",
      "Train epoch:1, batch index:135, loss:0.011508378982543945\n",
      "Train epoch:1, batch index:136, loss:0.011506626605987549\n",
      "Train epoch:1, batch index:137, loss:0.011510392427444458\n",
      "Train epoch:1, batch index:138, loss:0.01150962233543396\n",
      "Train epoch:1, batch index:139, loss:0.01150962233543396\n",
      "Train epoch:1, batch index:140, loss:0.01150821328163147\n",
      "Train epoch:1, batch index:141, loss:0.011508076190948487\n",
      "Train epoch:1, batch index:142, loss:0.011508811712265015\n",
      "Train epoch:1, batch index:143, loss:0.011510558128356933\n",
      "Train epoch:1, batch index:144, loss:0.011509585380554199\n",
      "Train epoch:1, batch index:145, loss:0.011510329246520996\n",
      "Train epoch:1, batch index:146, loss:0.011509827375411986\n",
      "Train epoch:1, batch index:147, loss:0.011510752439498902\n",
      "Train epoch:1, batch index:148, loss:0.011509519815444947\n",
      "Train epoch:1, batch index:149, loss:0.011510287523269653\n",
      "Train epoch:1, batch index:150, loss:0.011508188247680663\n",
      "Train epoch:1, batch index:151, loss:0.01150776982307434\n",
      "Train epoch:1, batch index:152, loss:0.011510957479476929\n",
      "Train epoch:1, batch index:153, loss:0.01151057481765747\n",
      "Train epoch:1, batch index:154, loss:0.011508954763412476\n",
      "Train epoch:1, batch index:155, loss:0.011508958339691162\n",
      "Train epoch:1, batch index:156, loss:0.011510608196258544\n",
      "Train epoch:1, batch index:157, loss:0.011506434679031372\n",
      "Train epoch:1, batch index:158, loss:0.011509946584701537\n",
      "Train epoch:1, batch index:159, loss:0.011511703729629516\n",
      "Train epoch:1, batch index:160, loss:0.011510850191116332\n",
      "Train epoch:1, batch index:161, loss:0.011509572267532348\n",
      "Train epoch:1, batch index:162, loss:0.011509785652160645\n",
      "Train epoch:1, batch index:163, loss:0.011510668992996216\n",
      "Train epoch:1, batch index:164, loss:0.01151395082473755\n",
      "Train epoch:1, batch index:165, loss:0.011511538028717041\n",
      "Train epoch:1, batch index:166, loss:0.011510655879974366\n",
      "Train epoch:1, batch index:167, loss:0.011509058475494384\n",
      "Train epoch:1, batch index:168, loss:0.01150968074798584\n",
      "Train epoch:1, batch index:169, loss:0.01150989294052124\n",
      "Train epoch:1, batch index:170, loss:0.011508461236953735\n",
      "Train epoch:1, batch index:171, loss:0.011508458852767944\n",
      "Train epoch:1, batch index:172, loss:0.01151050567626953\n",
      "Train epoch:1, batch index:173, loss:0.011510592699050904\n",
      "Train epoch:1, batch index:174, loss:0.011508709192276001\n",
      "Train epoch:1, batch index:175, loss:0.011510645151138305\n",
      "Train epoch:1, batch index:176, loss:0.011508841514587403\n",
      "Train epoch:1, batch index:177, loss:0.01150924563407898\n",
      "Train epoch:1, batch index:178, loss:0.011510621309280395\n",
      "Train epoch:1, batch index:179, loss:0.0115072500705719\n",
      "Train epoch:1, batch index:180, loss:0.0115079665184021\n",
      "Train epoch:1, batch index:181, loss:0.011507506370544434\n",
      "Train epoch:1, batch index:182, loss:0.011507766246795654\n",
      "Train epoch:1, batch index:183, loss:0.011509616374969483\n",
      "Train epoch:1, batch index:184, loss:0.011507463455200196\n",
      "Train epoch:1, batch index:185, loss:0.011507644653320312\n",
      "Train epoch:1, batch index:186, loss:0.011505891084671021\n",
      "Train epoch:1, batch index:187, loss:0.011509277820587159\n",
      "Train epoch:1, batch index:188, loss:0.011509162187576295\n",
      "Train epoch:1, batch index:189, loss:0.01151203989982605\n",
      "Train epoch:1, batch index:190, loss:0.011511425971984863\n",
      "Train epoch:1, batch index:191, loss:0.011506675481796265\n",
      "Train epoch:1, batch index:192, loss:0.01151009440422058\n",
      "Train epoch:1, batch index:193, loss:0.011508455276489258\n",
      "Train epoch:1, batch index:194, loss:0.011509450674057007\n",
      "Train epoch:1, batch index:195, loss:0.011510931253433228\n",
      "Train epoch:1, batch index:196, loss:0.011511433124542236\n",
      "Train epoch:1, batch index:197, loss:0.011510018110275268\n",
      "Train epoch:1, batch index:198, loss:0.011505851745605469\n",
      "Train epoch:1, batch index:199, loss:0.011508474349975586\n",
      "Train epoch:1, batch index:200, loss:0.011509759426116943\n",
      "Train epoch:1, batch index:201, loss:0.011509374380111695\n",
      "Train epoch:1, batch index:202, loss:0.011509788036346436\n",
      "Train epoch:1, batch index:203, loss:0.011509277820587159\n",
      "Train epoch:1, batch index:204, loss:0.011509559154510497\n",
      "Train epoch:1, batch index:205, loss:0.011509236097335816\n",
      "Train epoch:1, batch index:206, loss:0.011508666276931763\n",
      "Train epoch:1, batch index:207, loss:0.011506649255752564\n",
      "Train epoch:1, batch index:208, loss:0.011511304378509522\n",
      "Train epoch:1, batch index:209, loss:0.011507216691970825\n",
      "Train epoch:1, batch index:210, loss:0.011508247852325439\n",
      "Train epoch:1, batch index:211, loss:0.011508628129959106\n",
      "Train epoch:1, batch index:212, loss:0.011507670879364013\n",
      "Train epoch:1, batch index:213, loss:0.01150989055633545\n",
      "Train epoch:1, batch index:214, loss:0.011509982347488403\n",
      "Train epoch:1, batch index:215, loss:0.011510560512542725\n",
      "Train epoch:1, batch index:216, loss:0.011510584354400635\n",
      "Train epoch:1, batch index:218, loss:0.011508538722991943\n",
      "Train epoch:1, batch index:219, loss:0.01150799036026001\n",
      "Train epoch:1, batch index:220, loss:0.011508214473724365\n",
      "Train epoch:1, batch index:221, loss:0.011509873867034913\n",
      "Train epoch:1, batch index:222, loss:0.011508688926696778\n",
      "Train epoch:1, batch index:223, loss:0.011509267091751098\n",
      "Train epoch:1, batch index:224, loss:0.011506638526916503\n",
      "Train epoch:1, batch index:225, loss:0.011507083177566529\n",
      "Train epoch:1, batch index:226, loss:0.011507052183151244\n",
      "Train epoch:1, batch index:227, loss:0.011508679389953614\n",
      "Train epoch:1, batch index:228, loss:0.011507948637008667\n",
      "Train epoch:1, batch index:229, loss:0.011508810520172118\n",
      "Train epoch:1, batch index:230, loss:0.011509354114532471\n",
      "Train epoch:1, batch index:231, loss:0.011510484218597412\n",
      "Train epoch:1, batch index:232, loss:0.011511088609695434\n",
      "Train epoch:1, batch index:233, loss:0.011510077714920044\n",
      "Train epoch:1, batch index:234, loss:0.011511720418930053\n",
      "Train epoch:1, batch index:235, loss:0.011510297060012817\n",
      "Train epoch:1, batch index:236, loss:0.011508252620697022\n",
      "Train epoch:1, batch index:237, loss:0.011510016918182374\n",
      "Train epoch:1, batch index:238, loss:0.01150843381881714\n",
      "Train epoch:1, batch index:239, loss:0.011508653163909912\n",
      "Train epoch:1, batch index:240, loss:0.011510601043701172\n",
      "Train epoch:1, batch index:241, loss:0.0115073823928833\n",
      "Train epoch:1, batch index:242, loss:0.011506534814834594\n",
      "Train epoch:1, batch index:243, loss:0.011509977579116822\n",
      "Train epoch:1, batch index:244, loss:0.011507930755615235\n",
      "Train epoch:1, batch index:245, loss:0.011510587930679321\n",
      "Train epoch:1, batch index:246, loss:0.011508073806762695\n",
      "Train epoch:1, batch index:247, loss:0.011507800817489623\n",
      "Train epoch:1, batch index:248, loss:0.011508448123931885\n",
      "Train epoch:1, batch index:249, loss:0.011508413553237916\n",
      "Train epoch:1, batch index:250, loss:0.011511269807815552\n",
      "Train epoch:1, batch index:251, loss:0.01150761365890503\n",
      "Train epoch:1, batch index:252, loss:0.011510342359542847\n",
      "Train epoch:1, batch index:253, loss:0.011508475542068481\n",
      "Train epoch:1, batch index:254, loss:0.011508009433746337\n",
      "Train epoch:1, batch index:255, loss:0.011510161161422729\n",
      "Train epoch:1, batch index:256, loss:0.011506826877593994\n",
      "Train epoch:1, batch index:257, loss:0.011505930423736573\n",
      "Train epoch:1, batch index:258, loss:0.011511225700378418\n",
      "Train epoch:1, batch index:259, loss:0.011509400606155396\n",
      "Train epoch:1, batch index:260, loss:0.0115073561668396\n",
      "Train epoch:1, batch index:261, loss:0.011508738994598389\n",
      "Train epoch:1, batch index:262, loss:0.011510356664657592\n",
      "Train epoch:1, batch index:263, loss:0.011508382558822632\n",
      "Train epoch:1, batch index:264, loss:0.011508070230484009\n",
      "Train epoch:1, batch index:265, loss:0.011510686874389648\n",
      "Train epoch:1, batch index:266, loss:0.011507490873336792\n",
      "Train epoch:1, batch index:267, loss:0.011508848667144776\n",
      "Train epoch:1, batch index:268, loss:0.011507130861282348\n",
      "Train epoch:1, batch index:269, loss:0.011508101224899292\n",
      "Train epoch:1, batch index:270, loss:0.011507080793380737\n",
      "Train epoch:1, batch index:271, loss:0.011508738994598389\n",
      "Train epoch:1, batch index:272, loss:0.011508704423904418\n",
      "Train epoch:1, batch index:273, loss:0.011509689092636109\n",
      "Train epoch:1, batch index:274, loss:0.011508452892303466\n",
      "Train epoch:1, batch index:275, loss:0.011508536338806153\n",
      "Train epoch:1, batch index:276, loss:0.011509767770767211\n",
      "Train epoch:1, batch index:277, loss:0.011508580446243286\n",
      "Train epoch:1, batch index:278, loss:0.011508336067199707\n",
      "Train epoch:1, batch index:279, loss:0.011509779691696167\n",
      "Train epoch:1, batch index:280, loss:0.011509678363800048\n",
      "Train epoch:1, batch index:281, loss:0.011511051654815673\n",
      "Train epoch:1, batch index:282, loss:0.011508309841156006\n",
      "Train epoch:1, batch index:283, loss:0.011508245468139649\n",
      "Train epoch:1, batch index:284, loss:0.011509164571762084\n",
      "Train epoch:1, batch index:285, loss:0.011509716510772705\n",
      "Train epoch:1, batch index:286, loss:0.011509685516357422\n",
      "Train epoch:1, batch index:287, loss:0.011507548093795776\n",
      "Train epoch:1, batch index:288, loss:0.01150736927986145\n",
      "Train epoch:1, batch index:289, loss:0.011509857177734374\n",
      "Train epoch:1, batch index:290, loss:0.011508700847625732\n",
      "Train epoch:1, batch index:291, loss:0.01150882601737976\n",
      "Train epoch:1, batch index:292, loss:0.01150676965713501\n",
      "Train epoch:1, batch index:293, loss:0.011509701013565063\n",
      "Train epoch:1, batch index:294, loss:0.011509246826171875\n",
      "Train epoch:1, batch index:295, loss:0.011508480310440064\n",
      "Train epoch:1, batch index:296, loss:0.011509181261062621\n",
      "Train epoch:1, batch index:297, loss:0.011509943008422851\n",
      "Train epoch:1, batch index:298, loss:0.011506905555725097\n",
      "Train epoch:1, batch index:299, loss:0.011508456468582152\n",
      "Train epoch:1, batch index:300, loss:0.011509592533111573\n",
      "Train epoch:1, batch index:301, loss:0.011511566638946534\n",
      "Train epoch:1, batch index:302, loss:0.011507222652435303\n",
      "Train epoch:1, batch index:303, loss:0.011510268449783326\n",
      "Train epoch:1, batch index:304, loss:0.011508451700210571\n",
      "Train epoch:1, batch index:305, loss:0.011508100032806397\n",
      "Train epoch:1, batch index:306, loss:0.011510075330734252\n",
      "Train epoch:1, batch index:307, loss:0.011508079767227173\n",
      "Train epoch:1, batch index:308, loss:0.01150998830795288\n",
      "Train epoch:1, batch index:309, loss:0.011507277488708495\n",
      "Train epoch:1, batch index:310, loss:0.011511156558990479\n",
      "Train epoch:1, batch index:311, loss:0.011509112119674682\n",
      "Train epoch:1, batch index:312, loss:0.011510862112045288\n",
      "Train epoch:1, batch index:313, loss:0.011508278846740723\n",
      "Train epoch:1, batch index:314, loss:0.011508067846298218\n",
      "Train epoch:1, batch index:315, loss:0.011508656740188599\n",
      "Train epoch:1, batch index:316, loss:0.011510064601898193\n",
      "Train epoch:1, batch index:317, loss:0.011508880853652955\n",
      "Train epoch:1, batch index:318, loss:0.0115077805519104\n",
      "Train epoch:1, batch index:319, loss:0.011511391401290894\n",
      "Train epoch:1, batch index:320, loss:0.0115081787109375\n",
      "Train epoch:1, batch index:321, loss:0.011511166095733643\n",
      "Train epoch:1, batch index:322, loss:0.011507954597473145\n",
      "Train epoch:1, batch index:323, loss:0.011507247686386108\n",
      "Train epoch:1, batch index:324, loss:0.011509106159210206\n",
      "Train epoch:1, batch index:325, loss:0.011508078575134277\n",
      "Train epoch:1, batch index:326, loss:0.011507954597473145\n",
      "Train epoch:1, batch index:327, loss:0.011509577035903931\n",
      "Train epoch:1, batch index:328, loss:0.011508268117904664\n",
      "Train epoch:1, batch index:329, loss:0.011508162021636964\n",
      "Train epoch:1, batch index:330, loss:0.011510215997695923\n",
      "Train epoch:1, batch index:331, loss:0.01150985598564148\n",
      "Train epoch:1, batch index:332, loss:0.011509099006652833\n",
      "Train epoch:1, batch index:333, loss:0.011508146524429321\n",
      "Train epoch:1, batch index:334, loss:0.011510751247406005\n",
      "Train epoch:1, batch index:335, loss:0.011509022712707519\n",
      "Train epoch:1, batch index:336, loss:0.011509989500045776\n",
      "Train epoch:1, batch index:337, loss:0.01150781512260437\n",
      "Train epoch:1, batch index:338, loss:0.011510359048843384\n",
      "Train epoch:1, batch index:339, loss:0.011507681608200072\n",
      "Train epoch:1, batch index:340, loss:0.011508837938308716\n",
      "Train epoch:1, batch index:341, loss:0.011508611440658569\n",
      "Train epoch:1, batch index:342, loss:0.011509089469909669\n",
      "Train epoch:1, batch index:343, loss:0.011506909132003784\n",
      "Train epoch:1, batch index:344, loss:0.011508396863937377\n",
      "Train epoch:1, batch index:345, loss:0.01150774359703064\n",
      "Train epoch:1, batch index:346, loss:0.011507893800735474\n",
      "Train epoch:1, batch index:347, loss:0.011508049964904786\n",
      "Train epoch:1, batch index:348, loss:0.011508044004440308\n",
      "Train epoch:1, batch index:349, loss:0.011509428024291992\n",
      "Train epoch:1, batch index:350, loss:0.011509391069412232\n",
      "Train epoch:1, batch index:351, loss:0.011510416269302368\n",
      "Train epoch:1, batch index:352, loss:0.01151066780090332\n",
      "Train epoch:1, batch index:353, loss:0.011509472131729126\n",
      "Train epoch:1, batch index:354, loss:0.011506222486495972\n",
      "Train epoch:1, batch index:355, loss:0.011508523225784302\n",
      "Train epoch:1, batch index:356, loss:0.011509861946105957\n",
      "Train epoch:1, batch index:357, loss:0.011507703065872192\n",
      "Train epoch:1, batch index:358, loss:0.011509348154067994\n",
      "Train epoch:1, batch index:359, loss:0.011509991884231567\n",
      "Train epoch:1, batch index:360, loss:0.01150885820388794\n",
      "Train epoch:1, batch index:361, loss:0.011507138013839721\n",
      "Train epoch:1, batch index:362, loss:0.011506924629211426\n",
      "Train epoch:1, batch index:363, loss:0.011507929563522338\n",
      "Train epoch:1, batch index:364, loss:0.011508903503417968\n",
      "Train epoch:1, batch index:365, loss:0.011506797075271606\n",
      "Train epoch:1, batch index:366, loss:0.011508907079696656\n",
      "Train epoch:1, batch index:367, loss:0.011506005525588989\n",
      "Train epoch:1, batch index:368, loss:0.011510875225067139\n",
      "Train epoch:1, batch index:369, loss:0.011508804559707642\n",
      "Train epoch:1, batch index:370, loss:0.011508554220199585\n",
      "Train epoch:1, batch index:371, loss:0.011510140895843506\n",
      "Train epoch:1, batch index:372, loss:0.011506625413894654\n",
      "Train epoch:1, batch index:373, loss:0.0115079927444458\n",
      "Train epoch:1, batch index:374, loss:0.011507039070129394\n",
      "Train epoch:1, batch index:375, loss:0.01150928258895874\n",
      "Train epoch:1, batch index:376, loss:0.011506633758544922\n",
      "Train epoch:1, batch index:377, loss:0.011510100364685059\n",
      "Train epoch:1, batch index:378, loss:0.011508909463882446\n",
      "Train epoch:1, batch index:379, loss:0.011508480310440064\n",
      "Train epoch:1, batch index:380, loss:0.011509544849395752\n",
      "Train epoch:1, batch index:381, loss:0.011510000228881836\n",
      "Train epoch:1, batch index:382, loss:0.011506621837615966\n",
      "Train epoch:1, batch index:383, loss:0.011509143114089966\n",
      "Train epoch:1, batch index:384, loss:0.01150870680809021\n",
      "Train epoch:1, batch index:385, loss:0.011508939266204833\n",
      "Train epoch:1, batch index:386, loss:0.011508809328079224\n",
      "Train epoch:1, batch index:387, loss:0.011508575677871703\n",
      "Train epoch:1, batch index:388, loss:0.011505557298660278\n",
      "Train epoch:1, batch index:389, loss:0.011508115530014039\n",
      "Train epoch:1, batch index:390, loss:0.011508291959762574\n",
      "Train epoch:1, batch index:391, loss:0.011509122848510743\n",
      "Train epoch:1, batch index:392, loss:0.011505768299102784\n",
      "Train epoch:1, batch index:393, loss:0.011506923437118531\n",
      "Train epoch:1, batch index:394, loss:0.011509149074554444\n",
      "Train epoch:1, batch index:395, loss:0.011510434150695801\n",
      "Train epoch:1, batch index:396, loss:0.011507093906402588\n",
      "Train epoch:1, batch index:397, loss:0.011509569883346558\n",
      "Train epoch:1, batch index:398, loss:0.011509617567062378\n",
      "Train epoch:1, batch index:399, loss:0.011508032083511352\n",
      "Train epoch:1, batch index:400, loss:0.011509369611740112\n",
      "Train epoch:1, batch index:401, loss:0.011509020328521729\n",
      "Train epoch:1, batch index:402, loss:0.011510388851165771\n",
      "Train epoch:1, batch index:403, loss:0.011508135795593262\n",
      "Train epoch:1, batch index:404, loss:0.011511144638061523\n",
      "Train epoch:1, batch index:405, loss:0.011507922410964965\n",
      "Train epoch:1, batch index:406, loss:0.011509355306625366\n",
      "Train epoch:1, batch index:407, loss:0.011509017944335937\n",
      "Train epoch:1, batch index:408, loss:0.01150855541229248\n",
      "Train epoch:1, batch index:409, loss:0.011510961055755616\n",
      "Train epoch:1, batch index:410, loss:0.011506942510604858\n",
      "Train epoch:1, batch index:411, loss:0.011508805751800537\n",
      "Train epoch:1, batch index:412, loss:0.01150721788406372\n",
      "Train epoch:1, batch index:413, loss:0.011510212421417237\n",
      "Train epoch:1, batch index:414, loss:0.011509289741516113\n",
      "Train epoch:1, batch index:415, loss:0.011506367921829224\n",
      "Train epoch:1, batch index:416, loss:0.011505972146987915\n",
      "Train epoch:1, batch index:418, loss:0.011509544849395752\n",
      "Train epoch:1, batch index:419, loss:0.011506592035293578\n",
      "Train epoch:1, batch index:420, loss:0.01151156187057495\n",
      "Train epoch:1, batch index:421, loss:0.01150896430015564\n",
      "Train epoch:1, batch index:422, loss:0.011505612134933473\n",
      "Train epoch:1, batch index:423, loss:0.011509166955947876\n",
      "Train epoch:1, batch index:424, loss:0.011506032943725587\n",
      "Train epoch:1, batch index:425, loss:0.011508686542510986\n",
      "Train epoch:1, batch index:426, loss:0.011508862972259521\n",
      "Train epoch:1, batch index:427, loss:0.01151030421257019\n",
      "Train epoch:1, batch index:428, loss:0.011508671045303344\n",
      "Train epoch:1, batch index:429, loss:0.011509069204330445\n",
      "Train epoch:1, batch index:430, loss:0.011508644819259643\n",
      "Train epoch:1, batch index:431, loss:0.01150816798210144\n",
      "Train epoch:1, batch index:432, loss:0.011510509252548217\n",
      "Train epoch:1, batch index:433, loss:0.01150600790977478\n",
      "Train epoch:1, batch index:434, loss:0.01150903820991516\n",
      "Train epoch:1, batch index:435, loss:0.011509737968444823\n",
      "Train epoch:1, batch index:436, loss:0.011507693529129028\n",
      "Train epoch:1, batch index:437, loss:0.011506166458129883\n",
      "Train epoch:1, batch index:438, loss:0.011509687900543212\n",
      "Train epoch:1, batch index:439, loss:0.011507989168167114\n",
      "Train epoch:1, batch index:440, loss:0.01150943398475647\n",
      "Train epoch:1, batch index:441, loss:0.011511616706848145\n",
      "Train epoch:1, batch index:442, loss:0.011507245302200318\n",
      "Train epoch:1, batch index:443, loss:0.01150825023651123\n",
      "Train epoch:1, batch index:444, loss:0.011508318185806275\n",
      "Train epoch:1, batch index:445, loss:0.01150644302368164\n",
      "Train epoch:1, batch index:446, loss:0.011506369113922119\n",
      "Train epoch:1, batch index:447, loss:0.011507036685943604\n",
      "Train epoch:1, batch index:448, loss:0.011510193347930908\n",
      "Train epoch:1, batch index:449, loss:0.01150971531867981\n",
      "Train epoch:1, batch index:450, loss:0.01150813102722168\n",
      "Train epoch:1, batch index:451, loss:0.011506648063659667\n",
      "Train epoch:1, batch index:452, loss:0.011507716178894043\n",
      "Train epoch:1, batch index:453, loss:0.011508616209030152\n",
      "Train epoch:1, batch index:454, loss:0.011505184173583984\n",
      "Train epoch:1, batch index:455, loss:0.011509841680526734\n",
      "Train epoch:1, batch index:456, loss:0.01151087999343872\n",
      "Train epoch:1, batch index:457, loss:0.01150962471961975\n",
      "Train epoch:1, batch index:458, loss:0.01150735855102539\n",
      "Train epoch:1, batch index:459, loss:0.0115095055103302\n",
      "Train epoch:1, batch index:460, loss:0.011506930589675904\n",
      "Train epoch:1, batch index:461, loss:0.011508276462554931\n",
      "Train epoch:1, batch index:462, loss:0.011508853435516357\n",
      "Train epoch:1, batch index:463, loss:0.01150769591331482\n",
      "Train epoch:1, batch index:464, loss:0.011508852243423462\n",
      "Train epoch:1, batch index:465, loss:0.01150761365890503\n",
      "Train epoch:1, batch index:466, loss:0.01150813341140747\n",
      "Train epoch:1, batch index:467, loss:0.01150760531425476\n",
      "Train epoch:1, batch index:468, loss:0.011507771015167235\n",
      "Train epoch:1, batch index:469, loss:0.01150922417640686\n",
      "Train epoch:1, batch index:470, loss:0.011508727073669433\n",
      "Train epoch:1, batch index:471, loss:0.011508926153182983\n",
      "Train epoch:1, batch index:472, loss:0.011508783102035522\n",
      "Train epoch:1, batch index:473, loss:0.01150970220565796\n",
      "Train epoch:1, batch index:474, loss:0.011507576704025269\n",
      "Train epoch:1, batch index:475, loss:0.011509448289871216\n",
      "Train epoch:1, batch index:476, loss:0.01150733232498169\n",
      "Train epoch:1, batch index:477, loss:0.011509391069412232\n",
      "Train epoch:1, batch index:478, loss:0.011505587100982666\n",
      "Train epoch:1, batch index:479, loss:0.011508969068527221\n",
      "Train epoch:1, batch index:480, loss:0.011505382061004639\n",
      "Train epoch:1, batch index:481, loss:0.011508668661117554\n",
      "Train epoch:1, batch index:482, loss:0.011507633924484253\n",
      "Train epoch:1, batch index:483, loss:0.011508469581604003\n",
      "Train epoch:1, batch index:484, loss:0.011508376598358154\n",
      "Train epoch:1, batch index:485, loss:0.011508307456970214\n",
      "Train epoch:1, batch index:486, loss:0.011507420539855958\n",
      "Train epoch:1, batch index:487, loss:0.01150680661201477\n",
      "Train epoch:1, batch index:488, loss:0.011506942510604858\n",
      "Train epoch:1, batch index:489, loss:0.011509091854095458\n",
      "Train epoch:1, batch index:490, loss:0.011507028341293335\n",
      "Train epoch:1, batch index:491, loss:0.011509085893630982\n",
      "Train epoch:1, batch index:492, loss:0.011506268978118897\n",
      "Train epoch:1, batch index:493, loss:0.011507741212844848\n",
      "Train epoch:1, batch index:494, loss:0.011506102085113525\n",
      "Train epoch:1, batch index:495, loss:0.01150668740272522\n",
      "Train epoch:1, batch index:496, loss:0.011507548093795776\n",
      "Train epoch:1, batch index:497, loss:0.011508601903915405\n",
      "Train epoch:1, batch index:498, loss:0.011508798599243164\n",
      "Train epoch:1, batch index:499, loss:0.011507964134216309\n",
      "Train epoch:1, batch index:500, loss:0.01150654673576355\n",
      "Train epoch:1, batch index:501, loss:0.011507331132888795\n",
      "Train epoch:1, batch index:502, loss:0.011505224704742432\n",
      "Train epoch:1, batch index:503, loss:0.011507047414779663\n",
      "Train epoch:1, batch index:504, loss:0.01150710105895996\n",
      "Train epoch:1, batch index:505, loss:0.011508203744888305\n",
      "Train epoch:1, batch index:506, loss:0.011508471965789795\n",
      "Train epoch:1, batch index:507, loss:0.011507028341293335\n",
      "Train epoch:1, batch index:508, loss:0.011505941152572632\n",
      "Train epoch:1, batch index:509, loss:0.011509196758270264\n",
      "Train epoch:1, batch index:510, loss:0.011509344577789307\n",
      "Train epoch:1, batch index:511, loss:0.011507928371429443\n",
      "Train epoch:1, batch index:512, loss:0.011506510972976685\n",
      "Train epoch:1, batch index:513, loss:0.011510099172592164\n",
      "Train epoch:1, batch index:514, loss:0.01150841474533081\n",
      "Train epoch:1, batch index:515, loss:0.011508194208145141\n",
      "Train epoch:1, batch index:516, loss:0.01150516390800476\n",
      "Train epoch:1, batch index:517, loss:0.011507580280303955\n",
      "Train epoch:1, batch index:518, loss:0.01150591254234314\n",
      "Train epoch:1, batch index:519, loss:0.01150928258895874\n",
      "Train epoch:1, batch index:520, loss:0.011507208347320557\n",
      "Train epoch:1, batch index:521, loss:0.011504799127578735\n",
      "Train epoch:1, batch index:522, loss:0.011507424116134644\n",
      "Train epoch:1, batch index:523, loss:0.011508108377456664\n",
      "Train epoch:1, batch index:524, loss:0.011508162021636964\n",
      "Train epoch:1, batch index:525, loss:0.011506733894348144\n",
      "Train epoch:1, batch index:526, loss:0.011506377458572388\n",
      "Train epoch:1, batch index:527, loss:0.011506404876708985\n",
      "Train epoch:1, batch index:528, loss:0.011507418155670166\n",
      "Train epoch:1, batch index:529, loss:0.011507952213287353\n",
      "Train epoch:1, batch index:530, loss:0.011508690118789673\n",
      "Train epoch:1, batch index:531, loss:0.01151004433631897\n",
      "Train epoch:1, batch index:532, loss:0.011510567665100098\n",
      "Train epoch:1, batch index:533, loss:0.011507352590560913\n",
      "Train epoch:1, batch index:534, loss:0.011509695053100587\n",
      "Train epoch:1, batch index:535, loss:0.011509921550750733\n",
      "Train epoch:1, batch index:536, loss:0.011506234407424926\n",
      "Train epoch:1, batch index:537, loss:0.011507272720336914\n",
      "Train epoch:1, batch index:538, loss:0.011506237983703614\n",
      "Train epoch:1, batch index:539, loss:0.011506373882293702\n",
      "Train epoch:1, batch index:540, loss:0.011509610414505005\n",
      "Train epoch:1, batch index:541, loss:0.011508541107177734\n",
      "Train epoch:1, batch index:542, loss:0.011509789228439331\n",
      "Train epoch:1, batch index:543, loss:0.011505612134933473\n",
      "Train epoch:1, batch index:544, loss:0.01150721788406372\n",
      "Train epoch:1, batch index:545, loss:0.011508798599243164\n",
      "Train epoch:1, batch index:546, loss:0.011507705450057984\n",
      "Train epoch:1, batch index:547, loss:0.011508138179779052\n",
      "Train epoch:1, batch index:548, loss:0.0115096116065979\n",
      "Train epoch:1, batch index:549, loss:0.011508598327636718\n",
      "Train epoch:1, batch index:550, loss:0.011509295701980591\n",
      "Train epoch:1, batch index:551, loss:0.011507500410079956\n",
      "Train epoch:1, batch index:552, loss:0.0115083646774292\n",
      "Train epoch:1, batch index:553, loss:0.01150519609451294\n",
      "Train epoch:1, batch index:554, loss:0.011506979465484618\n",
      "Train epoch:1, batch index:555, loss:0.011508692502975464\n",
      "Train epoch:1, batch index:556, loss:0.011508172750473023\n",
      "Train epoch:1, batch index:557, loss:0.011510125398635863\n",
      "Train epoch:1, batch index:558, loss:0.011508665084838866\n",
      "Train epoch:1, batch index:559, loss:0.011508381366729737\n",
      "Train epoch:1, batch index:560, loss:0.011509310007095336\n",
      "Train epoch:1, batch index:561, loss:0.01150792121887207\n",
      "Train epoch:1, batch index:562, loss:0.011508468389511108\n",
      "Train epoch:1, batch index:563, loss:0.011506580114364624\n",
      "Train epoch:1, batch index:564, loss:0.011508018970489501\n",
      "Train epoch:1, batch index:565, loss:0.011506755352020264\n",
      "Train epoch:1, batch index:566, loss:0.011508187055587768\n",
      "Train epoch:1, batch index:567, loss:0.01150693416595459\n",
      "Train epoch:1, batch index:568, loss:0.011505637168884277\n",
      "Train epoch:1, batch index:569, loss:0.011507854461669922\n",
      "Train epoch:1, batch index:570, loss:0.01150673747062683\n",
      "Train epoch:1, batch index:571, loss:0.011508729457855225\n",
      "Train epoch:1, batch index:572, loss:0.011509336233139038\n",
      "Train epoch:1, batch index:573, loss:0.011509878635406494\n",
      "Train epoch:1, batch index:574, loss:0.01150976300239563\n",
      "Train epoch:1, batch index:575, loss:0.011504529714584351\n",
      "Train epoch:1, batch index:576, loss:0.011506414413452149\n",
      "Train epoch:1, batch index:577, loss:0.011509512662887572\n",
      "Train epoch:1, batch index:578, loss:0.011509020328521729\n",
      "Train epoch:1, batch index:579, loss:0.011508179903030396\n",
      "Train epoch:1, batch index:580, loss:0.011507184505462646\n",
      "Train epoch:1, batch index:581, loss:0.011509019136428832\n",
      "Train epoch:1, batch index:582, loss:0.011505889892578124\n",
      "Train epoch:1, batch index:583, loss:0.011505696773529053\n",
      "Train epoch:1, batch index:584, loss:0.0115066397190094\n",
      "Train epoch:1, batch index:585, loss:0.011508382558822632\n",
      "Train epoch:1, batch index:586, loss:0.011507799625396728\n",
      "Train epoch:1, batch index:587, loss:0.011509610414505005\n",
      "Train epoch:1, batch index:588, loss:0.01150739312171936\n",
      "Train epoch:1, batch index:589, loss:0.011507781744003296\n",
      "Train epoch:1, batch index:590, loss:0.011510816812515258\n",
      "Train epoch:1, batch index:591, loss:0.011507104635238647\n",
      "Train epoch:1, batch index:592, loss:0.011507929563522338\n",
      "Train epoch:1, batch index:593, loss:0.011507189273834229\n",
      "Train epoch:1, batch index:594, loss:0.011507292985916137\n",
      "Train epoch:1, batch index:595, loss:0.011508617401123047\n",
      "Train epoch:1, batch index:596, loss:0.011505945920944213\n",
      "Train epoch:1, batch index:597, loss:0.011505225896835327\n",
      "Train epoch:1, batch index:598, loss:0.011506776809692382\n",
      "Train epoch:1, batch index:599, loss:0.011509603261947632\n",
      "Train epoch:1, batch index:600, loss:0.011507304906845093\n",
      "Train epoch:1, batch index:601, loss:0.011509243249893188\n",
      "Train epoch:1, batch index:602, loss:0.01151006817817688\n",
      "Train epoch:1, batch index:603, loss:0.011508493423461915\n",
      "Train epoch:1, batch index:604, loss:0.011509571075439453\n",
      "Train epoch:1, batch index:605, loss:0.01150870680809021\n",
      "Train epoch:1, batch index:606, loss:0.011508221626281739\n",
      "Train epoch:1, batch index:607, loss:0.011506237983703614\n",
      "Train epoch:1, batch index:608, loss:0.01150944948196411\n",
      "Train epoch:1, batch index:609, loss:0.011508247852325439\n",
      "Train epoch:1, batch index:610, loss:0.011506466865539551\n",
      "Train epoch:1, batch index:611, loss:0.011505420207977296\n",
      "Train epoch:1, batch index:612, loss:0.01150753378868103\n",
      "Train epoch:1, batch index:613, loss:0.011510491371154785\n",
      "Train epoch:1, batch index:614, loss:0.011510202884674072\n",
      "Train epoch:1, batch index:615, loss:0.011508138179779052\n",
      "Train epoch:1, batch index:616, loss:0.011504552364349364\n",
      "Train epoch:1, batch index:618, loss:0.011508175134658814\n",
      "Train epoch:1, batch index:619, loss:0.011509703397750854\n",
      "Train epoch:1, batch index:620, loss:0.011505687236785888\n",
      "Train epoch:1, batch index:621, loss:0.011506434679031372\n",
      "Train epoch:1, batch index:622, loss:0.011508839130401611\n",
      "Train epoch:1, batch index:623, loss:0.011508140563964844\n",
      "Train epoch:1, batch index:624, loss:0.011506402492523193\n",
      "Train epoch:1, batch index:625, loss:0.01150883436203003\n",
      "Train epoch:1, batch index:626, loss:0.011510807275772094\n",
      "Train epoch:1, batch index:627, loss:0.011507220268249511\n",
      "Train epoch:1, batch index:628, loss:0.011510212421417237\n",
      "Train epoch:1, batch index:629, loss:0.0115082585811615\n",
      "Train epoch:1, batch index:630, loss:0.011505857706069947\n",
      "Train epoch:1, batch index:631, loss:0.011509515047073364\n",
      "Train epoch:1, batch index:632, loss:0.011508030891418457\n",
      "Train epoch:1, batch index:633, loss:0.011508560180664063\n",
      "Train epoch:1, batch index:634, loss:0.011507933139801025\n",
      "Train epoch:1, batch index:635, loss:0.011506147384643554\n",
      "Train epoch:1, batch index:636, loss:0.011505337953567505\n",
      "Train epoch:1, batch index:637, loss:0.011508116722106934\n",
      "Train epoch:1, batch index:638, loss:0.011505674123764038\n",
      "Train epoch:1, batch index:639, loss:0.011509029865264893\n",
      "Train epoch:1, batch index:640, loss:0.011508405208587646\n",
      "Train epoch:1, batch index:641, loss:0.011508623361587525\n",
      "Train epoch:1, batch index:642, loss:0.011507478952407836\n",
      "Train epoch:1, batch index:643, loss:0.011506054401397705\n",
      "Train epoch:1, batch index:644, loss:0.011505426168441772\n",
      "Train epoch:1, batch index:645, loss:0.011510334014892577\n",
      "Train epoch:1, batch index:646, loss:0.011507623195648194\n",
      "Train epoch:1, batch index:647, loss:0.011507117748260498\n",
      "Train epoch:1, batch index:648, loss:0.011506929397583007\n",
      "Train epoch:1, batch index:649, loss:0.011507066488265992\n",
      "Train epoch:1, batch index:650, loss:0.011507905721664428\n",
      "Train epoch:1, batch index:651, loss:0.01150700330734253\n",
      "Train epoch:1, batch index:652, loss:0.011508197784423828\n",
      "Train epoch:1, batch index:653, loss:0.011506866216659545\n",
      "Train epoch:1, batch index:654, loss:0.011507357358932496\n",
      "Train epoch:1, batch index:655, loss:0.011508829593658447\n",
      "Train epoch:1, batch index:656, loss:0.011507540941238403\n",
      "Train epoch:1, batch index:657, loss:0.011507197618484497\n",
      "Train epoch:1, batch index:658, loss:0.011509972810745239\n",
      "Train epoch:1, batch index:659, loss:0.011507079601287842\n",
      "Train epoch:1, batch index:660, loss:0.011507792472839356\n",
      "Train epoch:1, batch index:661, loss:0.01150887131690979\n",
      "Train epoch:1, batch index:662, loss:0.011506896018981933\n",
      "Train epoch:1, batch index:663, loss:0.011509047746658325\n",
      "Train epoch:1, batch index:664, loss:0.01150539517402649\n",
      "Train epoch:1, batch index:665, loss:0.011505833864212035\n",
      "Train epoch:1, batch index:666, loss:0.011508903503417968\n",
      "Train epoch:1, batch index:667, loss:0.011508219242095948\n",
      "Train epoch:1, batch index:668, loss:0.011506655216217042\n",
      "Train epoch:1, batch index:669, loss:0.011506035327911376\n",
      "Train epoch:1, batch index:670, loss:0.011506742238998413\n",
      "Train epoch:1, batch index:671, loss:0.011506412029266357\n",
      "Train epoch:1, batch index:672, loss:0.011507480144500733\n",
      "Train epoch:1, batch index:673, loss:0.011506844758987427\n",
      "Train epoch:1, batch index:674, loss:0.011508604288101196\n",
      "Train epoch:1, batch index:675, loss:0.01150618314743042\n",
      "Train epoch:1, batch index:676, loss:0.011507139205932618\n",
      "Train epoch:1, batch index:677, loss:0.01150793194770813\n",
      "Train epoch:1, batch index:678, loss:0.011506669521331787\n",
      "Train epoch:1, batch index:679, loss:0.011505961418151855\n",
      "Train epoch:1, batch index:680, loss:0.011508800983428956\n",
      "Train epoch:1, batch index:681, loss:0.011505459547042846\n",
      "Train epoch:1, batch index:682, loss:0.01150619387626648\n",
      "Train epoch:1, batch index:683, loss:0.011508198976516724\n",
      "Train epoch:1, batch index:684, loss:0.011507065296173095\n",
      "Train epoch:1, batch index:685, loss:0.011508458852767944\n",
      "Train epoch:1, batch index:686, loss:0.011510463953018189\n",
      "Train epoch:1, batch index:687, loss:0.011507314443588258\n",
      "Train epoch:1, batch index:688, loss:0.01150681495666504\n",
      "Train epoch:1, batch index:689, loss:0.011506870985031128\n",
      "Train epoch:1, batch index:690, loss:0.011509008407592773\n",
      "Train epoch:1, batch index:691, loss:0.011507372856140136\n",
      "Train epoch:1, batch index:692, loss:0.01150741457939148\n",
      "Train epoch:1, batch index:693, loss:0.011508543491363526\n",
      "Train epoch:1, batch index:694, loss:0.01150525450706482\n",
      "Train epoch:1, batch index:695, loss:0.011506941318511963\n",
      "Train epoch:1, batch index:696, loss:0.01150798201560974\n",
      "Train epoch:1, batch index:697, loss:0.011505440473556519\n",
      "Train epoch:1, batch index:698, loss:0.011505777835845948\n",
      "Train epoch:1, batch index:699, loss:0.011509908437728882\n",
      "Train epoch:1, batch index:700, loss:0.011509262323379517\n",
      "Train epoch:1, batch index:701, loss:0.01150932788848877\n",
      "Train epoch:1, batch index:702, loss:0.011507632732391358\n",
      "Train epoch:1, batch index:703, loss:0.011507197618484497\n",
      "Train epoch:1, batch index:704, loss:0.011505190134048462\n",
      "Train epoch:1, batch index:705, loss:0.011507041454315185\n",
      "Train epoch:1, batch index:706, loss:0.011508390903472901\n",
      "Train epoch:1, batch index:707, loss:0.011509270668029785\n",
      "Train epoch:1, batch index:708, loss:0.0115079665184021\n",
      "Train epoch:1, batch index:709, loss:0.011505422592163085\n",
      "Train epoch:1, batch index:710, loss:0.011508278846740723\n",
      "Train epoch:1, batch index:711, loss:0.0115072762966156\n",
      "Train epoch:1, batch index:712, loss:0.011506118774414063\n",
      "Train epoch:1, batch index:713, loss:0.01150603175163269\n",
      "Train epoch:1, batch index:714, loss:0.01150700569152832\n",
      "Train epoch:1, batch index:715, loss:0.011507306098937988\n",
      "Train epoch:1, batch index:716, loss:0.011507452726364135\n",
      "Train epoch:1, batch index:717, loss:0.011507104635238647\n",
      "Train epoch:1, batch index:718, loss:0.01150635838508606\n",
      "Train epoch:1, batch index:719, loss:0.011508822441101074\n",
      "Train epoch:1, batch index:720, loss:0.011507502794265746\n",
      "Train epoch:1, batch index:721, loss:0.01150475263595581\n",
      "Train epoch:1, batch index:722, loss:0.011507056951522827\n",
      "Train epoch:1, batch index:723, loss:0.011506931781768799\n",
      "Train epoch:1, batch index:724, loss:0.011507343053817749\n",
      "Train epoch:1, batch index:725, loss:0.011509169340133667\n",
      "Train epoch:1, batch index:726, loss:0.011505883932113648\n",
      "Train epoch:1, batch index:727, loss:0.01150770902633667\n",
      "Train epoch:1, batch index:728, loss:0.011506797075271606\n",
      "Train epoch:1, batch index:729, loss:0.011506527662277222\n",
      "Train epoch:1, batch index:730, loss:0.011509755849838257\n",
      "Train epoch:1, batch index:731, loss:0.011506314277648927\n",
      "Train epoch:1, batch index:732, loss:0.011507480144500733\n",
      "Train epoch:1, batch index:733, loss:0.011507530212402344\n",
      "Train epoch:1, batch index:734, loss:0.011506762504577637\n",
      "Train epoch:1, batch index:735, loss:0.011505845785140991\n",
      "Train epoch:1, batch index:736, loss:0.011509604454040527\n",
      "Train epoch:1, batch index:737, loss:0.011508029699325562\n",
      "Train epoch:1, batch index:738, loss:0.011506563425064087\n",
      "Train epoch:1, batch index:739, loss:0.011508975028991699\n",
      "Train epoch:1, batch index:740, loss:0.011509013175964356\n",
      "Train epoch:1, batch index:741, loss:0.011507188081741332\n",
      "Train epoch:1, batch index:742, loss:0.011507084369659424\n",
      "Train epoch:1, batch index:743, loss:0.011506693363189697\n",
      "Train epoch:1, batch index:744, loss:0.011504936218261718\n",
      "Train epoch:1, batch index:745, loss:0.011505147218704223\n",
      "Train epoch:1, batch index:746, loss:0.011506989002227783\n",
      "Train epoch:1, batch index:747, loss:0.011510077714920044\n",
      "Train epoch:1, batch index:748, loss:0.011507079601287842\n",
      "Train epoch:1, batch index:749, loss:0.011505239009857178\n",
      "Train epoch:1, batch index:750, loss:0.011506234407424926\n",
      "Train epoch:1, batch index:751, loss:0.011506677865982055\n",
      "Train epoch:1, batch index:752, loss:0.011507787704467774\n",
      "Train epoch:1, batch index:753, loss:0.011506592035293578\n",
      "Train epoch:1, batch index:754, loss:0.011508398056030274\n",
      "Train epoch:1, batch index:755, loss:0.011506993770599366\n",
      "Train epoch:1, batch index:756, loss:0.011507949829101562\n",
      "Train epoch:1, batch index:757, loss:0.011508946418762206\n",
      "Train epoch:1, batch index:758, loss:0.011507270336151123\n",
      "Train epoch:1, batch index:759, loss:0.01150692105293274\n",
      "Train epoch:1, batch index:760, loss:0.01150661826133728\n",
      "Train epoch:1, batch index:761, loss:0.011506024599075317\n",
      "Train epoch:1, batch index:762, loss:0.011508159637451172\n",
      "Train epoch:1, batch index:763, loss:0.011506476402282716\n",
      "Train epoch:1, batch index:764, loss:0.011507147550582885\n",
      "Train epoch:1, batch index:765, loss:0.011508204936981202\n",
      "Train epoch:1, batch index:766, loss:0.011507446765899659\n",
      "Train epoch:1, batch index:767, loss:0.011507860422134399\n",
      "Train epoch:1, batch index:768, loss:0.011507562398910522\n",
      "Train epoch:1, batch index:769, loss:0.011506531238555908\n",
      "Train epoch:1, batch index:770, loss:0.011507508754730224\n",
      "Train epoch:1, batch index:771, loss:0.01151017427444458\n",
      "Train epoch:1, batch index:772, loss:0.01150538682937622\n",
      "Train epoch:1, batch index:773, loss:0.011507624387741089\n",
      "Train epoch:1, batch index:774, loss:0.011506428718566894\n",
      "Train epoch:1, batch index:775, loss:0.011507401466369629\n",
      "Train epoch:1, batch index:776, loss:0.011506993770599366\n",
      "Train epoch:1, batch index:777, loss:0.01150586485862732\n",
      "Train epoch:1, batch index:778, loss:0.01150675892829895\n",
      "Train epoch:1, batch index:779, loss:0.011507790088653564\n",
      "Train epoch:1, batch index:780, loss:0.011507816314697265\n",
      "Train epoch:1, batch index:781, loss:0.01150641918182373\n",
      "Train epoch:2, batch index:0, loss:0.011507060527801514\n",
      "Train epoch:2, batch index:1, loss:0.011505918502807617\n",
      "Train epoch:2, batch index:2, loss:0.011508593559265137\n",
      "Train epoch:2, batch index:3, loss:0.011506485939025878\n",
      "Train epoch:2, batch index:4, loss:0.011505100727081299\n",
      "Train epoch:2, batch index:5, loss:0.01150498390197754\n",
      "Train epoch:2, batch index:6, loss:0.011504687070846557\n",
      "Train epoch:2, batch index:7, loss:0.01150662899017334\n",
      "Train epoch:2, batch index:8, loss:0.011507021188735962\n",
      "Train epoch:2, batch index:9, loss:0.01150854468345642\n",
      "Train epoch:2, batch index:10, loss:0.011507524251937866\n",
      "Train epoch:2, batch index:11, loss:0.011506541967391967\n",
      "Train epoch:2, batch index:12, loss:0.011508824825286866\n",
      "Train epoch:2, batch index:13, loss:0.01150520920753479\n",
      "Train epoch:2, batch index:14, loss:0.011505248546600342\n",
      "Train epoch:2, batch index:15, loss:0.01150815486907959\n",
      "Train epoch:2, batch index:16, loss:0.011505661010742187\n",
      "Train epoch:2, batch index:17, loss:0.011504912376403808\n",
      "Train epoch:2, batch index:18, loss:0.011509459018707275\n",
      "Train epoch:2, batch index:19, loss:0.011506688594818116\n",
      "Train epoch:2, batch index:20, loss:0.011508793830871581\n",
      "Train epoch:2, batch index:21, loss:0.011506012678146361\n",
      "Train epoch:2, batch index:22, loss:0.011504765748977661\n",
      "Train epoch:2, batch index:23, loss:0.01150550127029419\n",
      "Train epoch:2, batch index:24, loss:0.011507366895675658\n",
      "Train epoch:2, batch index:25, loss:0.01150620698928833\n",
      "Train epoch:2, batch index:26, loss:0.011509077548980713\n",
      "Train epoch:2, batch index:27, loss:0.011505343914031983\n",
      "Train epoch:2, batch index:28, loss:0.011505483388900757\n",
      "Train epoch:2, batch index:29, loss:0.011508193016052246\n",
      "Train epoch:2, batch index:30, loss:0.011504244804382325\n",
      "Train epoch:2, batch index:31, loss:0.0115060293674469\n",
      "Train epoch:2, batch index:32, loss:0.011504406929016114\n",
      "Train epoch:2, batch index:33, loss:0.011509385108947754\n",
      "Train epoch:2, batch index:34, loss:0.01150704026222229\n",
      "Train epoch:2, batch index:36, loss:0.011508003473281861\n",
      "Train epoch:2, batch index:37, loss:0.011507514715194702\n",
      "Train epoch:2, batch index:38, loss:0.01150418519973755\n",
      "Train epoch:2, batch index:39, loss:0.011505266427993774\n",
      "Train epoch:2, batch index:40, loss:0.011505817174911498\n",
      "Train epoch:2, batch index:41, loss:0.011507115364074706\n",
      "Train epoch:2, batch index:42, loss:0.011502736806869506\n",
      "Train epoch:2, batch index:43, loss:0.01150467038154602\n",
      "Train epoch:2, batch index:44, loss:0.011506185531616211\n",
      "Train epoch:2, batch index:45, loss:0.011504493951797486\n",
      "Train epoch:2, batch index:46, loss:0.01150710940361023\n",
      "Train epoch:2, batch index:47, loss:0.011509454250335694\n",
      "Train epoch:2, batch index:48, loss:0.011507333517074584\n",
      "Train epoch:2, batch index:49, loss:0.011504504680633545\n",
      "Train epoch:2, batch index:50, loss:0.011507261991500855\n",
      "Train epoch:2, batch index:51, loss:0.011509029865264893\n",
      "Train epoch:2, batch index:52, loss:0.011505124568939208\n",
      "Train epoch:2, batch index:53, loss:0.011507339477539062\n",
      "Train epoch:2, batch index:54, loss:0.011506084203720093\n",
      "Train epoch:2, batch index:55, loss:0.011508444547653198\n",
      "Train epoch:2, batch index:56, loss:0.011507848501205445\n",
      "Train epoch:2, batch index:57, loss:0.011503435373306274\n",
      "Train epoch:2, batch index:58, loss:0.011508427858352661\n",
      "Train epoch:2, batch index:59, loss:0.01150611400604248\n",
      "Train epoch:2, batch index:60, loss:0.011506937742233277\n",
      "Train epoch:2, batch index:61, loss:0.011505011320114136\n",
      "Train epoch:2, batch index:62, loss:0.011507585048675537\n",
      "Train epoch:2, batch index:63, loss:0.011503217220306396\n",
      "Train epoch:2, batch index:64, loss:0.01150671362876892\n",
      "Train epoch:2, batch index:65, loss:0.01150834321975708\n",
      "Train epoch:2, batch index:66, loss:0.01150876522064209\n",
      "Train epoch:2, batch index:67, loss:0.011507303714752197\n",
      "Train epoch:2, batch index:68, loss:0.011507037878036499\n",
      "Train epoch:2, batch index:69, loss:0.011506733894348144\n",
      "Train epoch:2, batch index:70, loss:0.011504834890365601\n",
      "Train epoch:2, batch index:71, loss:0.011506279706954956\n",
      "Train epoch:2, batch index:72, loss:0.011507391929626465\n",
      "Train epoch:2, batch index:73, loss:0.011506931781768799\n",
      "Train epoch:2, batch index:74, loss:0.011505738496780396\n",
      "Train epoch:2, batch index:75, loss:0.011505873203277587\n",
      "Train epoch:2, batch index:76, loss:0.011505855321884155\n",
      "Train epoch:2, batch index:77, loss:0.01150679111480713\n",
      "Train epoch:2, batch index:78, loss:0.011506471633911133\n",
      "Train epoch:2, batch index:79, loss:0.011504689455032349\n",
      "Train epoch:2, batch index:80, loss:0.011508634090423584\n",
      "Train epoch:2, batch index:81, loss:0.011505770683288574\n",
      "Train epoch:2, batch index:82, loss:0.011507056951522827\n",
      "Train epoch:2, batch index:83, loss:0.01150745987892151\n",
      "Train epoch:2, batch index:84, loss:0.011505612134933473\n",
      "Train epoch:2, batch index:85, loss:0.011506527662277222\n",
      "Train epoch:2, batch index:86, loss:0.011507836580276489\n",
      "Train epoch:2, batch index:87, loss:0.011506998538970947\n",
      "Train epoch:2, batch index:88, loss:0.011506669521331787\n",
      "Train epoch:2, batch index:89, loss:0.01150867223739624\n",
      "Train epoch:2, batch index:90, loss:0.0115057635307312\n",
      "Train epoch:2, batch index:91, loss:0.01150944948196411\n",
      "Train epoch:2, batch index:92, loss:0.011508445739746093\n",
      "Train epoch:2, batch index:93, loss:0.011508409976959228\n",
      "Train epoch:2, batch index:94, loss:0.011506034135818482\n",
      "Train epoch:2, batch index:95, loss:0.011505491733551025\n",
      "Train epoch:2, batch index:96, loss:0.011507568359375\n",
      "Train epoch:2, batch index:97, loss:0.01150598406791687\n",
      "Train epoch:2, batch index:98, loss:0.011505671739578248\n",
      "Train epoch:2, batch index:99, loss:0.011507782936096191\n",
      "Train epoch:2, batch index:100, loss:0.011506928205490112\n",
      "Train epoch:2, batch index:101, loss:0.011507889032363891\n",
      "Train epoch:2, batch index:102, loss:0.011508209705352783\n",
      "Train epoch:2, batch index:103, loss:0.011507371664047241\n",
      "Train epoch:2, batch index:104, loss:0.011504811048507691\n",
      "Train epoch:2, batch index:105, loss:0.011506799459457397\n",
      "Train epoch:2, batch index:106, loss:0.011504133939743042\n",
      "Train epoch:2, batch index:107, loss:0.011506900787353516\n",
      "Train epoch:2, batch index:108, loss:0.011507135629653931\n",
      "Train epoch:2, batch index:109, loss:0.011504050493240357\n",
      "Train epoch:2, batch index:110, loss:0.011506706476211548\n",
      "Train epoch:2, batch index:111, loss:0.01150673747062683\n",
      "Train epoch:2, batch index:112, loss:0.011508197784423828\n",
      "Train epoch:2, batch index:113, loss:0.011507505178451538\n",
      "Train epoch:2, batch index:114, loss:0.011504414081573487\n",
      "Train epoch:2, batch index:115, loss:0.011505792140960694\n",
      "Train epoch:2, batch index:116, loss:0.011507636308670044\n",
      "Train epoch:2, batch index:117, loss:0.011505042314529418\n",
      "Train epoch:2, batch index:118, loss:0.011506423950195313\n",
      "Train epoch:2, batch index:119, loss:0.011506706476211548\n",
      "Train epoch:2, batch index:120, loss:0.011506438255310059\n",
      "Train epoch:2, batch index:121, loss:0.011507301330566407\n",
      "Train epoch:2, batch index:122, loss:0.011504933834075928\n",
      "Train epoch:2, batch index:123, loss:0.011505846977233886\n",
      "Train epoch:2, batch index:124, loss:0.011508443355560304\n",
      "Train epoch:2, batch index:125, loss:0.011504762172698975\n",
      "Train epoch:2, batch index:126, loss:0.011505503654479981\n",
      "Train epoch:2, batch index:127, loss:0.011505609750747681\n",
      "Train epoch:2, batch index:128, loss:0.011505992412567138\n",
      "Train epoch:2, batch index:129, loss:0.011507489681243897\n",
      "Train epoch:2, batch index:130, loss:0.011508228778839112\n",
      "Train epoch:2, batch index:131, loss:0.011508876085281372\n",
      "Train epoch:2, batch index:132, loss:0.01150578260421753\n",
      "Train epoch:2, batch index:133, loss:0.01150614619255066\n",
      "Train epoch:2, batch index:134, loss:0.011505295038223267\n",
      "Train epoch:2, batch index:135, loss:0.011508071422576904\n",
      "Train epoch:2, batch index:136, loss:0.011504141092300415\n",
      "Train epoch:2, batch index:137, loss:0.011504567861557007\n",
      "Train epoch:2, batch index:138, loss:0.011507400274276734\n",
      "Train epoch:2, batch index:139, loss:0.011507333517074584\n",
      "Train epoch:2, batch index:140, loss:0.011506435871124267\n",
      "Train epoch:2, batch index:141, loss:0.01150525689125061\n",
      "Train epoch:2, batch index:142, loss:0.011506147384643554\n",
      "Train epoch:2, batch index:143, loss:0.011505234241485595\n",
      "Train epoch:2, batch index:144, loss:0.011505931615829468\n",
      "Train epoch:2, batch index:145, loss:0.011507670879364013\n",
      "Train epoch:2, batch index:146, loss:0.01150356888771057\n",
      "Train epoch:2, batch index:147, loss:0.011507641077041626\n",
      "Train epoch:2, batch index:148, loss:0.011507951021194458\n",
      "Train epoch:2, batch index:149, loss:0.011506249904632568\n",
      "Train epoch:2, batch index:150, loss:0.011508060693740845\n",
      "Train epoch:2, batch index:151, loss:0.011507117748260498\n",
      "Train epoch:2, batch index:152, loss:0.011504950523376465\n",
      "Train epoch:2, batch index:153, loss:0.011506105661392213\n",
      "Train epoch:2, batch index:154, loss:0.011504344940185547\n",
      "Train epoch:2, batch index:155, loss:0.011507437229156495\n",
      "Train epoch:2, batch index:156, loss:0.011506991386413574\n",
      "Train epoch:2, batch index:157, loss:0.011505073308944702\n",
      "Train epoch:2, batch index:158, loss:0.011505714654922485\n",
      "Train epoch:2, batch index:159, loss:0.01150924563407898\n",
      "Train epoch:2, batch index:160, loss:0.01150673508644104\n",
      "Train epoch:2, batch index:161, loss:0.011505744457244872\n",
      "Train epoch:2, batch index:162, loss:0.011507126092910767\n",
      "Train epoch:2, batch index:163, loss:0.011508865356445313\n",
      "Train epoch:2, batch index:164, loss:0.011505956649780274\n",
      "Train epoch:2, batch index:165, loss:0.011505916118621826\n",
      "Train epoch:2, batch index:166, loss:0.011504206657409668\n",
      "Train epoch:2, batch index:167, loss:0.01150689959526062\n",
      "Train epoch:2, batch index:168, loss:0.011504287719726563\n",
      "Train epoch:2, batch index:169, loss:0.01150668978691101\n",
      "Train epoch:2, batch index:170, loss:0.011506789922714233\n",
      "Train epoch:2, batch index:171, loss:0.011506106853485108\n",
      "Train epoch:2, batch index:172, loss:0.011509898900985718\n",
      "Train epoch:2, batch index:173, loss:0.011504437923431397\n",
      "Train epoch:2, batch index:174, loss:0.011507216691970825\n",
      "Train epoch:2, batch index:175, loss:0.01150725245475769\n",
      "Train epoch:2, batch index:176, loss:0.011507123708724976\n",
      "Train epoch:2, batch index:177, loss:0.01150489091873169\n",
      "Train epoch:2, batch index:178, loss:0.011507527828216553\n",
      "Train epoch:2, batch index:179, loss:0.011506271362304688\n",
      "Train epoch:2, batch index:180, loss:0.011504284143447875\n",
      "Train epoch:2, batch index:181, loss:0.011505951881408691\n",
      "Train epoch:2, batch index:182, loss:0.011506987810134888\n",
      "Train epoch:2, batch index:183, loss:0.01150816798210144\n",
      "Train epoch:2, batch index:184, loss:0.011506637334823608\n",
      "Train epoch:2, batch index:185, loss:0.011507090330123902\n",
      "Train epoch:2, batch index:186, loss:0.01150694966316223\n",
      "Train epoch:2, batch index:187, loss:0.011506130695343017\n",
      "Train epoch:2, batch index:188, loss:0.01150369167327881\n",
      "Train epoch:2, batch index:189, loss:0.011509480476379395\n",
      "Train epoch:2, batch index:190, loss:0.011505670547485351\n",
      "Train epoch:2, batch index:191, loss:0.011501563787460327\n",
      "Train epoch:2, batch index:192, loss:0.011508910655975342\n",
      "Train epoch:2, batch index:193, loss:0.011506491899490356\n",
      "Train epoch:2, batch index:194, loss:0.01150872826576233\n",
      "Train epoch:2, batch index:195, loss:0.011503901481628418\n",
      "Train epoch:2, batch index:196, loss:0.011507576704025269\n",
      "Train epoch:2, batch index:197, loss:0.011505461931228637\n",
      "Train epoch:2, batch index:198, loss:0.01150745153427124\n",
      "Train epoch:2, batch index:199, loss:0.011505441665649414\n",
      "Train epoch:2, batch index:200, loss:0.011504148244857787\n",
      "Train epoch:2, batch index:201, loss:0.011504241228103639\n",
      "Train epoch:2, batch index:202, loss:0.011503398418426514\n",
      "Train epoch:2, batch index:203, loss:0.011506341695785523\n",
      "Train epoch:2, batch index:204, loss:0.011506247520446777\n",
      "Train epoch:2, batch index:205, loss:0.011503779888153076\n",
      "Train epoch:2, batch index:206, loss:0.011504870653152467\n",
      "Train epoch:2, batch index:207, loss:0.011507718563079835\n",
      "Train epoch:2, batch index:208, loss:0.011506433486938477\n",
      "Train epoch:2, batch index:209, loss:0.011505157947540283\n",
      "Train epoch:2, batch index:210, loss:0.011506941318511963\n",
      "Train epoch:2, batch index:211, loss:0.01150617480278015\n",
      "Train epoch:2, batch index:212, loss:0.011508753299713135\n",
      "Train epoch:2, batch index:213, loss:0.011506372690200805\n",
      "Train epoch:2, batch index:214, loss:0.011503208875656128\n",
      "Train epoch:2, batch index:215, loss:0.011507433652877808\n",
      "Train epoch:2, batch index:216, loss:0.011509406566619872\n",
      "Train epoch:2, batch index:217, loss:0.011505129337310792\n",
      "Train epoch:2, batch index:218, loss:0.011505486965179444\n",
      "Train epoch:2, batch index:219, loss:0.011506139039993287\n",
      "Train epoch:2, batch index:220, loss:0.011506519317626952\n",
      "Train epoch:2, batch index:221, loss:0.011506052017211913\n",
      "Train epoch:2, batch index:222, loss:0.01150560736656189\n",
      "Train epoch:2, batch index:223, loss:0.011505273580551147\n",
      "Train epoch:2, batch index:224, loss:0.011502904891967774\n",
      "Train epoch:2, batch index:225, loss:0.011506565809249879\n",
      "Train epoch:2, batch index:226, loss:0.011504724025726318\n",
      "Train epoch:2, batch index:227, loss:0.01150883436203003\n",
      "Train epoch:2, batch index:228, loss:0.011507419347763061\n",
      "Train epoch:2, batch index:229, loss:0.011506991386413574\n",
      "Train epoch:2, batch index:230, loss:0.011505919694900512\n",
      "Train epoch:2, batch index:231, loss:0.011505146026611329\n",
      "Train epoch:2, batch index:232, loss:0.011507036685943604\n",
      "Train epoch:2, batch index:233, loss:0.011506890058517455\n",
      "Train epoch:2, batch index:234, loss:0.011503186225891113\n",
      "Train epoch:2, batch index:236, loss:0.011506640911102295\n",
      "Train epoch:2, batch index:237, loss:0.011504812240600586\n",
      "Train epoch:2, batch index:238, loss:0.0115061616897583\n",
      "Train epoch:2, batch index:239, loss:0.011508814096450805\n",
      "Train epoch:2, batch index:240, loss:0.011508139371871949\n",
      "Train epoch:2, batch index:241, loss:0.011506688594818116\n",
      "Train epoch:2, batch index:242, loss:0.011506514549255371\n",
      "Train epoch:2, batch index:243, loss:0.011503945589065551\n",
      "Train epoch:2, batch index:244, loss:0.01150659441947937\n",
      "Train epoch:2, batch index:245, loss:0.011504794359207154\n",
      "Train epoch:2, batch index:246, loss:0.011504366397857665\n",
      "Train epoch:2, batch index:247, loss:0.011506208181381226\n",
      "Train epoch:2, batch index:248, loss:0.011504276990890502\n",
      "Train epoch:2, batch index:249, loss:0.011504932641983032\n",
      "Train epoch:2, batch index:250, loss:0.011504043340682984\n",
      "Train epoch:2, batch index:251, loss:0.011505601406097412\n",
      "Train epoch:2, batch index:252, loss:0.011504656076431275\n",
      "Train epoch:2, batch index:253, loss:0.011507809162139893\n",
      "Train epoch:2, batch index:254, loss:0.011506460905075074\n",
      "Train epoch:2, batch index:255, loss:0.011506991386413574\n",
      "Train epoch:2, batch index:256, loss:0.011504650115966797\n",
      "Train epoch:2, batch index:257, loss:0.011506152153015137\n",
      "Train epoch:2, batch index:258, loss:0.011509573459625244\n",
      "Train epoch:2, batch index:259, loss:0.01150435209274292\n",
      "Train epoch:2, batch index:260, loss:0.01150701642036438\n",
      "Train epoch:2, batch index:261, loss:0.011504554748535156\n",
      "Train epoch:2, batch index:262, loss:0.01150559663772583\n",
      "Train epoch:2, batch index:263, loss:0.011505761146545411\n",
      "Train epoch:2, batch index:264, loss:0.011505595445632934\n",
      "Train epoch:2, batch index:265, loss:0.011503386497497558\n",
      "Train epoch:2, batch index:266, loss:0.011505751609802247\n",
      "Train epoch:2, batch index:267, loss:0.01150660276412964\n",
      "Train epoch:2, batch index:268, loss:0.011504188776016236\n",
      "Train epoch:2, batch index:269, loss:0.011508003473281861\n",
      "Train epoch:2, batch index:270, loss:0.011505485773086547\n",
      "Train epoch:2, batch index:271, loss:0.011508479118347167\n",
      "Train epoch:2, batch index:272, loss:0.0115031898021698\n",
      "Train epoch:2, batch index:273, loss:0.011506630182266235\n",
      "Train epoch:2, batch index:274, loss:0.011504383087158203\n",
      "Train epoch:2, batch index:275, loss:0.01150473952293396\n",
      "Train epoch:2, batch index:276, loss:0.011503982543945312\n",
      "Train epoch:2, batch index:277, loss:0.011505813598632812\n",
      "Train epoch:2, batch index:278, loss:0.01150538682937622\n",
      "Train epoch:2, batch index:279, loss:0.011504305601119995\n",
      "Train epoch:2, batch index:280, loss:0.011504616737365723\n",
      "Train epoch:2, batch index:281, loss:0.011503592729568482\n",
      "Train epoch:2, batch index:282, loss:0.011504045724868774\n",
      "Train epoch:2, batch index:283, loss:0.01150351881980896\n",
      "Train epoch:2, batch index:284, loss:0.011505818367004395\n",
      "Train epoch:2, batch index:285, loss:0.011506960391998292\n",
      "Train epoch:2, batch index:286, loss:0.011505643129348755\n",
      "Train epoch:2, batch index:287, loss:0.011507227420806884\n",
      "Train epoch:2, batch index:288, loss:0.011504896879196168\n",
      "Train epoch:2, batch index:289, loss:0.011505212783813477\n",
      "Train epoch:2, batch index:290, loss:0.011505273580551147\n",
      "Train epoch:2, batch index:291, loss:0.01150565266609192\n",
      "Train epoch:2, batch index:292, loss:0.011505224704742432\n",
      "Train epoch:2, batch index:293, loss:0.011505661010742187\n",
      "Train epoch:2, batch index:294, loss:0.011505324840545655\n",
      "Train epoch:2, batch index:295, loss:0.01150485396385193\n",
      "Train epoch:2, batch index:296, loss:0.011503961086273193\n",
      "Train epoch:2, batch index:297, loss:0.01150644302368164\n",
      "Train epoch:2, batch index:298, loss:0.011505424976348877\n",
      "Train epoch:2, batch index:299, loss:0.011505111455917358\n",
      "Train epoch:2, batch index:300, loss:0.01150577425956726\n",
      "Train epoch:2, batch index:301, loss:0.011508429050445556\n",
      "Train epoch:2, batch index:302, loss:0.011509644985198974\n",
      "Train epoch:2, batch index:303, loss:0.01150671362876892\n",
      "Train epoch:2, batch index:304, loss:0.01150593400001526\n",
      "Train epoch:2, batch index:305, loss:0.011504510641098023\n",
      "Train epoch:2, batch index:306, loss:0.011505283117294311\n",
      "Train epoch:2, batch index:307, loss:0.01150639533996582\n",
      "Train epoch:2, batch index:308, loss:0.011501832008361816\n",
      "Train epoch:2, batch index:309, loss:0.011506015062332153\n",
      "Train epoch:2, batch index:310, loss:0.011506600379943848\n",
      "Train epoch:2, batch index:311, loss:0.011505975723266601\n",
      "Train epoch:2, batch index:312, loss:0.01150536298751831\n",
      "Train epoch:2, batch index:313, loss:0.011503139734268189\n",
      "Train epoch:2, batch index:314, loss:0.011506543159484864\n",
      "Train epoch:2, batch index:315, loss:0.011503899097442627\n",
      "Train epoch:2, batch index:316, loss:0.011505470275878907\n",
      "Train epoch:2, batch index:317, loss:0.011506832838058471\n",
      "Train epoch:2, batch index:318, loss:0.011507627964019775\n",
      "Train epoch:2, batch index:319, loss:0.011504378318786621\n",
      "Train epoch:2, batch index:320, loss:0.01150445818901062\n",
      "Train epoch:2, batch index:321, loss:0.011505659818649292\n",
      "Train epoch:2, batch index:322, loss:0.011505715847015381\n",
      "Train epoch:2, batch index:323, loss:0.011508491039276123\n",
      "Train epoch:2, batch index:324, loss:0.01150725245475769\n",
      "Train epoch:2, batch index:325, loss:0.011504342555999756\n",
      "Train epoch:2, batch index:326, loss:0.011504669189453125\n",
      "Train epoch:2, batch index:327, loss:0.011506938934326172\n",
      "Train epoch:2, batch index:328, loss:0.01150501251220703\n",
      "Train epoch:2, batch index:329, loss:0.011506139039993287\n",
      "Train epoch:2, batch index:330, loss:0.011504595279693603\n",
      "Train epoch:2, batch index:331, loss:0.011506766080856323\n",
      "Train epoch:2, batch index:332, loss:0.011505857706069947\n",
      "Train epoch:2, batch index:333, loss:0.011505533456802369\n",
      "Train epoch:2, batch index:334, loss:0.011505364179611207\n",
      "Train epoch:2, batch index:335, loss:0.011503778696060181\n",
      "Train epoch:2, batch index:336, loss:0.011507514715194702\n",
      "Train epoch:2, batch index:337, loss:0.011508749723434448\n",
      "Train epoch:2, batch index:338, loss:0.011505067348480225\n",
      "Train epoch:2, batch index:339, loss:0.01150539517402649\n",
      "Train epoch:2, batch index:340, loss:0.011503371000289917\n",
      "Train epoch:2, batch index:341, loss:0.011506240367889404\n",
      "Train epoch:2, batch index:342, loss:0.01150525689125061\n",
      "Train epoch:2, batch index:343, loss:0.01150525450706482\n",
      "Train epoch:2, batch index:344, loss:0.01150542140007019\n",
      "Train epoch:2, batch index:345, loss:0.011508984565734863\n",
      "Train epoch:2, batch index:346, loss:0.011505061388015747\n",
      "Train epoch:2, batch index:347, loss:0.011506849527359008\n",
      "Train epoch:2, batch index:348, loss:0.011506118774414063\n",
      "Train epoch:2, batch index:349, loss:0.011506428718566894\n",
      "Train epoch:2, batch index:350, loss:0.011506147384643554\n",
      "Train epoch:2, batch index:351, loss:0.011505664587020873\n",
      "Train epoch:2, batch index:352, loss:0.011506756544113159\n",
      "Train epoch:2, batch index:353, loss:0.011508820056915283\n",
      "Train epoch:2, batch index:354, loss:0.011505115032196044\n"
     ]
    }
   ],
   "source": [
    "net = VGG7()\n",
    "net.to(device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-3, weight_decay=0)\n",
    "\n",
    "logs_interval = 200\n",
    "epochs = 10\n",
    "\n",
    "train_losses = []\n",
    "iteration = 0\n",
    "\n",
    "net.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration = iteration + 1\n",
    "        if iteration % logs_interval:\n",
    "            print('Train epoch:{}, batch index:{}, loss:{}'.format(epoch, batch_idx, loss.item()/logs_interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "loss = 0\n",
    "correct = 0\n",
    "\n",
    "confusion_matrix = np.zeros((10, 10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = net(data)\n",
    "        loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        confusion_matrix[pred.cpu()[:, 0], target.cpu()] += 1\n",
    "\n",
    "    loss = loss/len(test_loader.dataset)\n",
    "    accuracy = 100.*correct/len(test_loader.dataset)\n",
    "    print('Train set average loss: {}, accuracy: {}%'.format(loss, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da9c6a45712c6b513c4a80738271fc65c25531d6aa017375b96d9ba6472bbc68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
