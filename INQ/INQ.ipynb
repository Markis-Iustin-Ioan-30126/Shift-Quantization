{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand digit classifier \n",
    "---\n",
    "## Incremental network quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from skimage import io"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root='../', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root=\"../\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_set = [train_data[i] for i in range(50000)]\n",
    "validation_set = [train_data[i] for i in range(50000, 60000)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=64, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definig a VGG-7 inspired architecture model\n",
    "---\n",
    "Featuring 4 convolutional and 3 fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG7, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=\"same\", stride=1, bias=False)\n",
    "        \n",
    "        self.fc1 = nn.Linear(7*7*128, 512, bias=False)\n",
    "        self.fc2 = nn.Linear(512, 256, bias=False)\n",
    "        self.fc3 = nn.Linear(256, 10, bias=False)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, stride=2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, stride=2)  \n",
    "\n",
    "        x = x.view(-1, 7*7*128)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)  \n",
    "\n",
    "        x = F.log_softmax(x, dim=1)  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(net, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    confusion_matrix = np.zeros((10, 10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = net(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
    "            confusion_matrix[pred.view(-1), target] += 1\n",
    "\n",
    "    print(\"[OK] Model evaluation complete [OK]\")\n",
    "    print(\"Average loss: {:.5f}\".format(loss/len(test_loader.dataset)))\n",
    "    print(\"Accuracy: {:.2f}%\".format(100.*(correct/len(test_loader.dataset))))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(confusion_matrix/len(train_loader.dataset))\n",
    "    ax.set_xticks(np.arange(10))\n",
    "    ax.set_yticks(np.arange(10))\n",
    "    ax.set_xlabel(\"Truth\")\n",
    "    ax.set_ylabel(\"Predictions\")\n",
    "    ax.set_title(\"Confusion matrix\")\n",
    "    fig.set_size_inches(4, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Quantization operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBounderyExponents(W, b):\n",
    "    s = torch.max(torch.abs(W)).item()\n",
    "    n1 = np.floor(np.log2(4*(s/3)))\n",
    "    n2 = n1 + 1 - (2**(b - 1))/2\n",
    "    return n1, n2\n",
    "\n",
    "\n",
    "def getQuantizationMask(W, percentage, T):\n",
    "    w = W.view(-1)\n",
    "    t = T.view(-1)\n",
    "    idx = t == 1\n",
    "\n",
    "    numberOfWeights = w.size(dim=0)\n",
    "    numberOfQWeights = int(percentage*numberOfWeights - t[idx].size(dim=0))\n",
    "\n",
    "    t_aux = torch.Tensor(np.ones_like(T)).view(-1)\n",
    "    w = w*(t_aux - t)\n",
    "    w = torch.abs(w)\n",
    "    sorted_w, indices_w = w.sort()\n",
    "    t[indices_w[-numberOfQWeights:]] = 1\n",
    "    \n",
    "    return t.view(T.size())\n",
    "\n",
    "\n",
    "def getFcQuantizationMask(In, percentage, T):\n",
    "    Ax = torch.Tensor(np.zeros_like(In.data))\n",
    "    eps = 1e-45\n",
    "\n",
    "    for i in range(len(In)):\n",
    "      Ax[i] = torch.where(T[i] == 0, In[i], 0)\n",
    "      if T[i] == 0 and In[i] == 0:\n",
    "        Ax[i] = eps\n",
    "\n",
    "    w_abs=torch.abs(Ax)\n",
    "    values_sort,indexes_sort=torch.sort(w_abs,descending=True)\n",
    "    count = torch.sum(T == 1)\n",
    "    k=np.floor(len(In)*percentage)-count\n",
    "    k=int(k)\n",
    "\n",
    "    if k < 0:\n",
    "      k = 0\n",
    "\n",
    "    sorted_indexes=indexes_sort[:int(k)]\n",
    "\n",
    "    for i in range(len(sorted_indexes)):\n",
    "        T[sorted_indexes[i]] = 1\n",
    "\n",
    "\n",
    "def quantizeWeights(W, T, n1, n2):\n",
    "    T_aux = torch.Tensor(np.ones_like(T))\n",
    "    eps = 1e-6\n",
    "    W1 = W*(T_aux - T)\n",
    "    idx = W == 0\n",
    "    W.data[idx] = eps\n",
    "\n",
    "    closestExp = torch.floor(torch.log2(torch.abs(W*4/3)))\n",
    "    Q = W1 + torch.sign(W)*(2**closestExp)*T\n",
    "\n",
    "    idx = closestExp*T < n2\n",
    "    Q[idx] = 0\n",
    "    idx = ((closestExp > n1)*T).bool()\n",
    "    Q[idx] = 2**n1\n",
    "\n",
    "    return closestExp, Q\n",
    "\n",
    "\n",
    "def quantize_conv_layer(W, T, percentage, number_of_bits):\n",
    "    n = T.size(dim=0)\n",
    "    m = T.size(dim=1)\n",
    "\n",
    "    n1, n2 = getBounderyExponents(W, number_of_bits)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            T[i, j, :, :] = getQuantizationMask(W[i, j, :, :], percentage, T[i, j, :, :])\n",
    "            _, W.data[i, j, :, :] = quantizeWeights(W[i, j, :, :], T[i, j, :, :], n1, n2)\n",
    "\n",
    "\n",
    "def quantize_fc_layer(W, T, percentage, number_of_bits):\n",
    "    n1, n2 = getBounderyExponents(W, number_of_bits)\n",
    "    n = W.size(dim=1)\n",
    "\n",
    "    for i in range(n):\n",
    "        getFcQuantizationMask(W[:, i], percentage, T[:, i])\n",
    "        _, W.data[:, i] = quantizeWeights(W[:, i], T[:, i], n1, n2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device initialization for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(\"../Baseline/baseline.pth\")\n",
    "net.to(device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-2, weight_decay=0)\n",
    "\n",
    "quantization_percentages = [0.5, 0.75, 0.875]\n",
    "quantization_precision = 8\n",
    "\n",
    "epochs = 4\n",
    "logs_interval = 100\n",
    "iteration = 0\n",
    "train_loss = []\n",
    "\n",
    "Tconv1 = torch.zeros_like(net.conv1.weight)\n",
    "Tconv2 = torch.zeros_like(net.conv2.weight)\n",
    "Tconv3 = torch.zeros_like(net.conv3.weight)\n",
    "Tconv4 = torch.zeros_like(net.conv4.weight)\n",
    "Tfc1 = torch.zeros_like(net.fc1.weight)\n",
    "Tfc2 = torch.zeros_like(net.fc2.weight)\n",
    "Tfc3 = torch.zeros_like(net.fc3.weight)\n",
    "\n",
    "net.train()\n",
    "\n",
    "for q_stage, percentage in enumerate(quantization_percentages):\n",
    "    # quantize layers\n",
    "    quantize_conv_layer(net.conv1.weight, Tconv1, percentage, quantization_precision)\n",
    "    quantize_conv_layer(net.conv2.weight, Tconv2, percentage, quantization_precision)\n",
    "    quantize_conv_layer(net.conv3.weight, Tconv3, percentage, quantization_precision)\n",
    "    quantize_conv_layer(net.conv4.weight, Tconv4, percentage, quantization_precision)\n",
    "    quantize_fc_layer(net.fc1.weight, Tfc1, percentage, quantization_precision)\n",
    "    quantize_fc_layer(net.fc2.weight, Tfc2, percentage, quantization_precision)\n",
    "    quantize_fc_layer(net.fc3.weight, Tfc3, percentage, quantization_precision)\n",
    "    \n",
    "    # correct remaining weights by training\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # setam gradienti la 0\n",
    "            net.conv1.weight.grad = net.conv1.weight.grad*(torch.ones_like(Tconv1) - Tconv1)\n",
    "            net.conv2.weight.grad = net.conv2.weight.grad*(torch.ones_like(Tconv2) - Tconv2)\n",
    "            net.conv3.weight.grad = net.conv3.weight.grad*(torch.ones_like(Tconv3) - Tconv3)\n",
    "            net.conv4.weight.grad = net.conv4.weight.grad*(torch.ones_like(Tconv4) - Tconv4)\n",
    "            net.fc1.weight.grad = net.fc1.weight.grad*(torch.ones_like(Tfc1) - Tfc1)\n",
    "            net.fc2.weight.grad = net.fc2.weight.grad*(torch.ones_like(Tfc2) - Tfc2)\n",
    "            net.fc3.weight.grad = net.fc3.weight.grad*(torch.ones_like(Tfc3) - Tfc3)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            iteration = iteration + 1\n",
    "            if iteration % logs_interval == 0:\n",
    "                print('Quantization step: {}/{}, Train epoch:{}/{}, batch index:{}, loss:{}'.format(\n",
    "                    q_stage + 1, len(quantization_percentages),\n",
    "                    epoch + 1, epochs, batch_idx + 1, loss.item()/logs_interval))\n",
    "                train_loss.append(loss.item())    \n",
    "\n",
    "quantize_conv_layer(net.conv1.weight, Tconv1, 1, quantization_precision)\n",
    "quantize_conv_layer(net.conv2.weight, Tconv2, 1, quantization_precision)\n",
    "quantize_conv_layer(net.conv3.weight, Tconv3, 1, quantization_precision)\n",
    "quantize_conv_layer(net.conv4.weight, Tconv4, 1, quantization_precision) \n",
    "quantize_fc_layer(net.fc1.weight, Tfc1, 1, quantization_precision)\n",
    "quantize_fc_layer(net.fc2.weight, Tfc2, 1, quantization_precision)\n",
    "quantize_fc_layer(net.fc3.weight, Tfc3, 1, quantization_precision)\n",
    "\n",
    "#torch.save(net, \"INQ.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(np.array([\n",
    "    [0.01, 0.02, -0.2, 0.04, 0.33],\n",
    "    [0.17, -0.42, -0.33, 0.02, -0.05], \n",
    "    [0.02, 0.83, -0.03, 0.03, 0.06],\n",
    "    [-0.9, 0.07, 0.11, 0.87, -0.36], \n",
    "    [-0.73, 0.41, 0.42, 0.39, 0.47]]))\n",
    "bit_length = 4\n",
    "n1, n2 = getBounderyExponents(W, bit_length)\n",
    "print(n1, n2)\n",
    "T = torch.Tensor(np.zeros_like(W))\n",
    "T = getQuantizationMask(W, 0.5, T)\n",
    "print(T)\n",
    "_, W = quantizeWeights(W, T, n1, n2)\n",
    "\n",
    "W = torch.Tensor(np.array([\n",
    "    [0.11, 0.04, -0.7, 0.19, -0.25],\n",
    "    [0.15, -0.5, -0.25, -0.09, -0.02],\n",
    "    [-0.02, 1, -0.06, 0.21, 0.15],\n",
    "    [-1, 0.27, -0.09, 1, -0.25],\n",
    "    [-0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "]))\n",
    "\n",
    "T = getQuantizationMask(W, 0.75, T)\n",
    "_, W = quantizeWeights(W, T, n1, n2)\n",
    "print(T)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VGG7()\n",
    "\n",
    "number_of_bits = 32\n",
    "mb_conv = 1.25e-7\n",
    "number_of_quantized_bits = 5\n",
    "number_of_weights = 0\n",
    "model_size = 0\n",
    "\n",
    "for param in net.parameters():\n",
    "    nr_weight_layer = 1\n",
    "    for n in param.size():\n",
    "        nr_weight_layer *= n\n",
    "    number_of_weights += nr_weight_layer\n",
    "\n",
    "model_size = number_of_weights*number_of_bits\n",
    "\n",
    "print(\"Numer of weights: \"+str(number_of_weights))\n",
    "print(\"Size needed to store the model with float32 precision: {:.3f} MB\".format(model_size*mb_conv))\n",
    "\n",
    "model_size = number_of_weights*number_of_quantized_bits\n",
    "print(\"Size needed to store the model with {} bits quantization: {:.3f} MB\".format(number_of_quantized_bits, model_size*mb_conv))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#time reduction with quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = VGG7()\n",
    "# net.to(device)\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), lr=1, weight_decay=0)\n",
    "\n",
    "# b = torch.clone(net.fc3.weight)\n",
    "# Tfc3 = torch.zeros_like(net.fc3.weight)\n",
    "# print(net.fc3.weight[:10, 0])\n",
    "# quantize_fc_layer(net.fc3.weight, Tfc3, 0.5, 5)\n",
    "# print(Tfc3[:10, 0])\n",
    "# print(net.fc3.weight[:10, 0])\n",
    "\n",
    "# net.train()\n",
    "\n",
    "# data, target = train_data[2]\n",
    "# target = torch.Tensor([target]).long()\n",
    "# optimizer.zero_grad()\n",
    "# output = net(data)\n",
    "# loss = F.nll_loss(output, target)\n",
    "# loss.backward()\n",
    "# net.fc3.weight.grad = net.fc3.weight.grad*(torch.ones_like(Tfc3) - Tfc3)\n",
    "# optimizer.step()\n",
    "\n",
    "# print(Tfc3[:10, 0])\n",
    "# print(net.fc3.weight[:10, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da9c6a45712c6b513c4a80738271fc65c25531d6aa017375b96d9ba6472bbc68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
