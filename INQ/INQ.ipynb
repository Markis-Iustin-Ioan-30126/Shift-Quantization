{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand digit classifier \n",
    "---\n",
    "## Incremental network quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from skimage import io"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root='../', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root=\"../\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_set = [train_data[i] for i in range(50000)]\n",
    "validation_set = [train_data[i] for i in range(50000, 60000)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=64, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definig a VGG-7 inspired architecture model\n",
    "---\n",
    "Featuring 4 convolutional and 3 fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG7, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=\"same\", stride=1, bias=False)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=\"same\", stride=1, bias=False)\n",
    "        \n",
    "        self.fc1 = nn.Linear(7*7*128, 512, bias=False)\n",
    "        self.fc2 = nn.Linear(512, 256, bias=False)\n",
    "        self.fc3 = nn.Linear(256, 10, bias=False)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, stride=2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, stride=2)  \n",
    "\n",
    "        x = x.view(-1, 7*7*128)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)  \n",
    "\n",
    "        x = F.log_softmax(x, dim=1)  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(net, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    confusion_matrix = np.zeros((10, 10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = net(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
    "            confusion_matrix[pred.view(-1), target] += 1\n",
    "\n",
    "    print(\"[OK] Model evaluation complete [OK]\")\n",
    "    print(\"Average loss: {:.5f}\".format(loss/len(test_loader.dataset)))\n",
    "    print(\"Accuracy: {:.2f}%\".format(100.*(correct/len(test_loader.dataset))))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(confusion_matrix/len(train_loader.dataset))\n",
    "    ax.set_xticks(np.arange(10))\n",
    "    ax.set_yticks(np.arange(10))\n",
    "    ax.set_xlabel(\"Truth\")\n",
    "    ax.set_ylabel(\"Predictions\")\n",
    "    ax.set_title(\"Confusion matrix\")\n",
    "    fig.set_size_inches(4, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Quantization operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBounderyExponents(W, b):\n",
    "    s = torch.max(torch.abs(W)).item()\n",
    "    n1 = np.floor(np.log2(4*(s/3)))\n",
    "    n2 = n1 + 1 - (2**(b - 1))/2\n",
    "    return n1, n2\n",
    "\n",
    "\n",
    "def getQuantizationMask(W, percentage, T):\n",
    "    w = W.view(-1)\n",
    "    t = T.view(-1)\n",
    "    idx = t == 1\n",
    "\n",
    "    numberOfWeights = w.size(dim=0)\n",
    "    numberOfQWeights = int(percentage*numberOfWeights - t[idx].size(dim=0))\n",
    "\n",
    "    t_aux = torch.Tensor(np.ones_like(T)).view(-1)\n",
    "    w = w*(t_aux - t)\n",
    "    w = torch.abs(w)\n",
    "    sorted_w, indices_w = w.sort()\n",
    "    t[indices_w[-numberOfQWeights:]] = 1\n",
    "    \n",
    "    return t.view(T.size())\n",
    "\n",
    "\n",
    "def getFcQuantizationMask(In, percentage, T):\n",
    "    Ax = torch.Tensor(np.zeros_like(In.data))\n",
    "    eps = 1e-45\n",
    "\n",
    "    for i in range(len(In)):\n",
    "      Ax[i] = torch.where(T[i] == 0, In[i], 0)\n",
    "      if T[i] == 0 and In[i] == 0:\n",
    "        Ax[i] = eps\n",
    "\n",
    "    w_abs=torch.abs(Ax)\n",
    "    values_sort,indexes_sort=torch.sort(w_abs,descending=True)\n",
    "    count = torch.sum(T == 1)\n",
    "    k=np.floor(len(In)*percentage)-count\n",
    "    k=int(k)\n",
    "\n",
    "    if k < 0:\n",
    "      k = 0\n",
    "\n",
    "    sorted_indexes=indexes_sort[:int(k)]\n",
    "\n",
    "    for i in range(len(sorted_indexes)):\n",
    "        T[sorted_indexes[i]] = 1\n",
    "\n",
    "\n",
    "def quantizeWeights(W, T, n1, n2):\n",
    "    T_aux = torch.Tensor(np.ones_like(T))\n",
    "    eps = 1e-6\n",
    "    W1 = W*(T_aux - T)\n",
    "    idx = W == 0\n",
    "    W.data[idx] = eps\n",
    "\n",
    "    closestExp = torch.floor(torch.log2(torch.abs(W*4/3)))\n",
    "    Q = W1 + torch.sign(W)*(2**closestExp)*T\n",
    "\n",
    "    idx = closestExp*T < n2\n",
    "    Q[idx] = 0\n",
    "    idx = ((closestExp > n1)*T).bool()\n",
    "    Q[idx] = 2**n1\n",
    "\n",
    "    return closestExp, Q\n",
    "\n",
    "\n",
    "def quantize_conv_layer(W, T, percentage, number_of_bits):\n",
    "    n = T.size(dim=0)\n",
    "    m = T.size(dim=1)\n",
    "\n",
    "    n1, n2 = getBounderyExponents(W, number_of_bits)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            T[i, j, :, :] = getQuantizationMask(W[i, j, :, :], percentage, T[i, j, :, :])\n",
    "            _, W.data[i, j, :, :] = quantizeWeights(W[i, j, :, :], T[i, j, :, :], n1, n2)\n",
    "\n",
    "\n",
    "def quantize_fc_layer(W, T, percentage, number_of_bits):\n",
    "    n1, n2 = getBounderyExponents(W, number_of_bits)\n",
    "    n = W.size(dim=1)\n",
    "\n",
    "    for i in range(n):\n",
    "        getFcQuantizationMask(W[:, i], percentage, T[:, i])\n",
    "        _, W.data[:, i] = quantizeWeights(W[:, i], T[:, i], n1, n2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device initialization for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization step: 1/3, Train epoch:1/4, batch index:100, loss:0.0005677900835871696\n",
      "Quantization step: 1/3, Train epoch:1/4, batch index:200, loss:0.00037589292973279955\n",
      "Quantization step: 1/3, Train epoch:1/4, batch index:300, loss:0.00025655049830675125\n",
      "Quantization step: 1/3, Train epoch:1/4, batch index:400, loss:1.6814402770251035e-05\n",
      "Quantization step: 1/3, Train epoch:1/4, batch index:500, loss:0.00036054309457540513\n",
      "Quantization step: 1/3, Train epoch:1/4, batch index:600, loss:0.0004614729434251785\n",
      "Quantization step: 1/3, Train epoch:1/4, batch index:700, loss:0.0010496847331523894\n",
      "Quantization step: 1/3, Train epoch:2/4, batch index:18, loss:0.00027374252676963805\n",
      "Quantization step: 1/3, Train epoch:2/4, batch index:118, loss:0.00011787299998104572\n",
      "Quantization step: 1/3, Train epoch:2/4, batch index:218, loss:0.00010540169663727284\n",
      "Quantization step: 1/3, Train epoch:2/4, batch index:318, loss:0.0002377958782017231\n",
      "Quantization step: 1/3, Train epoch:2/4, batch index:418, loss:0.0008500547707080841\n",
      "Quantization step: 1/3, Train epoch:2/4, batch index:518, loss:9.311460889875889e-05\n",
      "Quantization step: 1/3, Train epoch:2/4, batch index:618, loss:0.0003534173220396042\n",
      "Quantization step: 1/3, Train epoch:2/4, batch index:718, loss:0.00030104735866189004\n",
      "Quantization step: 1/3, Train epoch:3/4, batch index:36, loss:0.00026216214522719385\n",
      "Quantization step: 1/3, Train epoch:3/4, batch index:136, loss:0.00010031956247985363\n",
      "Quantization step: 1/3, Train epoch:3/4, batch index:236, loss:0.0003367409482598305\n",
      "Quantization step: 1/3, Train epoch:3/4, batch index:336, loss:0.00022274456918239594\n",
      "Quantization step: 1/3, Train epoch:3/4, batch index:436, loss:0.00034042075276374817\n",
      "Quantization step: 1/3, Train epoch:3/4, batch index:536, loss:0.000481971874833107\n",
      "Quantization step: 1/3, Train epoch:3/4, batch index:636, loss:8.840742520987988e-05\n",
      "Quantization step: 1/3, Train epoch:3/4, batch index:736, loss:0.00011415828950703144\n",
      "Quantization step: 1/3, Train epoch:4/4, batch index:54, loss:0.00019251761958003044\n",
      "Quantization step: 1/3, Train epoch:4/4, batch index:154, loss:0.00011735538020730019\n",
      "Quantization step: 1/3, Train epoch:4/4, batch index:254, loss:0.00010251596570014953\n",
      "Quantization step: 1/3, Train epoch:4/4, batch index:354, loss:9.048693813383579e-05\n",
      "Quantization step: 1/3, Train epoch:4/4, batch index:454, loss:0.000382404625415802\n",
      "Quantization step: 1/3, Train epoch:4/4, batch index:554, loss:0.0008941281586885453\n",
      "Quantization step: 1/3, Train epoch:4/4, batch index:654, loss:0.0006339846551418304\n",
      "Quantization step: 1/3, Train epoch:4/4, batch index:754, loss:0.0008336484432220459\n",
      "Quantization step: 2/3, Train epoch:1/4, batch index:72, loss:0.00013903504237532615\n",
      "Quantization step: 2/3, Train epoch:1/4, batch index:172, loss:0.0001326165720820427\n",
      "Quantization step: 2/3, Train epoch:1/4, batch index:272, loss:0.00017653685063123703\n",
      "Quantization step: 2/3, Train epoch:1/4, batch index:372, loss:0.0002510017156600952\n",
      "Quantization step: 2/3, Train epoch:1/4, batch index:472, loss:0.000633586123585701\n",
      "Quantization step: 2/3, Train epoch:1/4, batch index:572, loss:5.701694637537003e-05\n",
      "Quantization step: 2/3, Train epoch:1/4, batch index:672, loss:5.91083662584424e-05\n",
      "Quantization step: 2/3, Train epoch:1/4, batch index:772, loss:0.00014086387120187282\n",
      "Quantization step: 2/3, Train epoch:2/4, batch index:90, loss:3.8145303260535e-05\n",
      "Quantization step: 2/3, Train epoch:2/4, batch index:190, loss:0.00011144954711198806\n",
      "Quantization step: 2/3, Train epoch:2/4, batch index:290, loss:2.9312972910702227e-05\n",
      "Quantization step: 2/3, Train epoch:2/4, batch index:390, loss:0.0006925979256629944\n",
      "Quantization step: 2/3, Train epoch:2/4, batch index:490, loss:0.001294451504945755\n",
      "Quantization step: 2/3, Train epoch:2/4, batch index:590, loss:0.0010465918481349945\n",
      "Quantization step: 2/3, Train epoch:2/4, batch index:690, loss:2.048044465482235e-05\n",
      "Quantization step: 2/3, Train epoch:3/4, batch index:8, loss:0.0014526623487472535\n",
      "Quantization step: 2/3, Train epoch:3/4, batch index:108, loss:0.00023105239495635032\n",
      "Quantization step: 2/3, Train epoch:3/4, batch index:208, loss:0.00016991691663861274\n",
      "Quantization step: 2/3, Train epoch:3/4, batch index:308, loss:0.00024216430261731148\n",
      "Quantization step: 2/3, Train epoch:3/4, batch index:408, loss:0.00024343637749552727\n",
      "Quantization step: 2/3, Train epoch:3/4, batch index:508, loss:0.00012056866660714149\n",
      "Quantization step: 2/3, Train epoch:3/4, batch index:608, loss:0.0006719518452882767\n",
      "Quantization step: 2/3, Train epoch:3/4, batch index:708, loss:0.0001758090779185295\n",
      "Quantization step: 2/3, Train epoch:4/4, batch index:26, loss:3.8134818896651265e-05\n",
      "Quantization step: 2/3, Train epoch:4/4, batch index:126, loss:0.00011099175550043583\n",
      "Quantization step: 2/3, Train epoch:4/4, batch index:226, loss:0.00043105706572532654\n",
      "Quantization step: 2/3, Train epoch:4/4, batch index:326, loss:6.295236293226481e-05\n",
      "Quantization step: 2/3, Train epoch:4/4, batch index:426, loss:5.1726452074944976e-05\n",
      "Quantization step: 2/3, Train epoch:4/4, batch index:526, loss:0.00022679418325424194\n",
      "Quantization step: 2/3, Train epoch:4/4, batch index:626, loss:2.0165466703474523e-05\n",
      "Quantization step: 2/3, Train epoch:4/4, batch index:726, loss:0.00017742669209837913\n",
      "Quantization step: 3/3, Train epoch:1/4, batch index:44, loss:6.387788336724043e-05\n",
      "Quantization step: 3/3, Train epoch:1/4, batch index:144, loss:8.981941267848015e-05\n",
      "Quantization step: 3/3, Train epoch:1/4, batch index:244, loss:0.000243629589676857\n",
      "Quantization step: 3/3, Train epoch:1/4, batch index:344, loss:2.3533033672720194e-05\n",
      "Quantization step: 3/3, Train epoch:1/4, batch index:444, loss:0.0002249140106141567\n",
      "Quantization step: 3/3, Train epoch:1/4, batch index:544, loss:6.709791719913482e-05\n",
      "Quantization step: 3/3, Train epoch:1/4, batch index:644, loss:4.833078011870384e-05\n",
      "Quantization step: 3/3, Train epoch:1/4, batch index:744, loss:0.00018299749121069908\n",
      "Quantization step: 3/3, Train epoch:2/4, batch index:62, loss:0.00019660843536257745\n",
      "Quantization step: 3/3, Train epoch:2/4, batch index:162, loss:0.0001438632607460022\n",
      "Quantization step: 3/3, Train epoch:2/4, batch index:262, loss:5.9020412154495714e-05\n",
      "Quantization step: 3/3, Train epoch:2/4, batch index:362, loss:0.0002013273350894451\n",
      "Quantization step: 3/3, Train epoch:2/4, batch index:462, loss:0.00013409565202891825\n",
      "Quantization step: 3/3, Train epoch:2/4, batch index:562, loss:0.00018660139292478562\n",
      "Quantization step: 3/3, Train epoch:2/4, batch index:662, loss:7.26961623877287e-05\n",
      "Quantization step: 3/3, Train epoch:2/4, batch index:762, loss:0.00011308048851788044\n",
      "Quantization step: 3/3, Train epoch:3/4, batch index:80, loss:1.748700626194477e-05\n",
      "Quantization step: 3/3, Train epoch:3/4, batch index:180, loss:0.00019108153879642486\n",
      "Quantization step: 3/3, Train epoch:3/4, batch index:280, loss:3.542184829711914e-05\n",
      "Quantization step: 3/3, Train epoch:3/4, batch index:380, loss:2.5528883561491967e-05\n",
      "Quantization step: 3/3, Train epoch:3/4, batch index:480, loss:0.00013789447955787183\n",
      "Quantization step: 3/3, Train epoch:3/4, batch index:580, loss:4.8584155738353726e-05\n",
      "Quantization step: 3/3, Train epoch:3/4, batch index:680, loss:0.000583907961845398\n",
      "Quantization step: 3/3, Train epoch:3/4, batch index:780, loss:7.784710731357337e-05\n",
      "Quantization step: 3/3, Train epoch:4/4, batch index:98, loss:0.00030966132879257203\n",
      "Quantization step: 3/3, Train epoch:4/4, batch index:198, loss:0.00011104537174105644\n",
      "Quantization step: 3/3, Train epoch:4/4, batch index:298, loss:2.4868387263268233e-05\n",
      "Quantization step: 3/3, Train epoch:4/4, batch index:398, loss:0.00012094855308532715\n",
      "Quantization step: 3/3, Train epoch:4/4, batch index:498, loss:2.5329350028187036e-05\n",
      "Quantization step: 3/3, Train epoch:4/4, batch index:598, loss:9.111120365560055e-05\n",
      "Quantization step: 3/3, Train epoch:4/4, batch index:698, loss:4.084633197635412e-05\n"
     ]
    }
   ],
   "source": [
    "net = torch.load(\"../Baseline/baseline.pth\")\n",
    "net.to(device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-2, weight_decay=0)\n",
    "\n",
    "quantization_percentages = [0.5, 0.75, 0.875]\n",
    "quantization_precision = 7\n",
    "\n",
    "epochs = 4\n",
    "logs_interval = 100\n",
    "iteration = 0\n",
    "train_loss = []\n",
    "\n",
    "Tconv1 = torch.zeros_like(net.conv1.weight)\n",
    "Tconv2 = torch.zeros_like(net.conv2.weight)\n",
    "Tconv3 = torch.zeros_like(net.conv3.weight)\n",
    "Tconv4 = torch.zeros_like(net.conv4.weight)\n",
    "Tfc1 = torch.zeros_like(net.fc1.weight)\n",
    "Tfc2 = torch.zeros_like(net.fc2.weight)\n",
    "Tfc3 = torch.zeros_like(net.fc3.weight)\n",
    "\n",
    "net.train()\n",
    "\n",
    "for q_stage, percentage in enumerate(quantization_percentages):\n",
    "    # quantize layers\n",
    "    quantize_conv_layer(net.conv1.weight, Tconv1, percentage, quantization_precision)\n",
    "    quantize_conv_layer(net.conv2.weight, Tconv2, percentage, quantization_precision)\n",
    "    quantize_conv_layer(net.conv3.weight, Tconv3, percentage, quantization_precision)\n",
    "    quantize_conv_layer(net.conv4.weight, Tconv4, percentage, quantization_precision)\n",
    "    quantize_fc_layer(net.fc1.weight, Tfc1, percentage, quantization_precision)\n",
    "    quantize_fc_layer(net.fc2.weight, Tfc2, percentage, quantization_precision)\n",
    "    quantize_fc_layer(net.fc3.weight, Tfc3, percentage, quantization_precision)\n",
    "    \n",
    "    # correct remaining weights by training\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # setam gradienti la 0\n",
    "            net.conv1.weight.grad = net.conv1.weight.grad*(torch.ones_like(Tconv1) - Tconv1)\n",
    "            net.conv2.weight.grad = net.conv2.weight.grad*(torch.ones_like(Tconv2) - Tconv2)\n",
    "            net.conv3.weight.grad = net.conv3.weight.grad*(torch.ones_like(Tconv3) - Tconv3)\n",
    "            net.conv4.weight.grad = net.conv4.weight.grad*(torch.ones_like(Tconv4) - Tconv4)\n",
    "            net.fc1.weight.grad = net.fc1.weight.grad*(torch.ones_like(Tfc1) - Tfc1)\n",
    "            net.fc2.weight.grad = net.fc2.weight.grad*(torch.ones_like(Tfc2) - Tfc2)\n",
    "            net.fc3.weight.grad = net.fc3.weight.grad*(torch.ones_like(Tfc3) - Tfc3)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            iteration = iteration + 1\n",
    "            if iteration % logs_interval == 0:\n",
    "                print('Quantization step: {}/{}, Train epoch:{}/{}, batch index:{}, loss:{}'.format(\n",
    "                    q_stage + 1, len(quantization_percentages),\n",
    "                    epoch + 1, epochs, batch_idx + 1, loss.item()/logs_interval))\n",
    "                train_loss.append(loss.item())    \n",
    "\n",
    "quantize_conv_layer(net.conv1.weight, Tconv1, 1, quantization_precision)\n",
    "quantize_conv_layer(net.conv2.weight, Tconv2, 1, quantization_precision)\n",
    "quantize_conv_layer(net.conv3.weight, Tconv3, 1, quantization_precision)\n",
    "quantize_conv_layer(net.conv4.weight, Tconv4, 1, quantization_precision) \n",
    "quantize_fc_layer(net.fc1.weight, Tfc1, 1, quantization_precision)\n",
    "quantize_fc_layer(net.fc2.weight, Tfc2, 1, quantization_precision)\n",
    "quantize_fc_layer(net.fc3.weight, Tfc3, 1, quantization_precision)\n",
    "\n",
    "torch.save(net, \"INQ_7bits_lr1e_2_I.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Model evaluation complete [OK]\n",
      "Average loss: 0.04007\n",
      "Accuracy: 98.74%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluateModel(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 -3.0\n",
      "tensor([[0., 0., 0., 0., 1.],\n",
      "        [0., 1., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[ 0.1100,  0.0400, -0.5000,  0.2500, -0.2500],\n",
      "        [ 0.1250, -0.5000, -0.2500, -0.0900, -0.0200],\n",
      "        [-0.0200,  1.0000, -0.0600,  0.2500,  0.1250],\n",
      "        [-1.0000,  0.2500, -0.0900,  1.0000, -0.2500],\n",
      "        [-0.5000,  0.5000,  0.5000,  0.5000,  0.5000]])\n"
     ]
    }
   ],
   "source": [
    "W = torch.tensor(np.array([\n",
    "    [0.01, 0.02, -0.2, 0.04, 0.33],\n",
    "    [0.17, -0.42, -0.33, 0.02, -0.05], \n",
    "    [0.02, 0.83, -0.03, 0.03, 0.06],\n",
    "    [-0.9, 0.07, 0.11, 0.87, -0.36], \n",
    "    [-0.73, 0.41, 0.42, 0.39, 0.47]]))\n",
    "bit_length = 4\n",
    "n1, n2 = getBounderyExponents(W, bit_length)\n",
    "print(n1, n2)\n",
    "T = torch.Tensor(np.zeros_like(W))\n",
    "T = getQuantizationMask(W, 0.5, T)\n",
    "print(T)\n",
    "_, W = quantizeWeights(W, T, n1, n2)\n",
    "\n",
    "W = torch.Tensor(np.array([\n",
    "    [0.11, 0.04, -0.7, 0.19, -0.25],\n",
    "    [0.15, -0.5, -0.25, -0.09, -0.02],\n",
    "    [-0.02, 1, -0.06, 0.21, 0.15],\n",
    "    [-1, 0.27, -0.09, 1, -0.25],\n",
    "    [-0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "]))\n",
    "\n",
    "T = getQuantizationMask(W, 0.75, T)\n",
    "_, W = quantizeWeights(W, T, n1, n2)\n",
    "print(T)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of weights: 3603520\n",
      "Size needed to store the model with float32 precision: 14.414 MB\n",
      "Size needed to store the model with 5 bits quantization: 2.252 MB\n"
     ]
    }
   ],
   "source": [
    "number_of_bits = 32\n",
    "mb_conv = 1.25e-7\n",
    "number_of_quantized_bits = 5\n",
    "number_of_weights = 0\n",
    "model_size = 0\n",
    "\n",
    "for param in net.parameters():\n",
    "    nr_weight_layer = 1\n",
    "    for n in param.size():\n",
    "        nr_weight_layer *= n\n",
    "    number_of_weights += nr_weight_layer\n",
    "\n",
    "model_size = number_of_weights*number_of_bits\n",
    "\n",
    "print(\"Numer of weights: \"+str(number_of_weights))\n",
    "print(\"Size needed to store the model with float32 precision: {:.3f} MB\".format(model_size*mb_conv))\n",
    "\n",
    "model_size = number_of_weights*number_of_quantized_bits\n",
    "print(\"Size needed to store the model with {} bits quantization: {:.3f} MB\".format(number_of_quantized_bits, model_size*mb_conv))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#time reduction with quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = VGG7()\n",
    "# net.to(device)\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), lr=1, weight_decay=0)\n",
    "\n",
    "# b = torch.clone(net.fc3.weight)\n",
    "# Tfc3 = torch.zeros_like(net.fc3.weight)\n",
    "# print(net.fc3.weight[:10, 0])\n",
    "# quantize_fc_layer(net.fc3.weight, Tfc3, 0.5, 5)\n",
    "# print(Tfc3[:10, 0])\n",
    "# print(net.fc3.weight[:10, 0])\n",
    "\n",
    "# net.train()\n",
    "\n",
    "# data, target = train_data[2]\n",
    "# target = torch.Tensor([target]).long()\n",
    "# optimizer.zero_grad()\n",
    "# output = net(data)\n",
    "# loss = F.nll_loss(output, target)\n",
    "# loss.backward()\n",
    "# net.fc3.weight.grad = net.fc3.weight.grad*(torch.ones_like(Tfc3) - Tfc3)\n",
    "# optimizer.step()\n",
    "\n",
    "# print(Tfc3[:10, 0])\n",
    "# print(net.fc3.weight[:10, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da9c6a45712c6b513c4a80738271fc65c25531d6aa017375b96d9ba6472bbc68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
